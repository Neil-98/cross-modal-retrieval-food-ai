{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12f1ceb6-cf3c-4fc3-ab77-298c2d2ae985",
   "metadata": {
    "executionInfo": {
     "elapsed": 9513,
     "status": "ok",
     "timestamp": 1649034359621,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "12f1ceb6-cf3c-4fc3-ab77-298c2d2ae985"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import json\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# from cca_zoo.models import CCA\n",
    "\n",
    "# from cca_zoo.deepmodels import architectures\n",
    "# from cca_zoo.deepmodels import DVCCA, DCCA\n",
    "# from cca_zoo.deepmodels.architectures import BaseEncoder, Encoder, Decoder\n",
    "# from cca_zoo.deepmodels.dcca import _DCCA_base\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# making sure that the whole embedding tensor is printed in output\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf6eb3a-48b0-457a-af11-86238c12c079",
   "metadata": {
    "id": "4bf6eb3a-48b0-457a-af11-86238c12c079"
   },
   "source": [
    "# Loading necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64504e16-fae1-40de-9dc2-ed26ef94f551",
   "metadata": {
    "executionInfo": {
     "elapsed": 116524,
     "status": "ok",
     "timestamp": 1649034476869,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "64504e16-fae1-40de-9dc2-ed26ef94f551"
   },
   "outputs": [],
   "source": [
    "img_val = torch.load(\"img_val.pt\")\n",
    "text_val = torch.load(\"text_val.pt\")\n",
    "\n",
    "img_train = torch.load(\"img_train.pt\")\n",
    "text_train = torch.load(\"text_train.pt\")\n",
    "\n",
    "img_test = torch.load(\"img_test.pt\")\n",
    "text_test = torch.load(\"text_test.pt\")\n",
    "\n",
    "ingredients_test = torch.load(\"test_ingredients.pt\")\n",
    "instructions_test = torch.load(\"test_instructions.pt\")\n",
    "title_test = torch.load(\"test_title.pt\")\n",
    "\n",
    "ingredients_train = torch.load(\"train_ingredients.pt\")\n",
    "instructions_train = torch.load(\"train_instructions.pt\")\n",
    "title_train = torch.load(\"train_title.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d139fea1",
   "metadata": {
    "id": "d139fea1"
   },
   "source": [
    "## Ranking function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c7d566",
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1649034476870,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "12c7d566"
   },
   "outputs": [],
   "source": [
    "def ranker(im_vecs, instr_vecs, N = 1000, flag = \"image\"):\n",
    "    # Ranker\n",
    "    idxs = range(N)\n",
    "\n",
    "    glob_rank = []\n",
    "    glob_recall = {1:0.0,5:0.0,10:0.0}\n",
    "    for i in range(10):\n",
    "\n",
    "        ids = random.sample(range(0,len(im_vecs)), N)\n",
    "        \n",
    "        im_sub = im_vecs[ids,:]\n",
    "        instr_sub = instr_vecs[ids,:]\n",
    "\n",
    "        if flag == \"image\":\n",
    "            sims = np.dot(im_sub,instr_sub.T) # for im2recipe\n",
    "        else:\n",
    "            sims = np.dot(instr_sub,im_sub.T) # for recipe2im\n",
    "\n",
    "        med_rank = []\n",
    "        recall = {1:0.0,5:0.0,10:0.0}\n",
    "\n",
    "        for ii in idxs:\n",
    "\n",
    "            # name = ids_sub[ii]\n",
    "            # get a column of similarities\n",
    "            sim = sims[ii,:]\n",
    "\n",
    "            # sort indices in descending order\n",
    "            sorting = np.argsort(sim)[::-1].tolist()\n",
    "\n",
    "            # find where the index of the pair sample ended up in the sorting\n",
    "            pos = sorting.index(ii)\n",
    "\n",
    "            if (pos+1) == 1:\n",
    "                recall[1]+=1\n",
    "            if (pos+1) <=5:\n",
    "                recall[5]+=1\n",
    "            if (pos+1)<=10:\n",
    "                recall[10]+=1\n",
    "\n",
    "            # store the position\n",
    "            med_rank.append(pos+1)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            recall[i]=recall[i]/N\n",
    "\n",
    "        med = np.median(med_rank)\n",
    "#         print (\"median\", med)\n",
    "\n",
    "        for i in recall.keys():\n",
    "            glob_recall[i]+=recall[i]\n",
    "        glob_rank.append(med)\n",
    "\n",
    "    for i in glob_recall.keys():\n",
    "        glob_recall[i] = glob_recall[i]/10\n",
    "    \n",
    "    print (\"Mean median\", np.average(glob_rank))\n",
    "    print (\"Recall\", glob_recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022f6705",
   "metadata": {
    "id": "022f6705"
   },
   "source": [
    "# STEP 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b12673-5d63-45c2-9090-2179bc0b2fb0",
   "metadata": {
    "id": "64b12673-5d63-45c2-9090-2179bc0b2fb0"
   },
   "source": [
    "## im2recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a4ee6f-1b30-4f88-ba4f-d0015d283cf7",
   "metadata": {
    "id": "38a4ee6f-1b30-4f88-ba4f-d0015d283cf7"
   },
   "source": [
    "### Dimensional Analysis with Val data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99ef6fc-d7e5-4e16-b9af-05c8db164df5",
   "metadata": {
    "id": "c99ef6fc-d7e5-4e16-b9af-05c8db164df5"
   },
   "outputs": [],
   "source": [
    "def determine_latent_dims(dims, size, flag = \"image\"):\n",
    "\n",
    "    print(\"Applying CCA\")\n",
    "    cca = CCA(latent_dims = dims, random_state = 0)\n",
    "    cca.fit((img_train, text_train))\n",
    "    print(\"CCA done\")\n",
    "    \n",
    "    print(\"Transforming\")\n",
    "    img_train_r, text_train_r = cca.transform((img_train, text_train))\n",
    "    img_val_r, text_val_r = cca.transform((img_val, text_val))\n",
    "\n",
    "    print(\"Results for latent dims:\", str(dims), \" and test sample:\", str(size), \" and im2recipe\")\n",
    "    ranker(img_val_r, text_val_r, size, flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0e6af1-ebb9-4a04-bead-4bf7618cd04e",
   "metadata": {
    "id": "5f0e6af1-ebb9-4a04-bead-4bf7618cd04e"
   },
   "source": [
    "##### For 1k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08b225b-1432-4b87-be3b-8d93e3d49282",
   "metadata": {
    "id": "c08b225b-1432-4b87-be3b-8d93e3d49282",
    "outputId": "46293239-1b56-4480-9299-c6d599e1be1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 2  and test sample: 1000  and im2recipe\n",
      "Mean median 208.7\n",
      "Recall {1: 0.0029000000000000002, 5: 0.015200000000000002, 10: 0.030700000000000005}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 10  and test sample: 1000  and im2recipe\n",
      "Mean median 22.65\n",
      "Recall {1: 0.0509, 5: 0.20049999999999998, 10: 0.31489999999999996}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 25  and test sample: 1000  and im2recipe\n",
      "Mean median 5.0\n",
      "Recall {1: 0.2132, 5: 0.524, 10: 0.6721000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 50  and test sample: 1000  and im2recipe\n",
      "Mean median 2.5\n",
      "Recall {1: 0.3512, 5: 0.6982999999999999, 10: 0.8155999999999999}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 100  and test sample: 1000  and im2recipe\n",
      "Mean median 2.0\n",
      "Recall {1: 0.41369999999999996, 5: 0.7453999999999998, 10: 0.8433999999999999}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 200  and test sample: 1000  and im2recipe\n",
      "Mean median 1.9\n",
      "Recall {1: 0.4866, 5: 0.7834000000000001, 10: 0.8588000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 500  and test sample: 1000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5435000000000001, 5: 0.7948000000000001, 10: 0.8448}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 1000  and test sample: 1000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5494000000000001, 5: 0.7711, 10: 0.8154999999999999}\n"
     ]
    }
   ],
   "source": [
    "# For 1k samples\n",
    "for dim in [2, 10, 25, 50, 100, 200, 500, 1000]:\n",
    "    determine_latent_dims(dim, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c8295-86c4-4a71-a669-bb8bc7cc5425",
   "metadata": {
    "id": "557c8295-86c4-4a71-a669-bb8bc7cc5425"
   },
   "source": [
    "##### For 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97fd6d7-3c32-4e53-a668-63f50a60617f",
   "metadata": {
    "id": "a97fd6d7-3c32-4e53-a668-63f50a60617f",
    "outputId": "a7d0eb35-3faf-43e3-f8ce-fa74ca93c177"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 2  and test sample: 10000  and im2recipe\n",
      "Mean median 201.25\n",
      "Recall {1: 0.0033, 5: 0.0157, 10: 0.031100000000000006}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 10  and test sample: 10000  and im2recipe\n",
      "Mean median 22.8\n",
      "Recall {1: 0.05349999999999999, 5: 0.20070000000000002, 10: 0.3192}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 25  and test sample: 10000  and im2recipe\n",
      "Mean median 4.85\n",
      "Recall {1: 0.21070000000000003, 5: 0.5294000000000001, 10: 0.6819000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 50  and test sample: 10000  and im2recipe\n",
      "Mean median 2.6\n",
      "Recall {1: 0.3436, 5: 0.6906000000000001, 10: 0.8056000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 100  and test sample: 10000  and im2recipe\n",
      "Mean median 2.0\n",
      "Recall {1: 0.4053, 5: 0.7384000000000001, 10: 0.8400000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 200  and test sample: 10000  and im2recipe\n",
      "Mean median 1.95\n",
      "Recall {1: 0.48760000000000003, 5: 0.7846, 10: 0.8592000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 500  and test sample: 10000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5571, 5: 0.8049000000000002, 10: 0.8551}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 1000  and test sample: 10000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5584, 5: 0.7686, 10: 0.8164}\n"
     ]
    }
   ],
   "source": [
    "# For 10k samples\n",
    "for dim in [2, 10, 25, 50, 100, 200, 500, 1000]:\n",
    "    determine_latent_dims(dim, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9b4352-7e5b-4983-a933-a2d78daa2ae7",
   "metadata": {
    "id": "4e9b4352-7e5b-4983-a933-a2d78daa2ae7"
   },
   "source": [
    "### Ablation Studies with fixed dim = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2606fc14-019e-40da-8e96-bb575671d842",
   "metadata": {
    "id": "2606fc14-019e-40da-8e96-bb575671d842"
   },
   "source": [
    "##### For sample size = 1000, latent_dims = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e5d2d-1753-4171-b0bd-7983f0901ef2",
   "metadata": {
    "id": "500e5d2d-1753-4171-b0bd-7983f0901ef2",
    "outputId": "f1648f99-5146-4156-b331-e6f30228b3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for im2recipe: latent dims = 500, all the recipe fields\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5516, 5: 0.7968000000000001, 10: 0.85}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2instructions: latent dims = 500, only instructions\n",
      "Mean median 2.8\n",
      "Recall {1: 0.3527, 5: 0.6108, 10: 0.6873}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2ingredients: latent dims = 500, only ingredients\n",
      "Mean median 3.0\n",
      "Recall {1: 0.35550000000000004, 5: 0.6089, 10: 0.6807000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2title: latent dims = 500, only title\n",
      "Mean median 9.6\n",
      "Recall {1: 0.22159999999999996, 5: 0.4396, 10: 0.5117}\n"
     ]
    }
   ],
   "source": [
    "# im2recipe\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, text_train))\n",
    "print(\"CCA done\")\n",
    "print(\"Transforming\")\n",
    "img_train_r, text_train_r = cca.transform((img_train, text_train))\n",
    "img_test_r, text_test_r = cca.transform((img_test, text_test))\n",
    "print(\"Results for im2recipe: latent dims = 500, all the recipe fields\")\n",
    "ranker(img_test_r, text_test_r,  1000, \"image\")\n",
    "\n",
    "# im2instructions\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, instructions_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, instructions_test_r = cca.transform((img_test, instructions_test))\n",
    "print(\"Results for im2instructions: latent dims = 500, only instructions\")\n",
    "ranker(img_test_r, instructions_test_r, 1000, \"image\")\n",
    "\n",
    "# im2ingredients\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, ingredients_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, ingredients_test_r = cca.transform((img_test, ingredients_test))\n",
    "print(\"Results for im2ingredients: latent dims = 500, only ingredients\")\n",
    "ranker(img_test_r, ingredients_test_r,  1000, \"image\")\n",
    "\n",
    "# im2title\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, title_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, title_test_r = cca.transform((img_test, title_test))\n",
    "print(\"Results for im2title: latent dims = 500, only title\")\n",
    "ranker(img_test_r, title_test_r,  1000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9804e73-7c90-4b09-a40e-1a83cdf7e514",
   "metadata": {
    "id": "c9804e73-7c90-4b09-a40e-1a83cdf7e514"
   },
   "source": [
    "##### For sample size = 10000, latent_dims = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a624439e-3a76-4c57-a1cf-c36d2266cdad",
   "metadata": {
    "id": "a624439e-3a76-4c57-a1cf-c36d2266cdad",
    "outputId": "85170c18-e13c-41db-df78-d2d263289f94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for im2recipe: latent dims = 500, all the recipe fields\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5408000000000002, 5: 0.7966000000000001, 10: 0.85}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2instructions: latent dims = 500, only instructions\n",
      "Mean median 3.0\n",
      "Recall {1: 0.3435, 5: 0.6043000000000001, 10: 0.6788000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2ingredients: latent dims = 500, only ingredients\n",
      "Mean median 3.0\n",
      "Recall {1: 0.36279999999999996, 5: 0.6005, 10: 0.6766000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for im2title: latent dims = 500, only title\n",
      "Mean median 10.9\n",
      "Recall {1: 0.2109, 5: 0.4307, 10: 0.4992}\n"
     ]
    }
   ],
   "source": [
    "# im2recipe\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, text_train))\n",
    "print(\"CCA done\")\n",
    "print(\"Transforming\")\n",
    "img_train_r, text_train_r = cca.transform((img_train, text_train))\n",
    "img_test_r, text_test_r = cca.transform((img_test, text_test))\n",
    "print(\"Results for im2recipe: latent dims = 500, all the recipe fields\")\n",
    "ranker(img_test_r, text_test_r,  10000, \"image\")\n",
    "\n",
    "# im2instructions\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, instructions_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, instructions_test_r = cca.transform((img_test, instructions_test))\n",
    "print(\"Results for im2instructions: latent dims = 500, only instructions\")\n",
    "ranker(img_test_r, instructions_test_r, 10000, \"image\")\n",
    "\n",
    "# im2ingredients\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, ingredients_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, ingredients_test_r = cca.transform((img_test, ingredients_test))\n",
    "print(\"Results for im2ingredients: latent dims = 500, only ingredients\")\n",
    "ranker(img_test_r, ingredients_test_r,  10000, \"image\")\n",
    "\n",
    "# im2title\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((img_train, title_train))\n",
    "print(\"CCA done\")\n",
    "img_test_r, title_test_r = cca.transform((img_test, title_test))\n",
    "print(\"Results for im2title: latent dims = 500, only title\")\n",
    "ranker(img_test_r, title_test_r,  10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccae98-d1ef-4f5f-9668-04c101da262e",
   "metadata": {
    "id": "dbccae98-d1ef-4f5f-9668-04c101da262e"
   },
   "source": [
    "## recipe2im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d6eaf-f99d-4111-b06b-b531501ada61",
   "metadata": {
    "id": "927d6eaf-f99d-4111-b06b-b531501ada61"
   },
   "source": [
    "### Using Validation Data to figure out optimal latent_dims for 1k and 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8996b277-2a91-4810-9335-d430458d9567",
   "metadata": {
    "id": "8996b277-2a91-4810-9335-d430458d9567"
   },
   "outputs": [],
   "source": [
    "def determine_latent_dims_recipe2im(dims, size, flag = \"text\"):\n",
    "\n",
    "    print(\"Applying CCA\")\n",
    "    cca = CCA(latent_dims = dims, random_state = 0)\n",
    "    cca.fit((text_train, img_train))\n",
    "    print(\"CCA done\")\n",
    "    \n",
    "    print(\"Transforming\")\n",
    "    text_train_r, img_train_r = cca.transform((img_train, text_train))\n",
    "    text_val_r, img_val_r = cca.transform((text_val, img_val))\n",
    "\n",
    "    print(\"Results for latent dims:\", str(dims), \" and test sample:\", str(size), \" and im2recipe\")\n",
    "    ranker(img_val_r, text_val_r, size, flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc8ca56-6956-47ff-9ee5-7012a23b6311",
   "metadata": {
    "id": "1cc8ca56-6956-47ff-9ee5-7012a23b6311"
   },
   "source": [
    "##### For 1k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f04a5-7b37-4bb0-ad02-a6ba5771bdce",
   "metadata": {
    "id": "475f04a5-7b37-4bb0-ad02-a6ba5771bdce",
    "outputId": "a7b63b5f-0481-447d-a120-00bc044b0be2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 2  and test sample: 1000  and im2recipe\n",
      "Mean median 206.7\n",
      "Recall {1: 0.0035000000000000005, 5: 0.016900000000000005, 10: 0.032900000000000006}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 10  and test sample: 1000  and im2recipe\n",
      "Mean median 22.45\n",
      "Recall {1: 0.0506, 5: 0.1991, 10: 0.3191}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 25  and test sample: 1000  and im2recipe\n",
      "Mean median 5.0\n",
      "Recall {1: 0.22290000000000001, 5: 0.5223000000000001, 10: 0.6703000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 50  and test sample: 1000  and im2recipe\n",
      "Mean median 2.6\n",
      "Recall {1: 0.35409999999999997, 5: 0.6824, 10: 0.8046000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 100  and test sample: 1000  and im2recipe\n",
      "Mean median 2.0\n",
      "Recall {1: 0.4122, 5: 0.7384999999999998, 10: 0.8432000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 200  and test sample: 1000  and im2recipe\n",
      "Mean median 1.9\n",
      "Recall {1: 0.47380000000000005, 5: 0.7702000000000001, 10: 0.8508999999999999}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 500  and test sample: 1000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5563, 5: 0.7981, 10: 0.8514999999999999}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 1000  and test sample: 1000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5508, 5: 0.7712000000000001, 10: 0.8161999999999999}\n"
     ]
    }
   ],
   "source": [
    "# For 1k samples\n",
    "for dim in [2, 10, 25, 50, 100, 200, 500, 1000]:\n",
    "    determine_latent_dims_recipe2im(dim, 1000, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07e2117-8725-4dc4-9f9a-b874d21e853b",
   "metadata": {
    "id": "c07e2117-8725-4dc4-9f9a-b874d21e853b"
   },
   "source": [
    "##### For 10k samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c7d68c-8305-44bb-ab0f-07434b2e58e8",
   "metadata": {
    "id": "08c7d68c-8305-44bb-ab0f-07434b2e58e8",
    "outputId": "0a63acc5-ece4-46ea-bcbb-f6101535d2ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 2  and test sample: 10000  and im2recipe\n",
      "Mean median 209.65\n",
      "Recall {1: 0.0035000000000000005, 5: 0.015999999999999997, 10: 0.03250000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 10  and test sample: 10000  and im2recipe\n",
      "Mean median 22.85\n",
      "Recall {1: 0.055700000000000006, 5: 0.2036, 10: 0.32120000000000004}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 25  and test sample: 10000  and im2recipe\n",
      "Mean median 4.9\n",
      "Recall {1: 0.22090000000000004, 5: 0.5244000000000001, 10: 0.6756000000000001}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 50  and test sample: 10000  and im2recipe\n",
      "Mean median 2.7\n",
      "Recall {1: 0.3515, 5: 0.6797000000000001, 10: 0.7995}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 100  and test sample: 10000  and im2recipe\n",
      "Mean median 2.0\n",
      "Recall {1: 0.41339999999999993, 5: 0.7431000000000001, 10: 0.8404999999999999}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 200  and test sample: 10000  and im2recipe\n",
      "Mean median 1.9\n",
      "Recall {1: 0.48529999999999995, 5: 0.7773000000000001, 10: 0.8526}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 500  and test sample: 10000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5543, 5: 0.7939999999999999, 10: 0.8513}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for latent dims: 1000  and test sample: 10000  and im2recipe\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5567, 5: 0.7662, 10: 0.8083}\n"
     ]
    }
   ],
   "source": [
    "# For 10k samples\n",
    "for dim in [2, 10, 25, 50, 100, 200, 500, 1000]:\n",
    "    determine_latent_dims_recipe2im(dim, 10000, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff253dc1-1f1c-4669-ba8f-ffec14253e51",
   "metadata": {
    "id": "ff253dc1-1f1c-4669-ba8f-ffec14253e51"
   },
   "source": [
    "### Ablation Studies with fixed dims = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1658c8-da88-427c-9736-5ccf1db5a2cc",
   "metadata": {
    "id": "2a1658c8-da88-427c-9736-5ccf1db5a2cc"
   },
   "source": [
    "##### For sample size = 1000, latent_dims = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b792b-cad8-4141-ab3f-9830282e11d5",
   "metadata": {
    "id": "dd2b792b-cad8-4141-ab3f-9830282e11d5",
    "outputId": "0a348a9b-8dd1-41d7-d1e0-bf0d4443b836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for recipe2im: latent dims = 500, all the recipe fields\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5437000000000001, 5: 0.7927000000000001, 10: 0.8454}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for instructions2im: latent dims = 500, only instructions\n",
      "Mean median 2.9\n",
      "Recall {1: 0.36050000000000004, 5: 0.6161, 10: 0.6944}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for ingredients2im: latent dims = 500, only ingredients\n",
      "Mean median 2.8\n",
      "Recall {1: 0.371, 5: 0.6043999999999999, 10: 0.6749}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for title2im: latent dims = 500, only title\n",
      "Mean median 9.8\n",
      "Recall {1: 0.21880000000000002, 5: 0.43279999999999996, 10: 0.5057}\n"
     ]
    }
   ],
   "source": [
    "# recipe2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((text_train, img_train))\n",
    "print(\"CCA done\")\n",
    "print(\"Transforming\")\n",
    "text_train_r, img_train_r = cca.transform((text_train, img_train))\n",
    "text_test_r, img_test_r = cca.transform((text_test, img_test))\n",
    "print(\"Results for recipe2im: latent dims = 500, all the recipe fields\")\n",
    "ranker(img_test_r, text_test_r,  1000, \"text\")\n",
    "\n",
    "# instructions2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((instructions_train, img_train))\n",
    "print(\"CCA done\")\n",
    "instructions_test_r, img_test_r = cca.transform((instructions_test, img_test))\n",
    "print(\"Results for instructions2im: latent dims = 500, only instructions\")\n",
    "ranker(img_test_r, instructions_test_r, 1000, \"text\")\n",
    "\n",
    "# ingredients2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((ingredients_train, img_train))\n",
    "print(\"CCA done\")\n",
    "ingredients_test_r, img_test_r = cca.transform((ingredients_test, img_test))\n",
    "print(\"Results for ingredients2im: latent dims = 500, only ingredients\")\n",
    "ranker(img_test_r, ingredients_test_r,  1000, \"text\")\n",
    "\n",
    "# title2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((title_train, img_train))\n",
    "print(\"CCA done\")\n",
    "title_test_r, img_test_r  = cca.transform((title_test, img_test))\n",
    "print(\"Results for title2im: latent dims = 500, only title\")\n",
    "ranker(img_test_r, title_test_r,  1000, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb3196-262a-4e57-b2fa-80c2cd02cd19",
   "metadata": {
    "id": "87eb3196-262a-4e57-b2fa-80c2cd02cd19"
   },
   "source": [
    "##### For sample size = 10000, latent_dims = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74bcda2-8dcd-4b72-b66e-7c26e6ff6ec2",
   "metadata": {
    "id": "b74bcda2-8dcd-4b72-b66e-7c26e6ff6ec2",
    "outputId": "8efa0d2b-7ddb-4210-a17a-af44375c9119"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CCA\n",
      "CCA done\n",
      "Transforming\n",
      "Results for recipe2im: latent dims = 500, all the recipe fields\n",
      "Mean median 1.0\n",
      "Recall {1: 0.5551, 5: 0.7871, 10: 0.842}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for instructions2im: latent dims = 500, only instructions\n",
      "Mean median 3.0\n",
      "Recall {1: 0.3602, 5: 0.6132, 10: 0.6824}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for ingredients2im: latent dims = 500, only ingredients\n",
      "Mean median 3.0\n",
      "Recall {1: 0.3642, 5: 0.6056, 10: 0.6761}\n",
      "Applying CCA\n",
      "CCA done\n",
      "Results for title2im: latent dims = 500, only title\n",
      "Mean median 10.65\n",
      "Recall {1: 0.21480000000000002, 5: 0.42969999999999997, 10: 0.5042000000000001}\n"
     ]
    }
   ],
   "source": [
    "# recipe2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((text_train, img_train))\n",
    "print(\"CCA done\")\n",
    "print(\"Transforming\")\n",
    "text_train_r, img_train_r = cca.transform((text_train, img_train))\n",
    "text_test_r, img_test_r = cca.transform((text_test, img_test))\n",
    "print(\"Results for recipe2im: latent dims = 500, all the recipe fields\")\n",
    "ranker(img_test_r, text_test_r,  10000, \"text\")\n",
    "\n",
    "# instructions2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((instructions_train, img_train))\n",
    "print(\"CCA done\")\n",
    "instructions_test_r, img_test_r = cca.transform((instructions_test, img_test))\n",
    "print(\"Results for instructions2im: latent dims = 500, only instructions\")\n",
    "ranker(img_test_r, instructions_test_r, 10000, \"text\")\n",
    "\n",
    "# ingredients2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((ingredients_train, img_train))\n",
    "print(\"CCA done\")\n",
    "ingredients_test_r, img_test_r = cca.transform((ingredients_test, img_test))\n",
    "print(\"Results for ingredients2im: latent dims = 500, only ingredients\")\n",
    "ranker(img_test_r, ingredients_test_r,  10000, \"text\")\n",
    "\n",
    "# title2im\n",
    "print(\"Applying CCA\")\n",
    "cca = CCA(latent_dims = 500, random_state = 0)\n",
    "cca.fit((title_train, img_train))\n",
    "print(\"CCA done\")\n",
    "title_test_r, img_test_r  = cca.transform((title_test, img_test))\n",
    "print(\"Results for title2im: latent dims = 500, only title\")\n",
    "ranker(img_test_r, title_test_r,  10000, \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfcad30",
   "metadata": {
    "id": "6dfcad30"
   },
   "source": [
    "# STEP 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7f444d-cfff-43ec-97dc-c92d837a69d2",
   "metadata": {
    "id": "df7f444d-cfff-43ec-97dc-c92d837a69d2"
   },
   "source": [
    "## Non Linear Embeddings - Normal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d95bb898-c671-41e0-8907-5616e6907542",
   "metadata": {
    "id": "d95bb898-c671-41e0-8907-5616e6907542"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6fa414-9e21-4783-9299-dc112db34c4c",
   "metadata": {
    "id": "ef6fa414-9e21-4783-9299-dc112db34c4c"
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES='2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db9def68-099e-486a-82b0-048edf70fd73",
   "metadata": {
    "id": "db9def68-099e-486a-82b0-048edf70fd73"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c05f885",
   "metadata": {
    "id": "3c05f885"
   },
   "source": [
    "### Model Creation dims = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c1c7ac0-20ee-4838-bff7-d70cdc11946e",
   "metadata": {
    "id": "4c1c7ac0-20ee-4838-bff7-d70cdc11946e"
   },
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, image_emb, text_emb, transform=None):\n",
    "        self.image_emb = torch.as_tensor(np.array(image_emb))\n",
    "        self.text_emb = torch.as_tensor(np.array(text_emb))        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_emb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_emb[idx], self.text_emb[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f656948f-ac43-4e9d-aba7-fee81028079b",
   "metadata": {
    "id": "f656948f-ac43-4e9d-aba7-fee81028079b"
   },
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, output_size, input_size=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75afd4aa",
   "metadata": {
    "id": "75afd4aa"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7a7bb5-5a25-4b22-8320-f5592174989b",
   "metadata": {
    "id": "fd7a7bb5-5a25-4b22-8320-f5592174989b"
   },
   "outputs": [],
   "source": [
    "img_model = EmbeddingNetwork(512)\n",
    "img_model= nn.DataParallel(img_model, device_ids=[1,2,3])\n",
    "img_model.to(device);\n",
    "\n",
    "txt_model = EmbeddingNetwork(512);\n",
    "txt_model= nn.DataParallel(txt_model, device_ids=[1,2,3])\n",
    "txt_model.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fab9e0-8dd7-474d-bc2b-83ecb1331c62",
   "metadata": {
    "id": "c5fab9e0-8dd7-474d-bc2b-83ecb1331c62"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ca9526-3d71-4c3f-8de6-6df416012b18",
   "metadata": {
    "id": "c1ca9526-3d71-4c3f-8de6-6df416012b18"
   },
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(img_train, text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f6d580-3975-4aaa-97bf-bd1b1afbcf25",
   "metadata": {
    "id": "42f6d580-3975-4aaa-97bf-bd1b1afbcf25"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    # Utility function for timers\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c3f3bc-3c34-49e3-a02c-fa4564491ec6",
   "metadata": {
    "id": "87c3f3bc-3c34-49e3-a02c-fa4564491ec6"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, img_model, txt_model, criterion, optimizer, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    img_model.train()\n",
    "    txt_model.train()\n",
    "    \n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    end = time.time()\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (image_emb, text_emb) in enumerate(train_loader):\n",
    "    \n",
    "        # Use GPU if available\n",
    "        if use_gpu: \n",
    "            image_emb, text_emb = image_emb.to(f'cuda:{img_model.device_ids[0]}'), text_emb.to(f'cuda:{txt_model.device_ids[0]}')\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Run forward pass\n",
    "        out_image_emb = img_model(image_emb) \n",
    "        out_text_emb = txt_model(text_emb)\n",
    "        loss = criterion(out_image_emb, out_text_emb) \n",
    "\n",
    "        # Compute gradient and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Print model accuracy -- in the code below\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 0:\n",
    "            last_loss = running_loss / 2000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "        \n",
    "        if i % 2000 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                'Time {batch_time.val} ({batch_time.avg})\\t'\n",
    "                'Data {data_time.val} ({data_time.avg})\\t'.format(\n",
    "                  epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                 data_time=data_time)) \n",
    "\n",
    "    print('Finished training epoch {}'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe8ffc",
   "metadata": {
    "id": "fabe8ffc",
    "outputId": "57b67426-2b51-4ba5-dcb6-a4ee81fe3143",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00012609273195266724\n",
      "Epoch: [0][0/4400]\tTime 0.04702162742614746 (0.04702162742614746)\tData 0.004031181335449219 (0.004031181335449219)\t\n",
      "  batch 2001 loss: 0.24717661296576263\n",
      "Epoch: [0][2000/4400]\tTime 0.016170024871826172 (0.02201926880988522)\tData 0.0010480880737304688 (0.0020143839194141943)\t\n",
      "  batch 4001 loss: 0.24701430730521678\n",
      "Epoch: [0][4000/4400]\tTime 0.007898330688476562 (0.021020093759099592)\tData 0.0008800029754638672 (0.0025363345052027398)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012473785132169723\n",
      "Epoch: [1][0/4400]\tTime 0.013044357299804688 (0.013044357299804688)\tData 0.0018548965454101562 (0.0018548965454101562)\t\n",
      "  batch 2001 loss: 0.24695295250415802\n",
      "Epoch: [1][2000/4400]\tTime 0.01843857765197754 (0.013877756532462223)\tData 0.010588645935058594 (0.003927832183570995)\t\n",
      "  batch 4001 loss: 0.2469778815358877\n",
      "Epoch: [1][4000/4400]\tTime 0.009508848190307617 (0.01323062686495887)\tData 0.0009267330169677734 (0.00400590431806416)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00012417319416999816\n",
      "Epoch: [2][0/4400]\tTime 0.00838017463684082 (0.00838017463684082)\tData 0.001226663589477539 (0.001226663589477539)\t\n",
      "  batch 2001 loss: 0.24680873446911572\n",
      "Epoch: [2][2000/4400]\tTime 0.02420353889465332 (0.010789274156599984)\tData 0.0012309551239013672 (0.002520732078952589)\t\n",
      "  batch 4001 loss: 0.24685503162443637\n",
      "Epoch: [2][4000/4400]\tTime 0.03175020217895508 (0.012776631171987582)\tData 0.023952722549438477 (0.0027336732830532907)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012356488406658174\n",
      "Epoch: [3][0/4400]\tTime 0.009073257446289062 (0.009073257446289062)\tData 0.0019693374633789062 (0.0019693374633789062)\t\n",
      "  batch 2001 loss: 0.24677599707245826\n",
      "Epoch: [3][2000/4400]\tTime 0.008017778396606445 (0.012680473951981224)\tData 0.0008685588836669922 (0.004303122567630064)\t\n",
      "  batch 4001 loss: 0.24680619213730096\n",
      "Epoch: [3][4000/4400]\tTime 0.014058113098144531 (0.012673362974106327)\tData 0.006962776184082031 (0.004262264595422647)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.00012457979470491409\n",
      "Epoch: [4][0/4400]\tTime 0.00871586799621582 (0.00871586799621582)\tData 0.0012311935424804688 (0.0012311935424804688)\t\n",
      "  batch 2001 loss: 0.2466205129250884\n",
      "Epoch: [4][2000/4400]\tTime 0.008228302001953125 (0.012524732883306577)\tData 0.0008909702301025391 (0.004089054615720399)\t\n",
      "  batch 4001 loss: 0.24676169481128454\n",
      "Epoch: [4][4000/4400]\tTime 0.008185148239135742 (0.012575608615546309)\tData 0.0008809566497802734 (0.004083134775845833)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.00012439538538455962\n",
      "Epoch: [5][0/4400]\tTime 0.008723020553588867 (0.008723020553588867)\tData 0.0019516944885253906 (0.0019516944885253906)\t\n",
      "  batch 2001 loss: 0.24662472403049468\n",
      "Epoch: [5][2000/4400]\tTime 0.01852250099182129 (0.012877663393606846)\tData 0.010009527206420898 (0.004273617642930244)\t\n",
      "  batch 4001 loss: 0.24666338563710452\n",
      "Epoch: [5][4000/4400]\tTime 0.011287689208984375 (0.012826405713272763)\tData 0.0024967193603515625 (0.004070989729851015)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.00012615114450454713\n",
      "Epoch: [6][0/4400]\tTime 0.00932002067565918 (0.00932002067565918)\tData 0.0011491775512695312 (0.0011491775512695312)\t\n",
      "  batch 2001 loss: 0.2469269755780697\n",
      "Epoch: [6][2000/4400]\tTime 0.008359670639038086 (0.012327542369333522)\tData 0.0008921623229980469 (0.003957157787950202)\t\n",
      "  batch 4001 loss: 0.24677036824822426\n",
      "Epoch: [6][4000/4400]\tTime 0.011483907699584961 (0.012662301031359135)\tData 0.0038046836853027344 (0.004078528667145805)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.00012349564582109452\n",
      "Epoch: [7][0/4400]\tTime 0.009047746658325195 (0.009047746658325195)\tData 0.0017075538635253906 (0.0017075538635253906)\t\n",
      "  batch 2001 loss: 0.24655575805157423\n",
      "Epoch: [7][2000/4400]\tTime 0.02346181869506836 (0.0152139184714436)\tData 0.0009188652038574219 (0.0035965106655275266)\t\n",
      "  batch 4001 loss: 0.2464637188538909\n",
      "Epoch: [7][4000/4400]\tTime 0.01176142692565918 (0.019815494286599858)\tData 0.0010609626770019531 (0.0029389224210937927)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.00012582740187644959\n",
      "Epoch: [8][0/4400]\tTime 0.016866207122802734 (0.016866207122802734)\tData 0.0022430419921875 (0.0022430419921875)\t\n",
      "  batch 2001 loss: 0.24658256811648607\n",
      "Epoch: [8][2000/4400]\tTime 0.011252641677856445 (0.02249426784543977)\tData 0.0009453296661376953 (0.0022755220852632156)\t\n",
      "  batch 4001 loss: 0.24662985403835774\n",
      "Epoch: [8][4000/4400]\tTime 0.021701574325561523 (0.02373869798684591)\tData 0.0011188983917236328 (0.002340112856345545)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.00012194472551345825\n",
      "Epoch: [9][0/4400]\tTime 0.010298728942871094 (0.010298728942871094)\tData 0.0024166107177734375 (0.0024166107177734375)\t\n",
      "  batch 2001 loss: 0.24650946644693614\n",
      "Epoch: [9][2000/4400]\tTime 0.014107465744018555 (0.024044001596918826)\tData 0.0009734630584716797 (0.002231361149907529)\t\n",
      "  batch 4001 loss: 0.24652025111019613\n",
      "Epoch: [9][4000/4400]\tTime 0.009194135665893555 (0.022960016829345977)\tData 0.0010347366333007812 (0.0022629829264676324)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'checkpoints/img-model-full-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'checkpoints/txt-model-full-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9add441",
   "metadata": {
    "id": "f9add441"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcffa099",
   "metadata": {
    "id": "fcffa099"
   },
   "outputs": [],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982f6ee",
   "metadata": {
    "id": "3982f6ee"
   },
   "outputs": [],
   "source": [
    "img_model_title = EmbeddingNetwork(512)\n",
    "img_model_title= nn.DataParallel(img_model_title, device_ids=[1,2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(512);\n",
    "txt_model_title= nn.DataParallel(txt_model_title, device_ids=[1,2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_title.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed98cef",
   "metadata": {
    "id": "1ed98cef"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    img_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    txt_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b5971",
   "metadata": {
    "id": "837b5971",
    "outputId": "bf921120-cce4-4d82-f757-bfc91d856969",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0002364463210105896\n",
      "Epoch: [0][0/4400]\tTime 0.032234907150268555 (0.032234907150268555)\tData 0.004739999771118164 (0.004739999771118164)\t\n",
      "  batch 2001 loss: 0.4691778804063797\n",
      "Epoch: [0][2000/4400]\tTime 0.019277572631835938 (0.021416319542560264)\tData 0.001470804214477539 (0.002138279843842727)\t\n",
      "  batch 4001 loss: 0.46928279390931127\n",
      "Epoch: [0][4000/4400]\tTime 0.012997865676879883 (0.021658227194014026)\tData 0.0013132095336914062 (0.0022000336879433708)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0002340276688337326\n",
      "Epoch: [1][0/4400]\tTime 0.013715028762817383 (0.013715028762817383)\tData 0.002393484115600586 (0.002393484115600586)\t\n",
      "  batch 2001 loss: 0.4689103953242302\n",
      "Epoch: [1][2000/4400]\tTime 0.01427316665649414 (0.020105006634027346)\tData 0.0012676715850830078 (0.0022924063147335633)\t\n",
      "  batch 4001 loss: 0.4691453038007021\n",
      "Epoch: [1][4000/4400]\tTime 0.008884906768798828 (0.020155201372996593)\tData 0.0013082027435302734 (0.0022263292728796867)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00023178084194660188\n",
      "Epoch: [2][0/4400]\tTime 0.015045404434204102 (0.015045404434204102)\tData 0.002467632293701172 (0.002467632293701172)\t\n",
      "  batch 2001 loss: 0.4687591468691826\n",
      "Epoch: [2][2000/4400]\tTime 0.016688823699951172 (0.02199234192756222)\tData 0.001440286636352539 (0.002360478572283072)\t\n",
      "  batch 4001 loss: 0.4690358355045319\n",
      "Epoch: [2][4000/4400]\tTime 0.019841909408569336 (0.0224044777041404)\tData 0.001584768295288086 (0.0024001830162271444)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.000236532986164093\n",
      "Epoch: [3][0/4400]\tTime 0.012968301773071289 (0.012968301773071289)\tData 0.002588510513305664 (0.002588510513305664)\t\n",
      "  batch 2001 loss: 0.46891292287409303\n",
      "Epoch: [3][2000/4400]\tTime 0.0339810848236084 (0.022500560618471586)\tData 0.0009245872497558594 (0.0022351155574175195)\t\n",
      "  batch 4001 loss: 0.4690849600583315\n",
      "Epoch: [3][4000/4400]\tTime 0.025032758712768555 (0.022958364703601016)\tData 0.0009751319885253906 (0.002388756980123713)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.00023791208863258362\n",
      "Epoch: [4][0/4400]\tTime 0.013506650924682617 (0.013506650924682617)\tData 0.0015087127685546875 (0.0015087127685546875)\t\n",
      "  batch 2001 loss: 0.4691920136958361\n",
      "Epoch: [4][2000/4400]\tTime 0.028635501861572266 (0.021163219573913605)\tData 0.0016944408416748047 (0.0022314445547078144)\t\n",
      "  batch 4001 loss: 0.46909729258716104\n",
      "Epoch: [4][4000/4400]\tTime 0.014439821243286133 (0.021174889271094243)\tData 0.006082773208618164 (0.002323212608102857)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "    torch.save(img_model_title.state_dict(), 'checkpoints/img-model-title-512-epoch-{}.pth'.format(epoch+1))\n",
    "    torch.save(txt_model_title.state_dict(), 'checkpoints/txt-model-title-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9663baa6",
   "metadata": {
    "id": "9663baa6"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f244c",
   "metadata": {
    "id": "0a9f244c",
    "outputId": "3b71a0b6-df8c-42f9-fd5f-8db94ed8f016",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 10.312287109375\n",
      "Epoch: [0][0/4400]\tTime 0.017222166061401367 (0.017222166061401367)\tData 0.0023643970489501953 (0.0023643970489501953)\t\n",
      "  batch 2001 loss: 8465.284108154297\n",
      "Epoch: [0][2000/4400]\tTime 0.017594575881958008 (0.012661087935951458)\tData 0.00970458984375 (0.004405748719039528)\t\n",
      "  batch 4001 loss: 8259.536986328125\n",
      "Epoch: [0][4000/4400]\tTime 0.03874492645263672 (0.012619327706296693)\tData 0.0299530029296875 (0.004399139980648911)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 4.2461435546875\n",
      "Epoch: [1][0/4400]\tTime 0.008536815643310547 (0.008536815643310547)\tData 0.0014150142669677734 (0.0014150142669677734)\t\n",
      "  batch 2001 loss: 8320.310545898437\n",
      "Epoch: [1][2000/4400]\tTime 0.009508609771728516 (0.012479719431742258)\tData 0.0010323524475097656 (0.00433090947259372)\t\n",
      "  batch 4001 loss: 8351.937978759766\n",
      "Epoch: [1][4000/4400]\tTime 0.008218765258789062 (0.01246253724874064)\tData 0.0009169578552246094 (0.004248894145863559)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 4.15165966796875\n",
      "Epoch: [2][0/4400]\tTime 0.009237051010131836 (0.009237051010131836)\tData 0.0014986991882324219 (0.0014986991882324219)\t\n",
      "  batch 2001 loss: 8362.919387939453\n",
      "Epoch: [2][2000/4400]\tTime 0.022101163864135742 (0.012497181775628299)\tData 0.01412820816040039 (0.004231098828942462)\t\n",
      "  batch 4001 loss: 8366.723030029298\n",
      "Epoch: [2][4000/4400]\tTime 0.008820295333862305 (0.012536554329397083)\tData 0.0009195804595947266 (0.004248079613607188)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 4.105224609375\n",
      "Epoch: [3][0/4400]\tTime 0.019619226455688477 (0.019619226455688477)\tData 0.007081508636474609 (0.007081508636474609)\t\n",
      "  batch 2001 loss: 8358.54123046875\n",
      "Epoch: [3][2000/4400]\tTime 0.010164499282836914 (0.01269237855742539)\tData 0.0014395713806152344 (0.004196186294441281)\t\n",
      "  batch 4001 loss: 8360.161078369141\n",
      "Epoch: [3][4000/4400]\tTime 0.011054277420043945 (0.012586066318255488)\tData 0.0038423538208007812 (0.004131380125034812)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 4.164775390625\n",
      "Epoch: [4][0/4400]\tTime 0.009961366653442383 (0.009961366653442383)\tData 0.0017933845520019531 (0.0017933845520019531)\t\n",
      "  batch 2001 loss: 8351.245053222656\n",
      "Epoch: [4][2000/4400]\tTime 0.009833812713623047 (0.012526017198081257)\tData 0.0021512508392333984 (0.004038147066069626)\t\n",
      "  batch 4001 loss: 8356.503283935546\n",
      "Epoch: [4][4000/4400]\tTime 0.008254051208496094 (0.012737476536703837)\tData 0.0008840560913085938 (0.004151491843530817)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(512)\n",
    "img_model_ingredients= nn.DataParallel(img_model_ingredients, device_ids=[1,2,3])\n",
    "img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(512)\n",
    "txt_model_ingredients= nn.DataParallel(txt_model_ingredients, device_ids=[1,2,3])\n",
    "txt_model_ingredients.to((f'cuda:{txt_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_ingredients.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    img_model_ingredients = img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    txt_model_ingredients = txt_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_ingredients.state_dict(), 'checkpoints/img-model-ingredients-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'checkpoints/txt-model-ingredients-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3062b9f9",
   "metadata": {
    "id": "3062b9f9"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2689a3",
   "metadata": {
    "id": "de2689a3",
    "outputId": "4923b29d-9cb8-4954-f192-deea8a109276",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0003244844079017639\n",
      "Epoch: [0][0/4400]\tTime 0.01202392578125 (0.01202392578125)\tData 0.0015544891357421875 (0.0015544891357421875)\t\n",
      "  batch 2001 loss: 0.26000524858385327\n",
      "Epoch: [0][2000/4400]\tTime 0.008465290069580078 (0.012482271618630992)\tData 0.0008780956268310547 (0.004153854188056423)\t\n",
      "  batch 4001 loss: 0.2534713015407324\n",
      "Epoch: [0][4000/4400]\tTime 0.01741790771484375 (0.012547566276346258)\tData 0.009671688079833984 (0.0042191943774310325)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0001269243061542511\n",
      "Epoch: [1][0/4400]\tTime 0.00867319107055664 (0.00867319107055664)\tData 0.0012242794036865234 (0.0012242794036865234)\t\n",
      "  batch 2001 loss: 0.255437093809247\n",
      "Epoch: [1][2000/4400]\tTime 0.00827479362487793 (0.011620957156767076)\tData 0.0007474422454833984 (0.00357155559183299)\t\n",
      "  batch 4001 loss: 0.25655249509960415\n",
      "Epoch: [1][4000/4400]\tTime 0.034348487854003906 (0.012103707693243468)\tData 0.027225017547607422 (0.0039833227833102145)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00013121607899665833\n",
      "Epoch: [2][0/4400]\tTime 0.014897346496582031 (0.014897346496582031)\tData 0.0019195079803466797 (0.0019195079803466797)\t\n",
      "  batch 2001 loss: 0.25679434490203856\n",
      "Epoch: [2][2000/4400]\tTime 0.008372306823730469 (0.01239327547968417)\tData 0.0013477802276611328 (0.0044111974354924584)\t\n",
      "  batch 4001 loss: 0.25695992396771905\n",
      "Epoch: [2][4000/4400]\tTime 0.025528669357299805 (0.01252390336644736)\tData 0.01619100570678711 (0.004397540532240597)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00013017278909683227\n",
      "Epoch: [3][0/4400]\tTime 0.010421276092529297 (0.010421276092529297)\tData 0.0018799304962158203 (0.0018799304962158203)\t\n",
      "  batch 2001 loss: 0.25662765406817195\n",
      "Epoch: [3][2000/4400]\tTime 0.007941961288452148 (0.012778329706263506)\tData 0.0008673667907714844 (0.0041696988362660715)\t\n",
      "  batch 4001 loss: 0.2568015474230051\n",
      "Epoch: [3][4000/4400]\tTime 0.01091313362121582 (0.01284114577358468)\tData 0.0030684471130371094 (0.004115178864528405)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.00013043658435344695\n",
      "Epoch: [4][0/4400]\tTime 0.009485483169555664 (0.009485483169555664)\tData 0.0019526481628417969 (0.0019526481628417969)\t\n",
      "  batch 2001 loss: 0.2563004427179694\n",
      "Epoch: [4][2000/4400]\tTime 0.008645057678222656 (0.012689082638017539)\tData 0.0008752346038818359 (0.004159314462031679)\t\n",
      "  batch 4001 loss: 0.2564817107766867\n",
      "Epoch: [4][4000/4400]\tTime 0.013062238693237305 (0.01273939705467081)\tData 0.002601146697998047 (0.004347247202615087)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_instructions = EmbeddingNetwork(512)\n",
    "img_model_instructions= nn.DataParallel(img_model_instructions, device_ids=[1,2,3])\n",
    "img_model_instructions.to((f'cuda:{img_model_instructions.device_ids[0]}'));\n",
    "txt_model_instructions = EmbeddingNetwork(512)\n",
    "txt_model_instructions= nn.DataParallel(txt_model_instructions, device_ids=[1,2,3])\n",
    "txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_instructions.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    img_model_instructions = img_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    txt_model_instructions = txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_instructions.state_dict(), 'checkpoints/img-model-instructions-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'checkpoints/txt-model-instructions-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364fd19b",
   "metadata": {
    "id": "364fd19b"
   },
   "source": [
    "### Model Creation dims = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7aeb7",
   "metadata": {
    "id": "37c7aeb7"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gscCkfG1LWWN",
   "metadata": {
    "id": "gscCkfG1LWWN",
    "outputId": "7ed3a2b0-c885-4100-dd0b-4ad8f4f47242",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00033096739649772646\n",
      "Epoch: [0][0/4400]\tTime 0.009809494018554688 (0.009809494018554688)\tData 0.0020685195922851562 (0.0020685195922851562)\t\n",
      "  batch 2001 loss: 0.25110939494520423\n",
      "Epoch: [0][2000/4400]\tTime 0.00599360466003418 (0.006085285242053045)\tData 0.0007460117340087891 (0.0007254072929965682)\t\n",
      "  batch 4001 loss: 0.2445601435303688\n",
      "Epoch: [0][4000/4400]\tTime 0.005810737609863281 (0.006053785537666334)\tData 0.0006694793701171875 (0.0007168109701204765)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012183801084756851\n",
      "Epoch: [1][0/4400]\tTime 0.0065135955810546875 (0.0065135955810546875)\tData 0.0011260509490966797 (0.0011260509490966797)\t\n",
      "  batch 2001 loss: 0.2472734719440341\n",
      "Epoch: [1][2000/4400]\tTime 0.006289243698120117 (0.006054450010788673)\tData 0.0007345676422119141 (0.0007097547141270063)\t\n",
      "  batch 4001 loss: 0.24817016860842706\n",
      "Epoch: [1][4000/4400]\tTime 0.0058710575103759766 (0.0060453876737534065)\tData 0.0006687641143798828 (0.0007085868103210404)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00012301819026470185\n",
      "Epoch: [2][0/4400]\tTime 0.006758451461791992 (0.006758451461791992)\tData 0.001125335693359375 (0.001125335693359375)\t\n",
      "  batch 2001 loss: 0.2485082032084465\n",
      "Epoch: [2][2000/4400]\tTime 0.006041765213012695 (0.006074590721111307)\tData 0.0007030963897705078 (0.0007075889297630237)\t\n",
      "  batch 4001 loss: 0.24833616990596055\n",
      "Epoch: [2][4000/4400]\tTime 0.007643699645996094 (0.00609174116764388)\tData 0.000736236572265625 (0.0007223303512882155)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012250711768865586\n",
      "Epoch: [3][0/4400]\tTime 0.00645756721496582 (0.00645756721496582)\tData 0.0011250972747802734 (0.0011250972747802734)\t\n",
      "  batch 2001 loss: 0.24837130752950906\n",
      "Epoch: [3][2000/4400]\tTime 0.005834102630615234 (0.006064330500879626)\tData 0.0006778240203857422 (0.0007089262661607429)\t\n",
      "  batch 4001 loss: 0.24846231215447187\n",
      "Epoch: [3][4000/4400]\tTime 0.006003141403198242 (0.006073107870779345)\tData 0.0007207393646240234 (0.0007147651349148492)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001263936311006546\n",
      "Epoch: [4][0/4400]\tTime 0.006554365158081055 (0.006554365158081055)\tData 0.0011105537414550781 (0.0011105537414550781)\t\n",
      "  batch 2001 loss: 0.2483235849738121\n",
      "Epoch: [4][2000/4400]\tTime 0.005991935729980469 (0.006091501044369172)\tData 0.0006864070892333984 (0.0007190094299163894)\t\n",
      "  batch 4001 loss: 0.2481560707911849\n",
      "Epoch: [4][4000/4400]\tTime 0.005963325500488281 (0.0060917956326729476)\tData 0.0006976127624511719 (0.0007172802393807676)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.00012072319537401199\n",
      "Epoch: [5][0/4400]\tTime 0.006374835968017578 (0.006374835968017578)\tData 0.0011913776397705078 (0.0011913776397705078)\t\n",
      "  batch 2001 loss: 0.24794168265908956\n",
      "Epoch: [5][2000/4400]\tTime 0.00667572021484375 (0.006311297237962439)\tData 0.0007922649383544922 (0.0007464360261428124)\t\n",
      "  batch 4001 loss: 0.24808085921406747\n",
      "Epoch: [5][4000/4400]\tTime 0.0060765743255615234 (0.0062348321806457874)\tData 0.0007278919219970703 (0.0007369183385649254)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.00012784627079963685\n",
      "Epoch: [6][0/4400]\tTime 0.006455898284912109 (0.006455898284912109)\tData 0.0011327266693115234 (0.0011327266693115234)\t\n",
      "  batch 2001 loss: 0.24775777729600668\n",
      "Epoch: [6][2000/4400]\tTime 0.00613713264465332 (0.006050984064737956)\tData 0.0007226467132568359 (0.0007250075457037716)\t\n",
      "  batch 4001 loss: 0.24794762079417706\n",
      "Epoch: [6][4000/4400]\tTime 0.005990028381347656 (0.006070580550415222)\tData 0.0006926059722900391 (0.0007202423861312198)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.00012522569298744203\n",
      "Epoch: [7][0/4400]\tTime 0.0064182281494140625 (0.0064182281494140625)\tData 0.0011093616485595703 (0.0011093616485595703)\t\n",
      "  batch 2001 loss: 0.24762469801306725\n",
      "Epoch: [7][2000/4400]\tTime 0.00593876838684082 (0.006055217692400442)\tData 0.0007081031799316406 (0.0007092401065092454)\t\n",
      "  batch 4001 loss: 0.24744096121937037\n",
      "Epoch: [7][4000/4400]\tTime 0.0060465335845947266 (0.006020857137133258)\tData 0.0007002353668212891 (0.0007018920213870482)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.00012460515648126603\n",
      "Epoch: [8][0/4400]\tTime 0.006596803665161133 (0.006596803665161133)\tData 0.0012602806091308594 (0.0012602806091308594)\t\n",
      "  batch 2001 loss: 0.24762380949407817\n",
      "Epoch: [8][2000/4400]\tTime 0.0059947967529296875 (0.006015918422853392)\tData 0.0006706714630126953 (0.0007079082509984021)\t\n",
      "  batch 4001 loss: 0.24770203606039284\n",
      "Epoch: [8][4000/4400]\tTime 0.006403923034667969 (0.006025079666629668)\tData 0.0008275508880615234 (0.0007053171208845737)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.00012409419566392898\n",
      "Epoch: [9][0/4400]\tTime 0.006191730499267578 (0.006191730499267578)\tData 0.0008969306945800781 (0.0008969306945800781)\t\n",
      "  batch 2001 loss: 0.2475745042487979\n",
      "Epoch: [9][2000/4400]\tTime 0.005894899368286133 (0.0059880485896883105)\tData 0.0006823539733886719 (0.0006960415351635095)\t\n",
      "  batch 4001 loss: 0.24758576928824186\n",
      "Epoch: [9][4000/4400]\tTime 0.005818367004394531 (0.006008966539359575)\tData 0.0006935596466064453 (0.0006963302123192041)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "img_model = EmbeddingNetwork(64)\n",
    "img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "txt_model = EmbeddingNetwork(64);\n",
    "txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)\n",
    "\n",
    "train_dataset = EmbeddingDataset(img_train, text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'checkpoints/img-model-full-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'checkpoints/txt-model-full-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lTmZH8KqLWWN",
   "metadata": {
    "id": "lTmZH8KqLWWN"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "GbEwGQz1LWWO",
   "metadata": {
    "id": "GbEwGQz1LWWO",
    "outputId": "35578813-9148-45a2-b398-7da66dc6eb07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0002283381372690201\n",
      "Epoch: [0][0/4400]\tTime 0.007862329483032227 (0.007862329483032227)\tData 0.0012707710266113281 (0.0012707710266113281)\t\n",
      "  batch 2001 loss: 0.46875293447077276\n",
      "Epoch: [0][2000/4400]\tTime 0.005851030349731445 (0.006021185197691986)\tData 0.0006771087646484375 (0.0007055822102681569)\t\n",
      "  batch 4001 loss: 0.4688471542447805\n",
      "Epoch: [0][4000/4400]\tTime 0.005922555923461914 (0.00599160858226758)\tData 0.0006802082061767578 (0.0006984085835268783)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00023445358872413636\n",
      "Epoch: [1][0/4400]\tTime 0.006246089935302734 (0.006246089935302734)\tData 0.0010914802551269531 (0.0010914802551269531)\t\n",
      "  batch 2001 loss: 0.46840228547155854\n",
      "Epoch: [1][2000/4400]\tTime 0.0060079097747802734 (0.0059432157690914676)\tData 0.0006971359252929688 (0.0006883508976789071)\t\n",
      "  batch 4001 loss: 0.46883764205873013\n",
      "Epoch: [1][4000/4400]\tTime 0.005984783172607422 (0.005949453841564328)\tData 0.0006883144378662109 (0.0006881911466789675)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00022934100031852723\n",
      "Epoch: [2][0/4400]\tTime 0.006379365921020508 (0.006379365921020508)\tData 0.0011179447174072266 (0.0011179447174072266)\t\n",
      "  batch 2001 loss: 0.468740251198411\n",
      "Epoch: [2][2000/4400]\tTime 0.005934953689575195 (0.006073041655670578)\tData 0.0006973743438720703 (0.000721685532508404)\t\n",
      "  batch 4001 loss: 0.46797117657959464\n",
      "Epoch: [2][4000/4400]\tTime 0.005935192108154297 (0.006044397470921881)\tData 0.0007023811340332031 (0.0007139043610145437)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0002297530174255371\n",
      "Epoch: [3][0/4400]\tTime 0.00616765022277832 (0.00616765022277832)\tData 0.0008685588836669922 (0.0008685588836669922)\t\n",
      "  batch 2001 loss: 0.46831792177259923\n",
      "Epoch: [3][2000/4400]\tTime 0.006040811538696289 (0.00601495545485924)\tData 0.0006918907165527344 (0.0006994639915683638)\t\n",
      "  batch 4001 loss: 0.46851483649015424\n",
      "Epoch: [3][4000/4400]\tTime 0.0060956478118896484 (0.0060173555839898975)\tData 0.0006649494171142578 (0.0006995770192926927)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0002365802228450775\n",
      "Epoch: [4][0/4400]\tTime 0.0067403316497802734 (0.0067403316497802734)\tData 0.0012664794921875 (0.0012664794921875)\t\n",
      "  batch 2001 loss: 0.4684000356644392\n",
      "Epoch: [4][2000/4400]\tTime 0.005933523178100586 (0.006047239070055426)\tData 0.00075531005859375 (0.0007074346308824957)\t\n",
      "  batch 4001 loss: 0.46808909212052824\n",
      "Epoch: [4][4000/4400]\tTime 0.00616002082824707 (0.006051446699673043)\tData 0.0006778240203857422 (0.0007129738671098522)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "img_model_title = EmbeddingNetwork(64)\n",
    "img_model_title= nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(64);\n",
    "txt_model_title= nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    img_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    txt_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'checkpoints/img-model-title-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'checkpoints/txt-model-title-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Rr1WETBLWWO",
   "metadata": {
    "id": "6Rr1WETBLWWO"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309dac99",
   "metadata": {
    "id": "309dac99",
    "outputId": "3ff40ef0-e0e3-43ff-f3d5-cba64ea6645b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00033422619104385377\n",
      "Epoch: [0][0/4400]\tTime 0.01181483268737793 (0.01181483268737793)\tData 0.0015099048614501953 (0.0015099048614501953)\t\n",
      "  batch 2001 loss: 0.25932632664591077\n",
      "Epoch: [0][2000/4400]\tTime 0.006026506423950195 (0.006081557881528291)\tData 0.0006926059722900391 (0.0007135293770884943)\t\n",
      "  batch 4001 loss: 0.25341323935985566\n",
      "Epoch: [0][4000/4400]\tTime 0.006021738052368164 (0.006072451906840642)\tData 0.0006964206695556641 (0.0007056836574204533)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012086976319551468\n",
      "Epoch: [1][0/4400]\tTime 0.00619816780090332 (0.00619816780090332)\tData 0.0009703636169433594 (0.0009703636169433594)\t\n",
      "  batch 2001 loss: 0.25523896896839143\n",
      "Epoch: [1][2000/4400]\tTime 0.005952358245849609 (0.006008099699425495)\tData 0.0006701946258544922 (0.0006924394009888977)\t\n",
      "  batch 4001 loss: 0.2558343113809824\n",
      "Epoch: [1][4000/4400]\tTime 0.005926609039306641 (0.006048412210969322)\tData 0.0006899833679199219 (0.0006999907509084643)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00013261018693447112\n",
      "Epoch: [2][0/4400]\tTime 0.0063631534576416016 (0.0063631534576416016)\tData 0.0010516643524169922 (0.0010516643524169922)\t\n",
      "  batch 2001 loss: 0.2558107817545533\n",
      "Epoch: [2][2000/4400]\tTime 0.0060160160064697266 (0.0060640198775734205)\tData 0.0006983280181884766 (0.0007227122455999173)\t\n",
      "  batch 4001 loss: 0.25582169550657274\n",
      "Epoch: [2][4000/4400]\tTime 0.0060272216796875 (0.006076699702389924)\tData 0.0007243156433105469 (0.000720299771057907)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012726104259490966\n",
      "Epoch: [3][0/4400]\tTime 0.006685495376586914 (0.006685495376586914)\tData 0.0013127326965332031 (0.0013127326965332031)\t\n",
      "  batch 2001 loss: 0.2555959342867136\n",
      "Epoch: [3][2000/4400]\tTime 0.005988121032714844 (0.006085527115973873)\tData 0.0006749629974365234 (0.0007177547834206676)\t\n",
      "  batch 4001 loss: 0.2552686665803194\n",
      "Epoch: [3][4000/4400]\tTime 0.0067119598388671875 (0.00612659455537498)\tData 0.0010306835174560547 (0.0007261166838341074)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001279204934835434\n",
      "Epoch: [4][0/4400]\tTime 0.006636857986450195 (0.006636857986450195)\tData 0.0013239383697509766 (0.0013239383697509766)\t\n",
      "  batch 2001 loss: 0.2551034790202975\n",
      "Epoch: [4][2000/4400]\tTime 0.006028890609741211 (0.00607319631200025)\tData 0.0007054805755615234 (0.0007131610853203769)\t\n",
      "  batch 4001 loss: 0.2552118890509009\n",
      "Epoch: [4][4000/4400]\tTime 0.0058917999267578125 (0.00626892246207247)\tData 0.0007271766662597656 (0.0007430420789501721)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(64)\n",
    "img_model_ingredients= nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(64)\n",
    "txt_model_ingredients= nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to((f'cuda:{txt_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_ingredients.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    img_model_ingredients = img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    txt_model_ingredients = txt_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_ingredients.state_dict(), 'checkpoints/img-model-ingredients-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'checkpoints/txt-model-ingredients-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GFMxYQRpLWWO",
   "metadata": {
    "id": "GFMxYQRpLWWO"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4069577e",
   "metadata": {
    "id": "4069577e",
    "outputId": "44dd2405-0975-4689-d943-546bd4028134",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0003304431736469269\n",
      "Epoch: [0][0/4400]\tTime 0.007906436920166016 (0.007906436920166016)\tData 0.0015239715576171875 (0.0015239715576171875)\t\n",
      "  batch 2001 loss: 0.25847531884908675\n",
      "Epoch: [0][2000/4400]\tTime 0.006007671356201172 (0.0061836679955234175)\tData 0.0006887912750244141 (0.0007450405446843229)\t\n",
      "  batch 4001 loss: 0.2516851299479604\n",
      "Epoch: [0][4000/4400]\tTime 0.005887031555175781 (0.0061224357869082225)\tData 0.0006945133209228516 (0.0007264040255957739)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0001218315213918686\n",
      "Epoch: [1][0/4400]\tTime 0.006499052047729492 (0.006499052047729492)\tData 0.0012738704681396484 (0.0012738704681396484)\t\n",
      "  batch 2001 loss: 0.2536966943666339\n",
      "Epoch: [1][2000/4400]\tTime 0.006014823913574219 (0.006115863467382824)\tData 0.0006887912750244141 (0.0007232024275261661)\t\n",
      "  batch 4001 loss: 0.2543358924239874\n",
      "Epoch: [1][4000/4400]\tTime 0.005895376205444336 (0.00610050372081052)\tData 0.0006780624389648438 (0.0007214525347201713)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0001300206333398819\n",
      "Epoch: [2][0/4400]\tTime 0.006979227066040039 (0.006979227066040039)\tData 0.0015914440155029297 (0.0015914440155029297)\t\n",
      "  batch 2001 loss: 0.2539842413440347\n",
      "Epoch: [2][2000/4400]\tTime 0.006098508834838867 (0.006128118253838474)\tData 0.0006947517395019531 (0.000728066952927955)\t\n",
      "  batch 4001 loss: 0.2538456128463149\n",
      "Epoch: [2][4000/4400]\tTime 0.005997657775878906 (0.006129255475952637)\tData 0.0007202625274658203 (0.0007271741634903774)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0001231120079755783\n",
      "Epoch: [3][0/4400]\tTime 0.006363868713378906 (0.006363868713378906)\tData 0.0010759830474853516 (0.0010759830474853516)\t\n",
      "  batch 2001 loss: 0.2536744084879756\n",
      "Epoch: [3][2000/4400]\tTime 0.006068706512451172 (0.006047157690800291)\tData 0.0007207393646240234 (0.0007044688753340615)\t\n",
      "  batch 4001 loss: 0.25399095369130376\n",
      "Epoch: [3][4000/4400]\tTime 0.006136894226074219 (0.0060906771927051505)\tData 0.0007338523864746094 (0.0007127098845291424)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.000128129243850708\n",
      "Epoch: [4][0/4400]\tTime 0.006712198257446289 (0.006712198257446289)\tData 0.001438140869140625 (0.001438140869140625)\t\n",
      "  batch 2001 loss: 0.25373134262114766\n",
      "Epoch: [4][2000/4400]\tTime 0.005945920944213867 (0.0061543646721408584)\tData 0.0006897449493408203 (0.0007286368460133337)\t\n",
      "  batch 4001 loss: 0.25382133839279414\n",
      "Epoch: [4][4000/4400]\tTime 0.006006717681884766 (0.006185765923097473)\tData 0.0006799697875976562 (0.0007326071991142229)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_instructions = EmbeddingNetwork(64)\n",
    "img_model_instructions= nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to((f'cuda:{img_model_instructions.device_ids[0]}'));\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(64)\n",
    "txt_model_instructions= nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_instructions.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    img_model_instructions = img_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    txt_model_instructions = txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_instructions.state_dict(), 'checkpoints/img-model-instructions-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'checkpoints/txt-model-instructions-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1e814c",
   "metadata": {},
   "source": [
    "### Model Creation dims = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee70de",
   "metadata": {
    "id": "37c7aeb7"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9cef415",
   "metadata": {
    "id": "gscCkfG1LWWN",
    "outputId": "7ed3a2b0-c885-4100-dd0b-4ad8f4f47242",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0003114860653877258\n",
      "Epoch: [0][0/4400]\tTime 4.029629468917847 (4.029629468917847)\tData 0.0025763511657714844 (0.0025763511657714844)\t\n",
      "  batch 2001 loss: 0.24822020929306746\n",
      "Epoch: [0][2000/4400]\tTime 0.006309986114501953 (0.008277938581597264)\tData 0.0007994174957275391 (0.0007816712180713843)\t\n",
      "  batch 4001 loss: 0.24229517125338315\n",
      "Epoch: [0][4000/4400]\tTime 0.006105184555053711 (0.007278278630186813)\tData 0.0007100105285644531 (0.0007796298858673088)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00011981219798326493\n",
      "Epoch: [1][0/4400]\tTime 0.006433010101318359 (0.006433010101318359)\tData 0.0010907649993896484 (0.0010907649993896484)\t\n",
      "  batch 2001 loss: 0.24500784227997063\n",
      "Epoch: [1][2000/4400]\tTime 0.0060389041900634766 (0.006266078014840846)\tData 0.0006804466247558594 (0.0007230656436536981)\t\n",
      "  batch 4001 loss: 0.24615362104028463\n",
      "Epoch: [1][4000/4400]\tTime 0.006071805953979492 (0.0062487453736236355)\tData 0.0006935596466064453 (0.000730713675302793)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0001221299320459366\n",
      "Epoch: [2][0/4400]\tTime 0.006585597991943359 (0.006585597991943359)\tData 0.0012691020965576172 (0.0012691020965576172)\t\n",
      "  batch 2001 loss: 0.24640856870263814\n",
      "Epoch: [2][2000/4400]\tTime 0.00610041618347168 (0.006196775655636842)\tData 0.0006799697875976562 (0.0007161358962471279)\t\n",
      "  batch 4001 loss: 0.24636031150072812\n",
      "Epoch: [2][4000/4400]\tTime 0.006159782409667969 (0.0062129146544464345)\tData 0.0007123947143554688 (0.000725899955923037)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012254271656274795\n",
      "Epoch: [3][0/4400]\tTime 0.0071866512298583984 (0.0071866512298583984)\tData 0.0010597705841064453 (0.0010597705841064453)\t\n",
      "  batch 2001 loss: 0.24636009442061185\n",
      "Epoch: [3][2000/4400]\tTime 0.006204366683959961 (0.006348580732636306)\tData 0.0007538795471191406 (0.000779909112940783)\t\n",
      "  batch 4001 loss: 0.24631130450218916\n",
      "Epoch: [3][4000/4400]\tTime 0.0061435699462890625 (0.006282054493052457)\tData 0.0007305145263671875 (0.0007688667261132715)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.00012572497129440308\n",
      "Epoch: [4][0/4400]\tTime 0.007004261016845703 (0.007004261016845703)\tData 0.001367807388305664 (0.001367807388305664)\t\n",
      "  batch 2001 loss: 0.2464850318953395\n",
      "Epoch: [4][2000/4400]\tTime 0.006790876388549805 (0.0063855577742439815)\tData 0.0008823871612548828 (0.0007841545364250247)\t\n",
      "  batch 4001 loss: 0.24648765798658132\n",
      "Epoch: [4][4000/4400]\tTime 0.006196498870849609 (0.006302613432602237)\tData 0.0006990432739257812 (0.000759196919043402)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.00011833536624908447\n",
      "Epoch: [5][0/4400]\tTime 0.00652003288269043 (0.00652003288269043)\tData 0.0011141300201416016 (0.0011141300201416016)\t\n",
      "  batch 2001 loss: 0.246048943400383\n",
      "Epoch: [5][2000/4400]\tTime 0.006440877914428711 (0.006223803934367045)\tData 0.0006735324859619141 (0.0007219103680200306)\t\n",
      "  batch 4001 loss: 0.2458462586775422\n",
      "Epoch: [5][4000/4400]\tTime 0.0060160160064697266 (0.006210280728024323)\tData 0.0007266998291015625 (0.0007327086804539405)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.00011944425106048584\n",
      "Epoch: [6][0/4400]\tTime 0.006432056427001953 (0.006432056427001953)\tData 0.0009481906890869141 (0.0009481906890869141)\t\n",
      "  batch 2001 loss: 0.24590941977500916\n",
      "Epoch: [6][2000/4400]\tTime 0.0062067508697509766 (0.006175925408763209)\tData 0.0006792545318603516 (0.0007161911817147456)\t\n",
      "  batch 4001 loss: 0.246103334762156\n",
      "Epoch: [6][4000/4400]\tTime 0.006115436553955078 (0.006164887940040441)\tData 0.0007474422454833984 (0.0007153178298214143)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.00011918193101882934\n",
      "Epoch: [7][0/4400]\tTime 0.006162166595458984 (0.006162166595458984)\tData 0.0008847713470458984 (0.0008847713470458984)\t\n",
      "  batch 2001 loss: 0.24584370613843204\n",
      "Epoch: [7][2000/4400]\tTime 0.006371974945068359 (0.006182626150418138)\tData 0.0007033348083496094 (0.0007378664212129165)\t\n",
      "  batch 4001 loss: 0.24575211997330187\n",
      "Epoch: [7][4000/4400]\tTime 0.006107330322265625 (0.006148810149490521)\tData 0.000701904296875 (0.0007199719082680502)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.00011919386684894562\n",
      "Epoch: [8][0/4400]\tTime 0.006562471389770508 (0.006562471389770508)\tData 0.0012409687042236328 (0.0012409687042236328)\t\n",
      "  batch 2001 loss: 0.24575218838453292\n",
      "Epoch: [8][2000/4400]\tTime 0.005989789962768555 (0.0061112048326880265)\tData 0.0006732940673828125 (0.0006998159598255682)\t\n",
      "  batch 4001 loss: 0.24583167400956155\n",
      "Epoch: [8][4000/4400]\tTime 0.005988359451293945 (0.006164775077058982)\tData 0.0006701946258544922 (0.0007132140137916266)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.0001206599771976471\n",
      "Epoch: [9][0/4400]\tTime 0.00636744499206543 (0.00636744499206543)\tData 0.0010519027709960938 (0.0010519027709960938)\t\n",
      "  batch 2001 loss: 0.24570083506405355\n",
      "Epoch: [9][2000/4400]\tTime 0.0059680938720703125 (0.00614255062047986)\tData 0.0006775856018066406 (0.0007159501418419208)\t\n",
      "  batch 4001 loss: 0.24569915121793748\n",
      "Epoch: [9][4000/4400]\tTime 0.0061798095703125 (0.006167725603808703)\tData 0.0007176399230957031 (0.0007262451593293693)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "img_model = EmbeddingNetwork(128)\n",
    "img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "txt_model = EmbeddingNetwork(128);\n",
    "txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)\n",
    "\n",
    "train_dataset = EmbeddingDataset(img_train, text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'checkpoints/img-model-full-128-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'checkpoints/txt-model-full-128-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5e7a4",
   "metadata": {
    "id": "lTmZH8KqLWWN"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a65a9df4",
   "metadata": {
    "id": "GbEwGQz1LWWO",
    "outputId": "35578813-9148-45a2-b398-7da66dc6eb07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0002283381372690201\n",
      "Epoch: [0][0/4400]\tTime 0.007862329483032227 (0.007862329483032227)\tData 0.0012707710266113281 (0.0012707710266113281)\t\n",
      "  batch 2001 loss: 0.46875293447077276\n",
      "Epoch: [0][2000/4400]\tTime 0.005851030349731445 (0.006021185197691986)\tData 0.0006771087646484375 (0.0007055822102681569)\t\n",
      "  batch 4001 loss: 0.4688471542447805\n",
      "Epoch: [0][4000/4400]\tTime 0.005922555923461914 (0.00599160858226758)\tData 0.0006802082061767578 (0.0006984085835268783)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00023445358872413636\n",
      "Epoch: [1][0/4400]\tTime 0.006246089935302734 (0.006246089935302734)\tData 0.0010914802551269531 (0.0010914802551269531)\t\n",
      "  batch 2001 loss: 0.46840228547155854\n",
      "Epoch: [1][2000/4400]\tTime 0.0060079097747802734 (0.0059432157690914676)\tData 0.0006971359252929688 (0.0006883508976789071)\t\n",
      "  batch 4001 loss: 0.46883764205873013\n",
      "Epoch: [1][4000/4400]\tTime 0.005984783172607422 (0.005949453841564328)\tData 0.0006883144378662109 (0.0006881911466789675)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00022934100031852723\n",
      "Epoch: [2][0/4400]\tTime 0.006379365921020508 (0.006379365921020508)\tData 0.0011179447174072266 (0.0011179447174072266)\t\n",
      "  batch 2001 loss: 0.468740251198411\n",
      "Epoch: [2][2000/4400]\tTime 0.005934953689575195 (0.006073041655670578)\tData 0.0006973743438720703 (0.000721685532508404)\t\n",
      "  batch 4001 loss: 0.46797117657959464\n",
      "Epoch: [2][4000/4400]\tTime 0.005935192108154297 (0.006044397470921881)\tData 0.0007023811340332031 (0.0007139043610145437)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0002297530174255371\n",
      "Epoch: [3][0/4400]\tTime 0.00616765022277832 (0.00616765022277832)\tData 0.0008685588836669922 (0.0008685588836669922)\t\n",
      "  batch 2001 loss: 0.46831792177259923\n",
      "Epoch: [3][2000/4400]\tTime 0.006040811538696289 (0.00601495545485924)\tData 0.0006918907165527344 (0.0006994639915683638)\t\n",
      "  batch 4001 loss: 0.46851483649015424\n",
      "Epoch: [3][4000/4400]\tTime 0.0060956478118896484 (0.0060173555839898975)\tData 0.0006649494171142578 (0.0006995770192926927)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0002365802228450775\n",
      "Epoch: [4][0/4400]\tTime 0.0067403316497802734 (0.0067403316497802734)\tData 0.0012664794921875 (0.0012664794921875)\t\n",
      "  batch 2001 loss: 0.4684000356644392\n",
      "Epoch: [4][2000/4400]\tTime 0.005933523178100586 (0.006047239070055426)\tData 0.00075531005859375 (0.0007074346308824957)\t\n",
      "  batch 4001 loss: 0.46808909212052824\n",
      "Epoch: [4][4000/4400]\tTime 0.00616002082824707 (0.006051446699673043)\tData 0.0006778240203857422 (0.0007129738671098522)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "img_model_title = EmbeddingNetwork(64)\n",
    "img_model_title= nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(64);\n",
    "txt_model_title= nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    img_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    txt_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'checkpoints/img-model-title-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'checkpoints/txt-model-title-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1e097",
   "metadata": {
    "id": "6Rr1WETBLWWO"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a533bfc",
   "metadata": {
    "id": "309dac99",
    "outputId": "3ff40ef0-e0e3-43ff-f3d5-cba64ea6645b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00033422619104385377\n",
      "Epoch: [0][0/4400]\tTime 0.01181483268737793 (0.01181483268737793)\tData 0.0015099048614501953 (0.0015099048614501953)\t\n",
      "  batch 2001 loss: 0.25932632664591077\n",
      "Epoch: [0][2000/4400]\tTime 0.006026506423950195 (0.006081557881528291)\tData 0.0006926059722900391 (0.0007135293770884943)\t\n",
      "  batch 4001 loss: 0.25341323935985566\n",
      "Epoch: [0][4000/4400]\tTime 0.006021738052368164 (0.006072451906840642)\tData 0.0006964206695556641 (0.0007056836574204533)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012086976319551468\n",
      "Epoch: [1][0/4400]\tTime 0.00619816780090332 (0.00619816780090332)\tData 0.0009703636169433594 (0.0009703636169433594)\t\n",
      "  batch 2001 loss: 0.25523896896839143\n",
      "Epoch: [1][2000/4400]\tTime 0.005952358245849609 (0.006008099699425495)\tData 0.0006701946258544922 (0.0006924394009888977)\t\n",
      "  batch 4001 loss: 0.2558343113809824\n",
      "Epoch: [1][4000/4400]\tTime 0.005926609039306641 (0.006048412210969322)\tData 0.0006899833679199219 (0.0006999907509084643)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00013261018693447112\n",
      "Epoch: [2][0/4400]\tTime 0.0063631534576416016 (0.0063631534576416016)\tData 0.0010516643524169922 (0.0010516643524169922)\t\n",
      "  batch 2001 loss: 0.2558107817545533\n",
      "Epoch: [2][2000/4400]\tTime 0.0060160160064697266 (0.0060640198775734205)\tData 0.0006983280181884766 (0.0007227122455999173)\t\n",
      "  batch 4001 loss: 0.25582169550657274\n",
      "Epoch: [2][4000/4400]\tTime 0.0060272216796875 (0.006076699702389924)\tData 0.0007243156433105469 (0.000720299771057907)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012726104259490966\n",
      "Epoch: [3][0/4400]\tTime 0.006685495376586914 (0.006685495376586914)\tData 0.0013127326965332031 (0.0013127326965332031)\t\n",
      "  batch 2001 loss: 0.2555959342867136\n",
      "Epoch: [3][2000/4400]\tTime 0.005988121032714844 (0.006085527115973873)\tData 0.0006749629974365234 (0.0007177547834206676)\t\n",
      "  batch 4001 loss: 0.2552686665803194\n",
      "Epoch: [3][4000/4400]\tTime 0.0067119598388671875 (0.00612659455537498)\tData 0.0010306835174560547 (0.0007261166838341074)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001279204934835434\n",
      "Epoch: [4][0/4400]\tTime 0.006636857986450195 (0.006636857986450195)\tData 0.0013239383697509766 (0.0013239383697509766)\t\n",
      "  batch 2001 loss: 0.2551034790202975\n",
      "Epoch: [4][2000/4400]\tTime 0.006028890609741211 (0.00607319631200025)\tData 0.0007054805755615234 (0.0007131610853203769)\t\n",
      "  batch 4001 loss: 0.2552118890509009\n",
      "Epoch: [4][4000/4400]\tTime 0.0058917999267578125 (0.00626892246207247)\tData 0.0007271766662597656 (0.0007430420789501721)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(64)\n",
    "img_model_ingredients= nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(64)\n",
    "txt_model_ingredients= nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to((f'cuda:{txt_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_ingredients.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    img_model_ingredients = img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    txt_model_ingredients = txt_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_ingredients.state_dict(), 'checkpoints/img-model-ingredients-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'checkpoints/txt-model-ingredients-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GFMxYQRpLWWO",
   "metadata": {
    "id": "GFMxYQRpLWWO"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4069577e",
   "metadata": {
    "id": "4069577e",
    "outputId": "44dd2405-0975-4689-d943-546bd4028134",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0003304431736469269\n",
      "Epoch: [0][0/4400]\tTime 0.007906436920166016 (0.007906436920166016)\tData 0.0015239715576171875 (0.0015239715576171875)\t\n",
      "  batch 2001 loss: 0.25847531884908675\n",
      "Epoch: [0][2000/4400]\tTime 0.006007671356201172 (0.0061836679955234175)\tData 0.0006887912750244141 (0.0007450405446843229)\t\n",
      "  batch 4001 loss: 0.2516851299479604\n",
      "Epoch: [0][4000/4400]\tTime 0.005887031555175781 (0.0061224357869082225)\tData 0.0006945133209228516 (0.0007264040255957739)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0001218315213918686\n",
      "Epoch: [1][0/4400]\tTime 0.006499052047729492 (0.006499052047729492)\tData 0.0012738704681396484 (0.0012738704681396484)\t\n",
      "  batch 2001 loss: 0.2536966943666339\n",
      "Epoch: [1][2000/4400]\tTime 0.006014823913574219 (0.006115863467382824)\tData 0.0006887912750244141 (0.0007232024275261661)\t\n",
      "  batch 4001 loss: 0.2543358924239874\n",
      "Epoch: [1][4000/4400]\tTime 0.005895376205444336 (0.00610050372081052)\tData 0.0006780624389648438 (0.0007214525347201713)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0001300206333398819\n",
      "Epoch: [2][0/4400]\tTime 0.006979227066040039 (0.006979227066040039)\tData 0.0015914440155029297 (0.0015914440155029297)\t\n",
      "  batch 2001 loss: 0.2539842413440347\n",
      "Epoch: [2][2000/4400]\tTime 0.006098508834838867 (0.006128118253838474)\tData 0.0006947517395019531 (0.000728066952927955)\t\n",
      "  batch 4001 loss: 0.2538456128463149\n",
      "Epoch: [2][4000/4400]\tTime 0.005997657775878906 (0.006129255475952637)\tData 0.0007202625274658203 (0.0007271741634903774)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0001231120079755783\n",
      "Epoch: [3][0/4400]\tTime 0.006363868713378906 (0.006363868713378906)\tData 0.0010759830474853516 (0.0010759830474853516)\t\n",
      "  batch 2001 loss: 0.2536744084879756\n",
      "Epoch: [3][2000/4400]\tTime 0.006068706512451172 (0.006047157690800291)\tData 0.0007207393646240234 (0.0007044688753340615)\t\n",
      "  batch 4001 loss: 0.25399095369130376\n",
      "Epoch: [3][4000/4400]\tTime 0.006136894226074219 (0.0060906771927051505)\tData 0.0007338523864746094 (0.0007127098845291424)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.000128129243850708\n",
      "Epoch: [4][0/4400]\tTime 0.006712198257446289 (0.006712198257446289)\tData 0.001438140869140625 (0.001438140869140625)\t\n",
      "  batch 2001 loss: 0.25373134262114766\n",
      "Epoch: [4][2000/4400]\tTime 0.005945920944213867 (0.0061543646721408584)\tData 0.0006897449493408203 (0.0007286368460133337)\t\n",
      "  batch 4001 loss: 0.25382133839279414\n",
      "Epoch: [4][4000/4400]\tTime 0.006006717681884766 (0.006185765923097473)\tData 0.0006799697875976562 (0.0007326071991142229)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_instructions = EmbeddingNetwork(64)\n",
    "img_model_instructions= nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to((f'cuda:{img_model_instructions.device_ids[0]}'));\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(64)\n",
    "txt_model_instructions= nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_instructions.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    img_model_instructions = img_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    txt_model_instructions = txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_instructions.state_dict(), 'checkpoints/img-model-instructions-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'checkpoints/txt-model-instructions-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef8979",
   "metadata": {},
   "source": [
    "### Model Creation dims = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7aeb7",
   "metadata": {
    "id": "37c7aeb7"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "gscCkfG1LWWN",
   "metadata": {
    "id": "gscCkfG1LWWN",
    "outputId": "7ed3a2b0-c885-4100-dd0b-4ad8f4f47242",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00033096739649772646\n",
      "Epoch: [0][0/4400]\tTime 0.009809494018554688 (0.009809494018554688)\tData 0.0020685195922851562 (0.0020685195922851562)\t\n",
      "  batch 2001 loss: 0.25110939494520423\n",
      "Epoch: [0][2000/4400]\tTime 0.00599360466003418 (0.006085285242053045)\tData 0.0007460117340087891 (0.0007254072929965682)\t\n",
      "  batch 4001 loss: 0.2445601435303688\n",
      "Epoch: [0][4000/4400]\tTime 0.005810737609863281 (0.006053785537666334)\tData 0.0006694793701171875 (0.0007168109701204765)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012183801084756851\n",
      "Epoch: [1][0/4400]\tTime 0.0065135955810546875 (0.0065135955810546875)\tData 0.0011260509490966797 (0.0011260509490966797)\t\n",
      "  batch 2001 loss: 0.2472734719440341\n",
      "Epoch: [1][2000/4400]\tTime 0.006289243698120117 (0.006054450010788673)\tData 0.0007345676422119141 (0.0007097547141270063)\t\n",
      "  batch 4001 loss: 0.24817016860842706\n",
      "Epoch: [1][4000/4400]\tTime 0.0058710575103759766 (0.0060453876737534065)\tData 0.0006687641143798828 (0.0007085868103210404)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00012301819026470185\n",
      "Epoch: [2][0/4400]\tTime 0.006758451461791992 (0.006758451461791992)\tData 0.001125335693359375 (0.001125335693359375)\t\n",
      "  batch 2001 loss: 0.2485082032084465\n",
      "Epoch: [2][2000/4400]\tTime 0.006041765213012695 (0.006074590721111307)\tData 0.0007030963897705078 (0.0007075889297630237)\t\n",
      "  batch 4001 loss: 0.24833616990596055\n",
      "Epoch: [2][4000/4400]\tTime 0.007643699645996094 (0.00609174116764388)\tData 0.000736236572265625 (0.0007223303512882155)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012250711768865586\n",
      "Epoch: [3][0/4400]\tTime 0.00645756721496582 (0.00645756721496582)\tData 0.0011250972747802734 (0.0011250972747802734)\t\n",
      "  batch 2001 loss: 0.24837130752950906\n",
      "Epoch: [3][2000/4400]\tTime 0.005834102630615234 (0.006064330500879626)\tData 0.0006778240203857422 (0.0007089262661607429)\t\n",
      "  batch 4001 loss: 0.24846231215447187\n",
      "Epoch: [3][4000/4400]\tTime 0.006003141403198242 (0.006073107870779345)\tData 0.0007207393646240234 (0.0007147651349148492)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001263936311006546\n",
      "Epoch: [4][0/4400]\tTime 0.006554365158081055 (0.006554365158081055)\tData 0.0011105537414550781 (0.0011105537414550781)\t\n",
      "  batch 2001 loss: 0.2483235849738121\n",
      "Epoch: [4][2000/4400]\tTime 0.005991935729980469 (0.006091501044369172)\tData 0.0006864070892333984 (0.0007190094299163894)\t\n",
      "  batch 4001 loss: 0.2481560707911849\n",
      "Epoch: [4][4000/4400]\tTime 0.005963325500488281 (0.0060917956326729476)\tData 0.0006976127624511719 (0.0007172802393807676)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.00012072319537401199\n",
      "Epoch: [5][0/4400]\tTime 0.006374835968017578 (0.006374835968017578)\tData 0.0011913776397705078 (0.0011913776397705078)\t\n",
      "  batch 2001 loss: 0.24794168265908956\n",
      "Epoch: [5][2000/4400]\tTime 0.00667572021484375 (0.006311297237962439)\tData 0.0007922649383544922 (0.0007464360261428124)\t\n",
      "  batch 4001 loss: 0.24808085921406747\n",
      "Epoch: [5][4000/4400]\tTime 0.0060765743255615234 (0.0062348321806457874)\tData 0.0007278919219970703 (0.0007369183385649254)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.00012784627079963685\n",
      "Epoch: [6][0/4400]\tTime 0.006455898284912109 (0.006455898284912109)\tData 0.0011327266693115234 (0.0011327266693115234)\t\n",
      "  batch 2001 loss: 0.24775777729600668\n",
      "Epoch: [6][2000/4400]\tTime 0.00613713264465332 (0.006050984064737956)\tData 0.0007226467132568359 (0.0007250075457037716)\t\n",
      "  batch 4001 loss: 0.24794762079417706\n",
      "Epoch: [6][4000/4400]\tTime 0.005990028381347656 (0.006070580550415222)\tData 0.0006926059722900391 (0.0007202423861312198)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.00012522569298744203\n",
      "Epoch: [7][0/4400]\tTime 0.0064182281494140625 (0.0064182281494140625)\tData 0.0011093616485595703 (0.0011093616485595703)\t\n",
      "  batch 2001 loss: 0.24762469801306725\n",
      "Epoch: [7][2000/4400]\tTime 0.00593876838684082 (0.006055217692400442)\tData 0.0007081031799316406 (0.0007092401065092454)\t\n",
      "  batch 4001 loss: 0.24744096121937037\n",
      "Epoch: [7][4000/4400]\tTime 0.0060465335845947266 (0.006020857137133258)\tData 0.0007002353668212891 (0.0007018920213870482)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.00012460515648126603\n",
      "Epoch: [8][0/4400]\tTime 0.006596803665161133 (0.006596803665161133)\tData 0.0012602806091308594 (0.0012602806091308594)\t\n",
      "  batch 2001 loss: 0.24762380949407817\n",
      "Epoch: [8][2000/4400]\tTime 0.0059947967529296875 (0.006015918422853392)\tData 0.0006706714630126953 (0.0007079082509984021)\t\n",
      "  batch 4001 loss: 0.24770203606039284\n",
      "Epoch: [8][4000/4400]\tTime 0.006403923034667969 (0.006025079666629668)\tData 0.0008275508880615234 (0.0007053171208845737)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.00012409419566392898\n",
      "Epoch: [9][0/4400]\tTime 0.006191730499267578 (0.006191730499267578)\tData 0.0008969306945800781 (0.0008969306945800781)\t\n",
      "  batch 2001 loss: 0.2475745042487979\n",
      "Epoch: [9][2000/4400]\tTime 0.005894899368286133 (0.0059880485896883105)\tData 0.0006823539733886719 (0.0006960415351635095)\t\n",
      "  batch 4001 loss: 0.24758576928824186\n",
      "Epoch: [9][4000/4400]\tTime 0.005818367004394531 (0.006008966539359575)\tData 0.0006935596466064453 (0.0006963302123192041)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "img_model = EmbeddingNetwork(64)\n",
    "img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "txt_model = EmbeddingNetwork(64);\n",
    "txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)\n",
    "\n",
    "train_dataset = EmbeddingDataset(img_train, text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'checkpoints/img-model-full-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'checkpoints/txt-model-full-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lTmZH8KqLWWN",
   "metadata": {
    "id": "lTmZH8KqLWWN"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "GbEwGQz1LWWO",
   "metadata": {
    "id": "GbEwGQz1LWWO",
    "outputId": "35578813-9148-45a2-b398-7da66dc6eb07",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0002283381372690201\n",
      "Epoch: [0][0/4400]\tTime 0.007862329483032227 (0.007862329483032227)\tData 0.0012707710266113281 (0.0012707710266113281)\t\n",
      "  batch 2001 loss: 0.46875293447077276\n",
      "Epoch: [0][2000/4400]\tTime 0.005851030349731445 (0.006021185197691986)\tData 0.0006771087646484375 (0.0007055822102681569)\t\n",
      "  batch 4001 loss: 0.4688471542447805\n",
      "Epoch: [0][4000/4400]\tTime 0.005922555923461914 (0.00599160858226758)\tData 0.0006802082061767578 (0.0006984085835268783)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00023445358872413636\n",
      "Epoch: [1][0/4400]\tTime 0.006246089935302734 (0.006246089935302734)\tData 0.0010914802551269531 (0.0010914802551269531)\t\n",
      "  batch 2001 loss: 0.46840228547155854\n",
      "Epoch: [1][2000/4400]\tTime 0.0060079097747802734 (0.0059432157690914676)\tData 0.0006971359252929688 (0.0006883508976789071)\t\n",
      "  batch 4001 loss: 0.46883764205873013\n",
      "Epoch: [1][4000/4400]\tTime 0.005984783172607422 (0.005949453841564328)\tData 0.0006883144378662109 (0.0006881911466789675)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00022934100031852723\n",
      "Epoch: [2][0/4400]\tTime 0.006379365921020508 (0.006379365921020508)\tData 0.0011179447174072266 (0.0011179447174072266)\t\n",
      "  batch 2001 loss: 0.468740251198411\n",
      "Epoch: [2][2000/4400]\tTime 0.005934953689575195 (0.006073041655670578)\tData 0.0006973743438720703 (0.000721685532508404)\t\n",
      "  batch 4001 loss: 0.46797117657959464\n",
      "Epoch: [2][4000/4400]\tTime 0.005935192108154297 (0.006044397470921881)\tData 0.0007023811340332031 (0.0007139043610145437)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0002297530174255371\n",
      "Epoch: [3][0/4400]\tTime 0.00616765022277832 (0.00616765022277832)\tData 0.0008685588836669922 (0.0008685588836669922)\t\n",
      "  batch 2001 loss: 0.46831792177259923\n",
      "Epoch: [3][2000/4400]\tTime 0.006040811538696289 (0.00601495545485924)\tData 0.0006918907165527344 (0.0006994639915683638)\t\n",
      "  batch 4001 loss: 0.46851483649015424\n",
      "Epoch: [3][4000/4400]\tTime 0.0060956478118896484 (0.0060173555839898975)\tData 0.0006649494171142578 (0.0006995770192926927)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0002365802228450775\n",
      "Epoch: [4][0/4400]\tTime 0.0067403316497802734 (0.0067403316497802734)\tData 0.0012664794921875 (0.0012664794921875)\t\n",
      "  batch 2001 loss: 0.4684000356644392\n",
      "Epoch: [4][2000/4400]\tTime 0.005933523178100586 (0.006047239070055426)\tData 0.00075531005859375 (0.0007074346308824957)\t\n",
      "  batch 4001 loss: 0.46808909212052824\n",
      "Epoch: [4][4000/4400]\tTime 0.00616002082824707 (0.006051446699673043)\tData 0.0006778240203857422 (0.0007129738671098522)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "img_model_title = EmbeddingNetwork(64)\n",
    "img_model_title= nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(64);\n",
    "txt_model_title= nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    img_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "    txt_model_title = txt_model_title.to((f'cuda:{txt_model_title.device_ids[0]}'))\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'checkpoints/img-model-title-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'checkpoints/txt-model-title-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Rr1WETBLWWO",
   "metadata": {
    "id": "6Rr1WETBLWWO"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "309dac99",
   "metadata": {
    "id": "309dac99",
    "outputId": "3ff40ef0-e0e3-43ff-f3d5-cba64ea6645b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.00033422619104385377\n",
      "Epoch: [0][0/4400]\tTime 0.01181483268737793 (0.01181483268737793)\tData 0.0015099048614501953 (0.0015099048614501953)\t\n",
      "  batch 2001 loss: 0.25932632664591077\n",
      "Epoch: [0][2000/4400]\tTime 0.006026506423950195 (0.006081557881528291)\tData 0.0006926059722900391 (0.0007135293770884943)\t\n",
      "  batch 4001 loss: 0.25341323935985566\n",
      "Epoch: [0][4000/4400]\tTime 0.006021738052368164 (0.006072451906840642)\tData 0.0006964206695556641 (0.0007056836574204533)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00012086976319551468\n",
      "Epoch: [1][0/4400]\tTime 0.00619816780090332 (0.00619816780090332)\tData 0.0009703636169433594 (0.0009703636169433594)\t\n",
      "  batch 2001 loss: 0.25523896896839143\n",
      "Epoch: [1][2000/4400]\tTime 0.005952358245849609 (0.006008099699425495)\tData 0.0006701946258544922 (0.0006924394009888977)\t\n",
      "  batch 4001 loss: 0.2558343113809824\n",
      "Epoch: [1][4000/4400]\tTime 0.005926609039306641 (0.006048412210969322)\tData 0.0006899833679199219 (0.0006999907509084643)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00013261018693447112\n",
      "Epoch: [2][0/4400]\tTime 0.0063631534576416016 (0.0063631534576416016)\tData 0.0010516643524169922 (0.0010516643524169922)\t\n",
      "  batch 2001 loss: 0.2558107817545533\n",
      "Epoch: [2][2000/4400]\tTime 0.0060160160064697266 (0.0060640198775734205)\tData 0.0006983280181884766 (0.0007227122455999173)\t\n",
      "  batch 4001 loss: 0.25582169550657274\n",
      "Epoch: [2][4000/4400]\tTime 0.0060272216796875 (0.006076699702389924)\tData 0.0007243156433105469 (0.000720299771057907)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00012726104259490966\n",
      "Epoch: [3][0/4400]\tTime 0.006685495376586914 (0.006685495376586914)\tData 0.0013127326965332031 (0.0013127326965332031)\t\n",
      "  batch 2001 loss: 0.2555959342867136\n",
      "Epoch: [3][2000/4400]\tTime 0.005988121032714844 (0.006085527115973873)\tData 0.0006749629974365234 (0.0007177547834206676)\t\n",
      "  batch 4001 loss: 0.2552686665803194\n",
      "Epoch: [3][4000/4400]\tTime 0.0067119598388671875 (0.00612659455537498)\tData 0.0010306835174560547 (0.0007261166838341074)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001279204934835434\n",
      "Epoch: [4][0/4400]\tTime 0.006636857986450195 (0.006636857986450195)\tData 0.0013239383697509766 (0.0013239383697509766)\t\n",
      "  batch 2001 loss: 0.2551034790202975\n",
      "Epoch: [4][2000/4400]\tTime 0.006028890609741211 (0.00607319631200025)\tData 0.0007054805755615234 (0.0007131610853203769)\t\n",
      "  batch 4001 loss: 0.2552118890509009\n",
      "Epoch: [4][4000/4400]\tTime 0.0058917999267578125 (0.00626892246207247)\tData 0.0007271766662597656 (0.0007430420789501721)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(64)\n",
    "img_model_ingredients= nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(64)\n",
    "txt_model_ingredients= nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to((f'cuda:{txt_model_ingredients.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_ingredients.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    img_model_ingredients = img_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "    txt_model_ingredients = txt_model_ingredients.to((f'cuda:{img_model_ingredients.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_ingredients.state_dict(), 'checkpoints/img-model-ingredients-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'checkpoints/txt-model-ingredients-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GFMxYQRpLWWO",
   "metadata": {
    "id": "GFMxYQRpLWWO"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4069577e",
   "metadata": {
    "id": "4069577e",
    "outputId": "44dd2405-0975-4689-d943-546bd4028134",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0003304431736469269\n",
      "Epoch: [0][0/4400]\tTime 0.007906436920166016 (0.007906436920166016)\tData 0.0015239715576171875 (0.0015239715576171875)\t\n",
      "  batch 2001 loss: 0.25847531884908675\n",
      "Epoch: [0][2000/4400]\tTime 0.006007671356201172 (0.0061836679955234175)\tData 0.0006887912750244141 (0.0007450405446843229)\t\n",
      "  batch 4001 loss: 0.2516851299479604\n",
      "Epoch: [0][4000/4400]\tTime 0.005887031555175781 (0.0061224357869082225)\tData 0.0006945133209228516 (0.0007264040255957739)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0001218315213918686\n",
      "Epoch: [1][0/4400]\tTime 0.006499052047729492 (0.006499052047729492)\tData 0.0012738704681396484 (0.0012738704681396484)\t\n",
      "  batch 2001 loss: 0.2536966943666339\n",
      "Epoch: [1][2000/4400]\tTime 0.006014823913574219 (0.006115863467382824)\tData 0.0006887912750244141 (0.0007232024275261661)\t\n",
      "  batch 4001 loss: 0.2543358924239874\n",
      "Epoch: [1][4000/4400]\tTime 0.005895376205444336 (0.00610050372081052)\tData 0.0006780624389648438 (0.0007214525347201713)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0001300206333398819\n",
      "Epoch: [2][0/4400]\tTime 0.006979227066040039 (0.006979227066040039)\tData 0.0015914440155029297 (0.0015914440155029297)\t\n",
      "  batch 2001 loss: 0.2539842413440347\n",
      "Epoch: [2][2000/4400]\tTime 0.006098508834838867 (0.006128118253838474)\tData 0.0006947517395019531 (0.000728066952927955)\t\n",
      "  batch 4001 loss: 0.2538456128463149\n",
      "Epoch: [2][4000/4400]\tTime 0.005997657775878906 (0.006129255475952637)\tData 0.0007202625274658203 (0.0007271741634903774)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0001231120079755783\n",
      "Epoch: [3][0/4400]\tTime 0.006363868713378906 (0.006363868713378906)\tData 0.0010759830474853516 (0.0010759830474853516)\t\n",
      "  batch 2001 loss: 0.2536744084879756\n",
      "Epoch: [3][2000/4400]\tTime 0.006068706512451172 (0.006047157690800291)\tData 0.0007207393646240234 (0.0007044688753340615)\t\n",
      "  batch 4001 loss: 0.25399095369130376\n",
      "Epoch: [3][4000/4400]\tTime 0.006136894226074219 (0.0060906771927051505)\tData 0.0007338523864746094 (0.0007127098845291424)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.000128129243850708\n",
      "Epoch: [4][0/4400]\tTime 0.006712198257446289 (0.006712198257446289)\tData 0.001438140869140625 (0.001438140869140625)\t\n",
      "  batch 2001 loss: 0.25373134262114766\n",
      "Epoch: [4][2000/4400]\tTime 0.005945920944213867 (0.0061543646721408584)\tData 0.0006897449493408203 (0.0007286368460133337)\t\n",
      "  batch 4001 loss: 0.25382133839279414\n",
      "Epoch: [4][4000/4400]\tTime 0.006006717681884766 (0.006185765923097473)\tData 0.0006799697875976562 (0.0007326071991142229)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "img_model_instructions = EmbeddingNetwork(64)\n",
    "img_model_instructions= nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to((f'cuda:{img_model_instructions.device_ids[0]}'));\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(64)\n",
    "txt_model_instructions= nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'));\n",
    "\n",
    "optimizer = torch.optim.Adam(img_model_instructions.parameters(), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    img_model_instructions = img_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "    txt_model_instructions = txt_model_instructions.to((f'cuda:{txt_model_instructions.device_ids[0]}'))\n",
    "\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    train(ingredients_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "\n",
    "torch.save(img_model_instructions.state_dict(), 'checkpoints/img-model-instructions-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'checkpoints/txt-model-instructions-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96b882",
   "metadata": {
    "id": "ad96b882"
   },
   "source": [
    "### Dimensional Analysis. dims = 256 / 512 / 128 / 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296c17c0",
   "metadata": {
    "id": "296c17c0"
   },
   "source": [
    "#### 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8a4ffb",
   "metadata": {
    "id": "fa8a4ffb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# im2recipe 512\n",
    "img_model_full_512 = EmbeddingNetwork(64)\n",
    "img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"checkpoints/img-model-full-64-epoch-10.pth\"))\n",
    "img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(64)\n",
    "txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"checkpoints/txt-model-full-64-epoch-10.pth\"))\n",
    "txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "#im2title 512\n",
    "img_model_title_512 = EmbeddingNetwork(64)\n",
    "img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"checkpoints/img-model-title-64-epoch-5.pth\"))\n",
    "img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(64)\n",
    "txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"checkpoints/txt-model-title-64-epoch-5.pth\"))\n",
    "txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "#im2instructions 512\n",
    "img_model_instructions_512 = EmbeddingNetwork(64)\n",
    "img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"checkpoints/img-model-instructions-64-epoch-5.pth\"))\n",
    "img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(64)\n",
    "txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"checkpoints/txt-model-instructions-64-epoch-5.pth\"))\n",
    "txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 512\n",
    "img_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/img-model-ingredients-64-epoch-5.pth\"))\n",
    "img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/txt-model-ingredients-64-epoch-5.pth\"))\n",
    "txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cf32bae",
   "metadata": {
    "id": "4cf32bae",
    "outputId": "cea10112-e0c6-48e3-d07c-b8e20e61947d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 512 and sample = 1000\n",
      "Mean median 51.3\n",
      "Recall {1: 0.056499999999999995, 5: 0.1654, 10: 0.2392}\n",
      "Running im2recipe for dims = 512 and sample = 10000\n",
      "Mean median 510.75\n",
      "Recall {1: 0.009409999999999998, 5: 0.03598, 10: 0.05913}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20628288",
   "metadata": {
    "id": "20628288",
    "outputId": "718fdc4c-9dee-42de-b18a-bf51f33a2b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 512 and sample = 1000\n",
      "Mean median 4.9\n",
      "Recall {1: 0.27979999999999994, 5: 0.5205, 10: 0.6196999999999999}\n",
      "Running im2title for dims = 512 and sample = 10000\n",
      "Mean median 38.7\n",
      "Recall {1: 0.09568999999999998, 5: 0.24392, 10: 0.32765999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f66231",
   "metadata": {
    "id": "13f66231",
    "outputId": "02314dba-10af-4109-9845-673a9cdab1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 512 and sample = 1000\n",
      "Mean median 84.6\n",
      "Recall {1: 0.0372, 5: 0.11200000000000002, 10: 0.1706}\n",
      "Running im2ingredients for dims = 512 and sample = 10000\n",
      "Mean median 828.8\n",
      "Recall {1: 0.00601, 5: 0.022809999999999997, 10: 0.038959999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac0916c",
   "metadata": {
    "id": "5ac0916c",
    "outputId": "5d5d9cf4-9708-4921-fd53-8388d275b7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 512 and sample = 1000\n",
      "Mean median 84.05\n",
      "Recall {1: 0.028300000000000002, 5: 0.09469999999999999, 10: 0.1502}\n",
      "Running im2instructions for dims = 512 and sample = 10000\n",
      "Mean median 835.5\n",
      "Recall {1: 0.005619999999999999, 5: 0.02037, 10: 0.033839999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c60c319",
   "metadata": {
    "id": "5c60c319"
   },
   "source": [
    "#### 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fddd89",
   "metadata": {
    "id": "f8fddd89"
   },
   "outputs": [],
   "source": [
    "\n",
    "# im2recipe 256\n",
    "img_model_full_512 = EmbeddingNetwork(256)\n",
    "img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"checkpoints/img-model-full-256-epoch-10.pth\"))\n",
    "img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(256)\n",
    "txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"checkpoints/txt-model-full-256-epoch-10.pth\"))\n",
    "txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "#im2title 256\n",
    "img_model_title_512 = EmbeddingNetwork(256)\n",
    "img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"checkpoints/img-model-title-256-epoch-5.pth\"))\n",
    "img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(256)\n",
    "txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"checkpoints/txt-model-title-256-epoch-5.pth\"))\n",
    "txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "#im2instructions 256\n",
    "img_model_instructions_512 = EmbeddingNetwork(256)\n",
    "img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"checkpoints/img-model-instructions-256-epoch-5.pth\"))\n",
    "img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(256)\n",
    "txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"checkpoints/txt-model-instructions-256-epoch-5.pth\"))\n",
    "txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 256\n",
    "img_model_ingredients_512 = EmbeddingNetwork(256)\n",
    "img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/img-model-ingredients-256-epoch-5.pth\"))\n",
    "img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(256)\n",
    "txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/txt-model-ingredients-256-epoch-5.pth\"))\n",
    "txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1PP2zePpLWWQ",
   "metadata": {
    "id": "1PP2zePpLWWQ",
    "outputId": "5bc51443-60d9-44bb-90b5-847c1e1dd23f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 256 and sample = 1000\n",
      "Mean median 6.95\n",
      "Recall {1: 0.2076, 5: 0.4608, 10: 0.571}\n",
      "Running im2recipe for dims = 256 and sample = 10000\n",
      "Mean median 59.2\n",
      "Recall {1: 0.05, 5: 0.16153, 10: 0.24162}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dvv2Xs86LWWQ",
   "metadata": {
    "id": "dvv2Xs86LWWQ",
    "outputId": "ecc25cc9-529e-468e-e3a1-7395411303ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 256 and sample = 1000\n",
      "Mean median 2.0\n",
      "Recall {1: 0.45620000000000005, 5: 0.7178, 10: 0.7921000000000001}\n",
      "Running im2title for dims = 256 and sample = 10000\n",
      "Mean median 9.1\n",
      "Recall {1: 0.19510000000000002, 5: 0.41746999999999995, 10: 0.52024}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nARtvbqcLWWR",
   "metadata": {
    "id": "nARtvbqcLWWR",
    "outputId": "cc8b627f-75e8-40bf-a01c-8b782a51f8a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 256 and sample = 1000\n",
      "Mean median 13.3\n",
      "Recall {1: 0.09599999999999999, 5: 0.2969, 10: 0.4372}\n",
      "Running im2ingredients for dims = 256 and sample = 10000\n",
      "Mean median 127.05\n",
      "Recall {1: 0.015189999999999999, 5: 0.06169, 10: 0.10497000000000001}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7O98Q2NQLWWR",
   "metadata": {
    "id": "7O98Q2NQLWWR",
    "outputId": "ce59d651-dcda-4840-8795-609f1ef72ce2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 256 and sample = 1000\n",
      "Mean median 13.7\n",
      "Recall {1: 0.1041, 5: 0.3077, 10: 0.44000000000000006}\n",
      "Running im2instructions for dims = 256 and sample = 10000\n",
      "Mean median 129.0\n",
      "Recall {1: 0.01961, 5: 0.07319, 10: 0.12057999999999999}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfd7217",
   "metadata": {},
   "source": [
    "#### 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8a4ffb",
   "metadata": {
    "id": "fa8a4ffb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# im2recipe 512\n",
    "img_model_full_512 = EmbeddingNetwork(64)\n",
    "img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"checkpoints/img-model-full-64-epoch-10.pth\"))\n",
    "img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(64)\n",
    "txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"checkpoints/txt-model-full-64-epoch-10.pth\"))\n",
    "txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "#im2title 512\n",
    "img_model_title_512 = EmbeddingNetwork(64)\n",
    "img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"checkpoints/img-model-title-64-epoch-5.pth\"))\n",
    "img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(64)\n",
    "txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"checkpoints/txt-model-title-64-epoch-5.pth\"))\n",
    "txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "#im2instructions 512\n",
    "img_model_instructions_512 = EmbeddingNetwork(64)\n",
    "img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"checkpoints/img-model-instructions-64-epoch-5.pth\"))\n",
    "img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(64)\n",
    "txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"checkpoints/txt-model-instructions-64-epoch-5.pth\"))\n",
    "txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 512\n",
    "img_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/img-model-ingredients-64-epoch-5.pth\"))\n",
    "img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/txt-model-ingredients-64-epoch-5.pth\"))\n",
    "txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cf32bae",
   "metadata": {
    "id": "4cf32bae",
    "outputId": "cea10112-e0c6-48e3-d07c-b8e20e61947d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 512 and sample = 1000\n",
      "Mean median 51.3\n",
      "Recall {1: 0.056499999999999995, 5: 0.1654, 10: 0.2392}\n",
      "Running im2recipe for dims = 512 and sample = 10000\n",
      "Mean median 510.75\n",
      "Recall {1: 0.009409999999999998, 5: 0.03598, 10: 0.05913}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20628288",
   "metadata": {
    "id": "20628288",
    "outputId": "718fdc4c-9dee-42de-b18a-bf51f33a2b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 512 and sample = 1000\n",
      "Mean median 4.9\n",
      "Recall {1: 0.27979999999999994, 5: 0.5205, 10: 0.6196999999999999}\n",
      "Running im2title for dims = 512 and sample = 10000\n",
      "Mean median 38.7\n",
      "Recall {1: 0.09568999999999998, 5: 0.24392, 10: 0.32765999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f66231",
   "metadata": {
    "id": "13f66231",
    "outputId": "02314dba-10af-4109-9845-673a9cdab1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 512 and sample = 1000\n",
      "Mean median 84.6\n",
      "Recall {1: 0.0372, 5: 0.11200000000000002, 10: 0.1706}\n",
      "Running im2ingredients for dims = 512 and sample = 10000\n",
      "Mean median 828.8\n",
      "Recall {1: 0.00601, 5: 0.022809999999999997, 10: 0.038959999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac0916c",
   "metadata": {
    "id": "5ac0916c",
    "outputId": "5d5d9cf4-9708-4921-fd53-8388d275b7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 512 and sample = 1000\n",
      "Mean median 84.05\n",
      "Recall {1: 0.028300000000000002, 5: 0.09469999999999999, 10: 0.1502}\n",
      "Running im2instructions for dims = 512 and sample = 10000\n",
      "Mean median 835.5\n",
      "Recall {1: 0.005619999999999999, 5: 0.02037, 10: 0.033839999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e1f62",
   "metadata": {},
   "source": [
    "#### 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa8a4ffb",
   "metadata": {
    "id": "fa8a4ffb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# im2recipe 512\n",
    "img_model_full_512 = EmbeddingNetwork(64)\n",
    "img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"checkpoints/img-model-full-64-epoch-10.pth\"))\n",
    "img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(64)\n",
    "txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"checkpoints/txt-model-full-64-epoch-10.pth\"))\n",
    "txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "#im2title 512\n",
    "img_model_title_512 = EmbeddingNetwork(64)\n",
    "img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"checkpoints/img-model-title-64-epoch-5.pth\"))\n",
    "img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(64)\n",
    "txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"checkpoints/txt-model-title-64-epoch-5.pth\"))\n",
    "txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "#im2instructions 512\n",
    "img_model_instructions_512 = EmbeddingNetwork(64)\n",
    "img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"checkpoints/img-model-instructions-64-epoch-5.pth\"))\n",
    "img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(64)\n",
    "txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"checkpoints/txt-model-instructions-64-epoch-5.pth\"))\n",
    "txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 512\n",
    "img_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/img-model-ingredients-64-epoch-5.pth\"))\n",
    "img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(64)\n",
    "txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/txt-model-ingredients-64-epoch-5.pth\"))\n",
    "txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cf32bae",
   "metadata": {
    "id": "4cf32bae",
    "outputId": "cea10112-e0c6-48e3-d07c-b8e20e61947d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 512 and sample = 1000\n",
      "Mean median 51.3\n",
      "Recall {1: 0.056499999999999995, 5: 0.1654, 10: 0.2392}\n",
      "Running im2recipe for dims = 512 and sample = 10000\n",
      "Mean median 510.75\n",
      "Recall {1: 0.009409999999999998, 5: 0.03598, 10: 0.05913}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "20628288",
   "metadata": {
    "id": "20628288",
    "outputId": "718fdc4c-9dee-42de-b18a-bf51f33a2b8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 512 and sample = 1000\n",
      "Mean median 4.9\n",
      "Recall {1: 0.27979999999999994, 5: 0.5205, 10: 0.6196999999999999}\n",
      "Running im2title for dims = 512 and sample = 10000\n",
      "Mean median 38.7\n",
      "Recall {1: 0.09568999999999998, 5: 0.24392, 10: 0.32765999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "13f66231",
   "metadata": {
    "id": "13f66231",
    "outputId": "02314dba-10af-4109-9845-673a9cdab1ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 512 and sample = 1000\n",
      "Mean median 84.6\n",
      "Recall {1: 0.0372, 5: 0.11200000000000002, 10: 0.1706}\n",
      "Running im2ingredients for dims = 512 and sample = 10000\n",
      "Mean median 828.8\n",
      "Recall {1: 0.00601, 5: 0.022809999999999997, 10: 0.038959999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac0916c",
   "metadata": {
    "id": "5ac0916c",
    "outputId": "5d5d9cf4-9708-4921-fd53-8388d275b7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 512 and sample = 1000\n",
      "Mean median 84.05\n",
      "Recall {1: 0.028300000000000002, 5: 0.09469999999999999, 10: 0.1502}\n",
      "Running im2instructions for dims = 512 and sample = 10000\n",
      "Mean median 835.5\n",
      "Recall {1: 0.005619999999999999, 5: 0.02037, 10: 0.033839999999999995}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_val[i], 0))).cpu().detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(text_val[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa626f8",
   "metadata": {
    "id": "6fa626f8"
   },
   "source": [
    "### Evaluation and Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30c8a2a",
   "metadata": {
    "id": "b30c8a2a"
   },
   "source": [
    " We can see that dimensions = 512 has a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1MWUKqEeLWWR",
   "metadata": {
    "id": "1MWUKqEeLWWR"
   },
   "outputs": [],
   "source": [
    "# im2recipe 512\n",
    "img_model_full_512 = EmbeddingNetwork(512)\n",
    "img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"checkpoints/img-model-full-512-epoch-10.pth\"))\n",
    "img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(512)\n",
    "txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"checkpoints/txt-model-full-512-epoch-10.pth\"))\n",
    "txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "#im2title 512\n",
    "img_model_title_512 = EmbeddingNetwork(512)\n",
    "img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"checkpoints/img-model-title-512-epoch-5.pth\"))\n",
    "img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(512)\n",
    "txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"checkpoints/txt-model-title-512-epoch-5.pth\"))\n",
    "txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "#im2instructions 512\n",
    "img_model_instructions_512 = EmbeddingNetwork(512)\n",
    "img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"checkpoints/img-model-instructions-512-epoch-5.pth\"))\n",
    "img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(512)\n",
    "txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"checkpoints/txt-model-instructions-512-epoch-5.pth\"))\n",
    "txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 512\n",
    "img_model_ingredients_512 = EmbeddingNetwork(512)\n",
    "img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/img-model-ingredients-512-epoch-5.pth\"))\n",
    "img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(512)\n",
    "txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"checkpoints/txt-model-ingredients-512-epoch-5.pth\"))\n",
    "txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmYQP27-LWWR",
   "metadata": {
    "id": "mmYQP27-LWWR",
    "outputId": "20fd1bff-b618-49f8-eeb4-b6f954f35232"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 512 and sample = 1000\n",
      "Mean median 5.2\n",
      "Recall {1: 0.23480000000000004, 5: 0.5183000000000001, 10: 0.6353}\n",
      "Running im2recipe for dims = 512 and sample = 10000\n",
      "Mean median 43.1\n",
      "Recall {1: 0.06054999999999999, 5: 0.18576, 10: 0.27418}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_test[i], 0))).cpu().detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_test[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 512 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 512 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O_SmRhf-LWWS",
   "metadata": {
    "id": "O_SmRhf-LWWS",
    "outputId": "32359cd4-96fe-47b0-8ca2-99ef148eb36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 512 and sample = 1000\n",
      "Mean median 24.6\n",
      "Recall {1: 0.12349999999999998, 5: 0.29239999999999994, 10: 0.3778}\n",
      "Running im2title for dims = 512 and sample = 10000\n",
      "Mean median 247.35\n",
      "Recall {1: 0.03185, 5: 0.09637999999999998, 10: 0.14421}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_test[i], 0))).cpu().detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(title_test[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 512 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 512 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zdH1bFQ_LWWS",
   "metadata": {
    "id": "zdH1bFQ_LWWS",
    "outputId": "ef7b6bcf-c96c-4b42-beab-5bae641f772b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 512 and sample = 1000\n",
      "Mean median 15.2\n",
      "Recall {1: 0.08979999999999998, 5: 0.2829, 10: 0.41550000000000004}\n",
      "Running im2ingredients for dims = 512 and sample = 10000\n",
      "Mean median 141.25\n",
      "Recall {1: 0.014410000000000001, 5: 0.05796, 10: 0.10085999999999999}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_test[i], 0))).cpu().detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(ingredients_test[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QEmHUX9sLWWS",
   "metadata": {
    "id": "QEmHUX9sLWWS",
    "outputId": "223fe7bf-35d6-458d-a5c0-e9474716f9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 512 and sample = 1000\n",
      "Mean median 21.85\n",
      "Recall {1: 0.077, 5: 0.2365, 10: 0.35269999999999996}\n",
      "Running im2instructions for dims = 512 and sample = 10000\n",
      "Mean median 204.25\n",
      "Recall {1: 0.012629999999999999, 5: 0.04911, 10: 0.08324000000000001}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 512))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_test[i], 0))).cpu().detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(instructions_test[i], 0))).cpu().detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 512 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 512 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb162482",
   "metadata": {
    "id": "eb162482"
   },
   "source": [
    "## 2.2 Non-Linear Embeddings - Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6OX86xLvSBQP",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649034476871,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "6OX86xLvSBQP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7XxJ79aiSBQV",
   "metadata": {
    "executionInfo": {
     "elapsed": 482,
     "status": "ok",
     "timestamp": 1649034477531,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "7XxJ79aiSBQV"
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6LRrBHazSBQW",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649034477532,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "6LRrBHazSBQW"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HM8VRH7HiStf",
   "metadata": {
    "id": "HM8VRH7HiStf"
   },
   "source": [
    "### Negative Training Data Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "vxPhbgr7iRd9",
   "metadata": {
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1649034477908,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "vxPhbgr7iRd9"
   },
   "outputs": [],
   "source": [
    "indices = list(range(0, len(text_train)))\n",
    "random.seed(0)\n",
    "random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "-PH2Fof3JKtM",
   "metadata": {
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1649034478346,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "-PH2Fof3JKtM"
   },
   "outputs": [],
   "source": [
    "neg_text_train = [text_train[i] for i in indices]\n",
    "neg_title_train = [title_train[i] for i in indices]\n",
    "neg_ingredients_train = [ingredients_train[i] for i in indices]\n",
    "neg_instructions_train = [instructions_train[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aGpqTH3vSBQW",
   "metadata": {
    "id": "aGpqTH3vSBQW"
   },
   "source": [
    "### Model Creation dims = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WHjMsKENSPF4",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649034478348,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "WHjMsKENSPF4"
   },
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, anchor_emb, positive_emb, negative_emb, transform=None):\n",
    "        self.anchor_emb = torch.as_tensor(np.array(anchor_emb))\n",
    "        self.positive_emb = torch.as_tensor(np.array(positive_emb))\n",
    "        self.negative_emb = torch.as_tensor(np.array(negative_emb))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.anchor_emb)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.anchor_emb[idx], self.positive_emb[idx], self.negative_emb[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "km0UIj52SPF4",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649034478582,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "km0UIj52SPF4"
   },
   "outputs": [],
   "source": [
    "class EmbeddingNetwork(nn.Module):\n",
    "    def __init__(self, output_size, input_size=1024):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        self.layer2 = nn.Linear(512, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        return self.layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04O3sv9ZUG_q",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649034478582,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "04O3sv9ZUG_q"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    # Utility function for timers\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99A73aTSUG_q",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1649034478583,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "99A73aTSUG_q"
   },
   "outputs": [],
   "source": [
    "def train(train_loader, img_model, txt_model, criterion, optimizer, epoch):\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "    img_model.train()\n",
    "    txt_model.train()\n",
    "    \n",
    "    batch_time, data_time, losses = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "    end = time.time()\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, (anchor_emb, positive_emb, negative_emb) in enumerate(train_loader):\n",
    "    \n",
    "        # Use GPU if available\n",
    "        if use_gpu: \n",
    "            anchor_emb, positive_emb, negative_emb = anchor_emb.to(device), positive_emb.to(device), negative_emb.to(device)\n",
    "\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Run forward pass\n",
    "        out_anchor_emb = img_model(anchor_emb) \n",
    "        out_positive_emb = txt_model(positive_emb)\n",
    "        out_negative_emb = txt_model(negative_emb)\n",
    "        loss = criterion(out_anchor_emb, out_positive_emb, out_negative_emb) \n",
    "\n",
    "        # Compute gradient and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # Print model accuracy -- in the code below\n",
    "        running_loss += loss.item()\n",
    "        if i % 10000 == 0:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            running_loss = 0.\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                'Time {batch_time.val} ({batch_time.avg})\\t'\n",
    "                'Data {data_time.val} ({data_time.avg})\\t'.format(\n",
    "                  epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                 data_time=data_time)) \n",
    "\n",
    "    print('Finished training epoch {}'.format(epoch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aVzakDA6UG_p",
   "metadata": {
    "id": "aVzakDA6UG_p"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yadDS5RCUG_q",
   "metadata": {
    "id": "yadDS5RCUG_q"
   },
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(img_train, text_train, neg_text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bqj4gAUFUG_q",
   "metadata": {
    "id": "bqj4gAUFUG_q"
   },
   "outputs": [],
   "source": [
    "img_model = EmbeddingNetwork(512)\n",
    "# img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "\n",
    "txt_model = EmbeddingNetwork(512);\n",
    "# txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model.parameters()) + list(txt_model.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdrDHUqNUG_q",
   "metadata": {
    "id": "cdrDHUqNUG_q"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215e5ef-e322-4472-bc3c-5e39faf77538",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226729,
     "status": "ok",
     "timestamp": 1649012289896,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "d215e5ef-e322-4472-bc3c-5e39faf77538",
    "outputId": "8de2efd5-cef9-424b-f9c1-c3c235040cf5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011019824743270874\n",
      "Epoch: [0][0/4400]\tTime 0.2401421070098877 (0.2401421070098877)\tData 0.037871360778808594 (0.037871360778808594)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [1][0/4400]\tTime 0.00605463981628418 (0.00605463981628418)\tData 0.002034425735473633 (0.002034425735473633)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 7.46917724609375e-06\n",
      "Epoch: [2][0/4400]\tTime 0.006739377975463867 (0.006739377975463867)\tData 0.002443075180053711 (0.002443075180053711)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [3][0/4400]\tTime 0.0062716007232666016 (0.0062716007232666016)\tData 0.002272367477416992 (0.002272367477416992)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 1.800537109375e-05\n",
      "Epoch: [4][0/4400]\tTime 0.006798267364501953 (0.006798267364501953)\tData 0.0027828216552734375 (0.0027828216552734375)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.0001078338623046875\n",
      "Epoch: [5][0/4400]\tTime 0.0064792633056640625 (0.0064792633056640625)\tData 0.0023336410522460938 (0.0023336410522460938)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 1.2237548828125e-05\n",
      "Epoch: [6][0/4400]\tTime 0.006546974182128906 (0.006546974182128906)\tData 0.002421140670776367 (0.002421140670776367)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [7][0/4400]\tTime 0.007030487060546875 (0.007030487060546875)\tData 0.0025434494018554688 (0.0025434494018554688)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [8][0/4400]\tTime 0.006589412689208984 (0.006589412689208984)\tData 0.0024421215057373047 (0.0024421215057373047)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 5.096435546875e-05\n",
      "Epoch: [9][0/4400]\tTime 0.00598597526550293 (0.00598597526550293)\tData 0.0020568370819091797 (0.0020568370819091797)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'triplet_checkpoints/img-model-full-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'triplet_checkpoints/txt-model-full-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0mMr_cyVXn6",
   "metadata": {
    "id": "b0mMr_cyVXn6"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5gO69Co-XtG-",
   "metadata": {
    "id": "5gO69Co-XtG-"
   },
   "outputs": [],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train, neg_title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8NkmzTQQXtG-",
   "metadata": {
    "id": "8NkmzTQQXtG-"
   },
   "outputs": [],
   "source": [
    "img_model_title = EmbeddingNetwork(512)\n",
    "# img_model_title = nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(512);\n",
    "# txt_model_title = nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_title.parameters()) + list(txt_model_title.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2rD3vbcBXtG_",
   "metadata": {
    "id": "2rD3vbcBXtG_"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_title = img_model_title.to(device)\n",
    "    txt_model_title = txt_model_title.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vdPiw2N3XtG_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113065,
     "status": "ok",
     "timestamp": 1649012407332,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "vdPiw2N3XtG_",
    "outputId": "5207f145-f9d7-4550-ec04-3d046cd374a5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0009698835015296936\n",
      "Epoch: [0][0/4400]\tTime 0.00953817367553711 (0.00953817367553711)\tData 0.0030028820037841797 (0.0030028820037841797)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00015620994567871094\n",
      "Epoch: [1][0/4400]\tTime 0.005942344665527344 (0.005942344665527344)\tData 0.002160310745239258 (0.002160310745239258)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0001813507080078125\n",
      "Epoch: [2][0/4400]\tTime 0.0066792964935302734 (0.0066792964935302734)\tData 0.0028858184814453125 (0.0028858184814453125)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00028369140625\n",
      "Epoch: [3][0/4400]\tTime 0.006560325622558594 (0.006560325622558594)\tData 0.002418994903564453 (0.002418994903564453)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0001792449951171875\n",
      "Epoch: [4][0/4400]\tTime 0.006670236587524414 (0.006670236587524414)\tData 0.0025289058685302734 (0.0025289058685302734)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'triplet_checkpoints/img-model-title-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'triplet_checkpoints/txt-model-title-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vzdCubAmYl9T",
   "metadata": {
    "id": "vzdCubAmYl9T"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WNR5CtQvY0pT",
   "metadata": {
    "id": "WNR5CtQvY0pT"
   },
   "outputs": [],
   "source": [
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train, neg_ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fxzLUVhKY0pT",
   "metadata": {
    "id": "fxzLUVhKY0pT"
   },
   "outputs": [],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(512)\n",
    "# img_model_ingredients = nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to(device);\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(512);\n",
    "# txt_model_ingredients = nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_ingredients.parameters()) + list(txt_model_ingredients.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64HQ0r0Y0pU",
   "metadata": {
    "id": "c64HQ0r0Y0pU"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_ingredients = img_model_ingredients.to(device)\n",
    "    txt_model_ingredients = txt_model_ingredients.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MkVLVUsLY0pU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112643,
     "status": "ok",
     "timestamp": 1649012541329,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "MkVLVUsLY0pU",
    "outputId": "da0b174f-ed6d-4c3b-de36-48a60ff36dee",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0013143726587295532\n",
      "Epoch: [0][0/4400]\tTime 0.011211156845092773 (0.011211156845092773)\tData 0.00505518913269043 (0.00505518913269043)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00015041542053222657\n",
      "Epoch: [1][0/4400]\tTime 0.00642085075378418 (0.00642085075378418)\tData 0.002285480499267578 (0.002285480499267578)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.000117706298828125\n",
      "Epoch: [2][0/4400]\tTime 0.006246089935302734 (0.006246089935302734)\tData 0.002184629440307617 (0.002184629440307617)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0001165313720703125\n",
      "Epoch: [3][0/4400]\tTime 0.007015228271484375 (0.007015228271484375)\tData 0.002960205078125 (0.002960205078125)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 3.69110107421875e-05\n",
      "Epoch: [4][0/4400]\tTime 0.006520986557006836 (0.006520986557006836)\tData 0.002350330352783203 (0.002350330352783203)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_ingredients.state_dict(), 'triplet_checkpoints/img-model-ingredients-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'triplet_checkpoints/txt-model-ingredients-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ItFJ4j8VZ_En",
   "metadata": {
    "id": "ItFJ4j8VZ_En"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yRCQMtF4aFbr",
   "metadata": {
    "id": "yRCQMtF4aFbr"
   },
   "outputs": [],
   "source": [
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train, neg_instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9FOgosMaFbr",
   "metadata": {
    "id": "x9FOgosMaFbr"
   },
   "outputs": [],
   "source": [
    "img_model_instructions = EmbeddingNetwork(512)\n",
    "# img_model_instructions = nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to(device);\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(512);\n",
    "# txt_model_instructions = nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_instructions.parameters()) + list(txt_model_instructions.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_kOAwY8uaFbr",
   "metadata": {
    "id": "_kOAwY8uaFbr"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_instructions = img_model_instructions.to(device)\n",
    "    txt_model_instructions = txt_model_instructions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D1ndUDK0aFbs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112146,
     "status": "ok",
     "timestamp": 1649012951265,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "D1ndUDK0aFbs",
    "outputId": "9fc4f777-e9cc-4bc3-dadb-dac152e4029c",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011441240310668946\n",
      "Epoch: [0][0/4400]\tTime 0.24729275703430176 (0.24729275703430176)\tData 0.040459632873535156 (0.040459632873535156)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 8.026313781738282e-05\n",
      "Epoch: [1][0/4400]\tTime 0.006369590759277344 (0.006369590759277344)\tData 0.0022590160369873047 (0.0022590160369873047)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00019618606567382813\n",
      "Epoch: [2][0/4400]\tTime 0.006463050842285156 (0.006463050842285156)\tData 0.002434968948364258 (0.002434968948364258)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 2.0751953125e-05\n",
      "Epoch: [3][0/4400]\tTime 0.006967067718505859 (0.006967067718505859)\tData 0.0028460025787353516 (0.0028460025787353516)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.00011408233642578125\n",
      "Epoch: [4][0/4400]\tTime 0.007019996643066406 (0.007019996643066406)\tData 0.002835988998413086 (0.002835988998413086)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(instructions_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_instructions.state_dict(), 'triplet_checkpoints/img-model-instructions-512-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'triplet_checkpoints/txt-model-instructions-512-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KJFa2wqibUv6",
   "metadata": {
    "id": "KJFa2wqibUv6"
   },
   "source": [
    "### Model Creation dims = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NG0u8zdTbd1q",
   "metadata": {
    "id": "NG0u8zdTbd1q"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u0mRePqSbd1r",
   "metadata": {
    "id": "u0mRePqSbd1r"
   },
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(img_train, text_train, neg_text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mhEQMOs0bd1r",
   "metadata": {
    "id": "mhEQMOs0bd1r"
   },
   "outputs": [],
   "source": [
    "img_model = EmbeddingNetwork(256)\n",
    "# img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "\n",
    "txt_model = EmbeddingNetwork(256);\n",
    "# txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model.parameters()) + list(txt_model.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wBZ1kmWVbd1r",
   "metadata": {
    "id": "wBZ1kmWVbd1r"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kZIs4isnbd1r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 219860,
     "status": "ok",
     "timestamp": 1649010784769,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "kZIs4isnbd1r",
    "outputId": "6cd99bb1-1020-4e92-f5c9-60e035d02aa0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011071594953536987\n",
      "Epoch: [0][0/4400]\tTime 0.009580373764038086 (0.009580373764038086)\tData 0.004029989242553711 (0.004029989242553711)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 2.946949005126953e-05\n",
      "Epoch: [1][0/4400]\tTime 0.006267070770263672 (0.006267070770263672)\tData 0.0022928714752197266 (0.0022928714752197266)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [2][0/4400]\tTime 0.00625157356262207 (0.00625157356262207)\tData 0.002292156219482422 (0.002292156219482422)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 9.822845458984375e-05\n",
      "Epoch: [3][0/4400]\tTime 0.005419015884399414 (0.005419015884399414)\tData 0.0016505718231201172 (0.0016505718231201172)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [4][0/4400]\tTime 0.006276130676269531 (0.006276130676269531)\tData 0.002288818359375 (0.002288818359375)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [5][0/4400]\tTime 0.0057599544525146484 (0.0057599544525146484)\tData 0.0019431114196777344 (0.0019431114196777344)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [6][0/4400]\tTime 0.006215333938598633 (0.006215333938598633)\tData 0.002377748489379883 (0.002377748489379883)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [7][0/4400]\tTime 0.0062143802642822266 (0.0062143802642822266)\tData 0.0022592544555664062 (0.0022592544555664062)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 2.410888671875e-05\n",
      "Epoch: [8][0/4400]\tTime 0.006434202194213867 (0.006434202194213867)\tData 0.0024139881134033203 (0.0024139881134033203)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 5.6396484375e-05\n",
      "Epoch: [9][0/4400]\tTime 0.006539344787597656 (0.006539344787597656)\tData 0.002743244171142578 (0.002743244171142578)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'triplet_checkpoints/img-model-full-256-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'triplet_checkpoints/txt-model-full-256-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FHDEbCxBbd1s",
   "metadata": {
    "id": "FHDEbCxBbd1s"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wYGv68fYbd1s",
   "metadata": {
    "id": "wYGv68fYbd1s"
   },
   "outputs": [],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train, neg_title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ub7aARR6bd1s",
   "metadata": {
    "id": "Ub7aARR6bd1s"
   },
   "outputs": [],
   "source": [
    "img_model_title = EmbeddingNetwork(256)\n",
    "# img_model_title = nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(256);\n",
    "# txt_model_title = nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_title.parameters()) + list(txt_model_title.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H0OfOjSJbd1s",
   "metadata": {
    "id": "H0OfOjSJbd1s"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_title = img_model_title.to(device)\n",
    "    txt_model_title = txt_model_title.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yIk_p_p3bd1s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109704,
     "status": "ok",
     "timestamp": 1649010915980,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "yIk_p_p3bd1s",
    "outputId": "f1112409-b6cf-4d62-bcf0-3bd9d6932400",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011362178325653076\n",
      "Epoch: [0][0/4400]\tTime 0.01126861572265625 (0.01126861572265625)\tData 0.0036783218383789062 (0.0036783218383789062)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00026532459259033205\n",
      "Epoch: [1][0/4400]\tTime 0.0063397884368896484 (0.0063397884368896484)\tData 0.002401590347290039 (0.002401590347290039)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00017798614501953125\n",
      "Epoch: [2][0/4400]\tTime 0.0063893795013427734 (0.0063893795013427734)\tData 0.002305269241333008 (0.002305269241333008)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00026576995849609376\n",
      "Epoch: [3][0/4400]\tTime 0.006070852279663086 (0.006070852279663086)\tData 0.002187967300415039 (0.002187967300415039)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0003370819091796875\n",
      "Epoch: [4][0/4400]\tTime 0.0063018798828125 (0.0063018798828125)\tData 0.0024487972259521484 (0.0024487972259521484)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'triplet_checkpoints/img-model-title-256-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'triplet_checkpoints/txt-model-title-256-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-dEJnEfwbd1s",
   "metadata": {
    "id": "-dEJnEfwbd1s"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rDmNColybd1t",
   "metadata": {
    "id": "rDmNColybd1t"
   },
   "outputs": [],
   "source": [
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train, neg_ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OpKPJvR3bd1t",
   "metadata": {
    "id": "OpKPJvR3bd1t"
   },
   "outputs": [],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(256)\n",
    "# img_model_ingredients = nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to(device);\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(256);\n",
    "# txt_model_ingredients = nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_ingredients.parameters()) + list(txt_model_ingredients.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eNa4TRuebd1t",
   "metadata": {
    "id": "eNa4TRuebd1t"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_ingredients = img_model_ingredients.to(device)\n",
    "    txt_model_ingredients = txt_model_ingredients.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yZrDJThFbd1t",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 79127,
     "status": "ok",
     "timestamp": 1649016502110,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "yZrDJThFbd1t",
    "outputId": "f820e015-c414-478e-e3b1-ddbc35e17e59",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011087838411331177\n",
      "Epoch: [0][0/4400]\tTime 0.1564500331878662 (0.1564500331878662)\tData 0.012141704559326172 (0.012141704559326172)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.0001292104721069336\n",
      "Epoch: [1][0/4400]\tTime 0.0044171810150146484 (0.0044171810150146484)\tData 0.0018279552459716797 (0.0018279552459716797)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 3.330230712890625e-06\n",
      "Epoch: [2][0/4400]\tTime 0.0043370723724365234 (0.0043370723724365234)\tData 0.0017855167388916016 (0.0017855167388916016)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 4.06494140625e-05\n",
      "Epoch: [3][0/4400]\tTime 0.004402637481689453 (0.004402637481689453)\tData 0.001814126968383789 (0.001814126968383789)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 2.51922607421875e-05\n",
      "Epoch: [4][0/4400]\tTime 0.004415035247802734 (0.004415035247802734)\tData 0.001775503158569336 (0.001775503158569336)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_ingredients.state_dict(), 'triplet_checkpoints/img-model-ingredients-256-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'triplet_checkpoints/txt-model-ingredients-256-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qrmGP00Wbd1t",
   "metadata": {
    "id": "qrmGP00Wbd1t"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DU1gtziHbd1t",
   "metadata": {
    "id": "DU1gtziHbd1t"
   },
   "outputs": [],
   "source": [
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train, neg_instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rgtp_fyebd1t",
   "metadata": {
    "id": "rgtp_fyebd1t"
   },
   "outputs": [],
   "source": [
    "img_model_instructions = EmbeddingNetwork(256)\n",
    "# img_model_instructions = nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to(device);\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(256);\n",
    "# txt_model_instructions = nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_instructions.parameters()) + list(txt_model_instructions.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SSbpasPWbd1u",
   "metadata": {
    "id": "SSbpasPWbd1u"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_instructions = img_model_instructions.to(device)\n",
    "    txt_model_instructions = txt_model_instructions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cZl2xVxCbd1u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110973,
     "status": "ok",
     "timestamp": 1649011391762,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "cZl2xVxCbd1u",
    "outputId": "7761cd0a-8b44-4404-d054-77eaa796ec8d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011826374530792237\n",
      "Epoch: [0][0/4400]\tTime 0.012595415115356445 (0.012595415115356445)\tData 0.0024366378784179688 (0.0024366378784179688)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 8.850574493408204e-05\n",
      "Epoch: [1][0/4400]\tTime 0.00675201416015625 (0.00675201416015625)\tData 0.0028083324432373047 (0.0028083324432373047)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 1.4438629150390626e-05\n",
      "Epoch: [2][0/4400]\tTime 0.006068229675292969 (0.006068229675292969)\tData 0.0022687911987304688 (0.0022687911987304688)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [3][0/4400]\tTime 0.006207466125488281 (0.006207466125488281)\tData 0.0023376941680908203 (0.0023376941680908203)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 3.316497802734375e-05\n",
      "Epoch: [4][0/4400]\tTime 0.006067991256713867 (0.006067991256713867)\tData 0.002235889434814453 (0.002235889434814453)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(instructions_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_instructions.state_dict(), 'triplet_checkpoints/img-model-instructions-256-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'triplet_checkpoints/txt-model-instructions-256-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ItSD0uV1Ard",
   "metadata": {
    "id": "9ItSD0uV1Ard"
   },
   "source": [
    "### Model Creation dims = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IifdpWPV1Are",
   "metadata": {
    "id": "IifdpWPV1Are"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "CGvdrrlB1Are",
   "metadata": {
    "executionInfo": {
     "elapsed": 2515,
     "status": "ok",
     "timestamp": 1649030300319,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "CGvdrrlB1Are"
   },
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(img_train, text_train, neg_text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "OwH6m8S81Are",
   "metadata": {
    "executionInfo": {
     "elapsed": 10229,
     "status": "ok",
     "timestamp": 1649030310540,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "OwH6m8S81Are"
   },
   "outputs": [],
   "source": [
    "img_model = EmbeddingNetwork(128)\n",
    "# img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "\n",
    "txt_model = EmbeddingNetwork(128);\n",
    "# txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model.parameters()) + list(txt_model.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "hh32K9ty1Are",
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1649030310541,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "hh32K9ty1Are"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "uMw0PlnI1Arf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221009,
     "status": "ok",
     "timestamp": 1649030531540,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "uMw0PlnI1Arf",
    "outputId": "b5bc15a1-a2bd-4e1d-c4ef-f6418e037547",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0010089569091796876\n",
      "Epoch: [0][0/4400]\tTime 0.24431228637695312 (0.24431228637695312)\tData 0.033136606216430664 (0.033136606216430664)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 1.0908126831054688e-05\n",
      "Epoch: [1][0/4400]\tTime 0.006811380386352539 (0.006811380386352539)\tData 0.0026102066040039062 (0.0026102066040039062)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 9.984779357910156e-05\n",
      "Epoch: [2][0/4400]\tTime 0.0066530704498291016 (0.0066530704498291016)\tData 0.0027441978454589844 (0.0027441978454589844)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00013487625122070312\n",
      "Epoch: [3][0/4400]\tTime 0.0061986446380615234 (0.0061986446380615234)\tData 0.002288818359375 (0.002288818359375)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [4][0/4400]\tTime 0.00616002082824707 (0.00616002082824707)\tData 0.0018663406372070312 (0.0018663406372070312)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 2.72369384765625e-06\n",
      "Epoch: [5][0/4400]\tTime 0.006460905075073242 (0.006460905075073242)\tData 0.0025768280029296875 (0.0025768280029296875)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [6][0/4400]\tTime 0.0061299800872802734 (0.0061299800872802734)\tData 0.002057313919067383 (0.002057313919067383)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [7][0/4400]\tTime 0.005440950393676758 (0.005440950393676758)\tData 0.0015370845794677734 (0.0015370845794677734)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [8][0/4400]\tTime 0.008339405059814453 (0.008339405059814453)\tData 0.0038123130798339844 (0.0038123130798339844)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [9][0/4400]\tTime 0.006642341613769531 (0.006642341613769531)\tData 0.002767324447631836 (0.002767324447631836)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'triplet_checkpoints/img-model-full-128-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'triplet_checkpoints/txt-model-full-128-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bt8dset1Arf",
   "metadata": {
    "id": "0bt8dset1Arf"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4hmKiCas1Arf",
   "metadata": {
    "executionInfo": {
     "elapsed": 4189,
     "status": "ok",
     "timestamp": 1649030535713,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "4hmKiCas1Arf"
   },
   "outputs": [],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train, neg_title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "oUuGUNDJ1Arf",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649030535714,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "oUuGUNDJ1Arf"
   },
   "outputs": [],
   "source": [
    "img_model_title = EmbeddingNetwork(128)\n",
    "# img_model_title = nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(128);\n",
    "# txt_model_title = nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_title.parameters()) + list(txt_model_title.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "uAjTE_GN1Arg",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649030535715,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "uAjTE_GN1Arg"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_title = img_model_title.to(device)\n",
    "    txt_model_title = txt_model_title.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8Y_foc_J1Arg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110892,
     "status": "ok",
     "timestamp": 1649030646599,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "8Y_foc_J1Arg",
    "outputId": "ba1aebb7-9dd6-46c8-a786-d3d1c73a9cf1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0010203046798706054\n",
      "Epoch: [0][0/4400]\tTime 0.009414196014404297 (0.009414196014404297)\tData 0.003870248794555664 (0.003870248794555664)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00015114307403564454\n",
      "Epoch: [1][0/4400]\tTime 0.005550861358642578 (0.005550861358642578)\tData 0.0017321109771728516 (0.0017321109771728516)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00029604148864746096\n",
      "Epoch: [2][0/4400]\tTime 0.006967067718505859 (0.006967067718505859)\tData 0.0029146671295166016 (0.0029146671295166016)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0002752838134765625\n",
      "Epoch: [3][0/4400]\tTime 0.006146669387817383 (0.006146669387817383)\tData 0.002075672149658203 (0.002075672149658203)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0002822723388671875\n",
      "Epoch: [4][0/4400]\tTime 0.006098031997680664 (0.006098031997680664)\tData 0.002319812774658203 (0.002319812774658203)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'triplet_checkpoints/img-model-title-128-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'triplet_checkpoints/txt-model-title-128-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KDEOr14V1Arg",
   "metadata": {
    "id": "KDEOr14V1Arg"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "o2foU1QJ1Arg",
   "metadata": {
    "executionInfo": {
     "elapsed": 4000,
     "status": "ok",
     "timestamp": 1649030650583,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "o2foU1QJ1Arg"
   },
   "outputs": [],
   "source": [
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train, neg_ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ahjlkFxM1Arh",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1649030650585,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ahjlkFxM1Arh"
   },
   "outputs": [],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(128)\n",
    "# img_model_ingredients = nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to(device);\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(128);\n",
    "# txt_model_ingredients = nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_ingredients.parameters()) + list(txt_model_ingredients.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "w80WcT3x1Arh",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649030650586,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "w80WcT3x1Arh"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_ingredients = img_model_ingredients.to(device)\n",
    "    txt_model_ingredients = txt_model_ingredients.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ohXpbfz31Arh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 110472,
     "status": "ok",
     "timestamp": 1649030761050,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ohXpbfz31Arh",
    "outputId": "545bdbc4-5d8e-4468-c6ec-e0b71be17da3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011006090641021728\n",
      "Epoch: [0][0/4400]\tTime 0.009591341018676758 (0.009591341018676758)\tData 0.0042994022369384766 (0.0042994022369384766)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00011099052429199219\n",
      "Epoch: [1][0/4400]\tTime 0.006002664566040039 (0.006002664566040039)\tData 0.0017342567443847656 (0.0017342567443847656)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 3.583908081054687e-05\n",
      "Epoch: [2][0/4400]\tTime 0.005978822708129883 (0.005978822708129883)\tData 0.00231170654296875 (0.00231170654296875)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 4.360580444335937e-05\n",
      "Epoch: [3][0/4400]\tTime 0.008204460144042969 (0.008204460144042969)\tData 0.002305746078491211 (0.002305746078491211)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 7.928848266601563e-05\n",
      "Epoch: [4][0/4400]\tTime 0.0068166255950927734 (0.0068166255950927734)\tData 0.002844095230102539 (0.002844095230102539)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_ingredients.state_dict(), 'triplet_checkpoints/img-model-ingredients-128-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'triplet_checkpoints/txt-model-ingredients-128-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YfVhiOCX1Arh",
   "metadata": {
    "id": "YfVhiOCX1Arh"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "DGpHVBmQ1Arh",
   "metadata": {
    "executionInfo": {
     "elapsed": 2987,
     "status": "ok",
     "timestamp": 1649031047270,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "DGpHVBmQ1Arh"
   },
   "outputs": [],
   "source": [
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train, neg_instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "X7061swC1Ari",
   "metadata": {
    "executionInfo": {
     "elapsed": 9875,
     "status": "ok",
     "timestamp": 1649031057140,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "X7061swC1Ari"
   },
   "outputs": [],
   "source": [
    "img_model_instructions = EmbeddingNetwork(128)\n",
    "# img_model_instructions = nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to(device);\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(128);\n",
    "# txt_model_instructions = nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_instructions.parameters()) + list(txt_model_instructions.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "L8Y8sWtq1Ari",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649031057141,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "L8Y8sWtq1Ari"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_instructions = img_model_instructions.to(device)\n",
    "    txt_model_instructions = txt_model_instructions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "CKdkMC921Ari",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114248,
     "status": "ok",
     "timestamp": 1649031171383,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "CKdkMC921Ari",
    "outputId": "b36d72f7-f4e4-4b20-b9b6-45896fe11543",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0009684301614761352\n",
      "Epoch: [0][0/4400]\tTime 0.24962973594665527 (0.24962973594665527)\tData 0.03155255317687988 (0.03155255317687988)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00011184978485107422\n",
      "Epoch: [1][0/4400]\tTime 0.0062177181243896484 (0.0062177181243896484)\tData 0.0022830963134765625 (0.0022830963134765625)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 3.931236267089844e-05\n",
      "Epoch: [2][0/4400]\tTime 0.006750583648681641 (0.006750583648681641)\tData 0.002492189407348633 (0.002492189407348633)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00016136932373046875\n",
      "Epoch: [3][0/4400]\tTime 0.0062749385833740234 (0.0062749385833740234)\tData 0.002020597457885742 (0.002020597457885742)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.000120208740234375\n",
      "Epoch: [4][0/4400]\tTime 0.006830930709838867 (0.006830930709838867)\tData 0.0025472640991210938 (0.0025472640991210938)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(instructions_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_instructions.state_dict(), 'triplet_checkpoints/img-model-instructions-128-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'triplet_checkpoints/txt-model-instructions-128-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rHLlq90yEXlV",
   "metadata": {
    "id": "rHLlq90yEXlV"
   },
   "source": [
    "### Model Creation dims = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4j-NZikmEXlW",
   "metadata": {
    "id": "4j-NZikmEXlW"
   },
   "source": [
    "#### im2recipe and recipe2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ZeSMhQZEXlW",
   "metadata": {
    "executionInfo": {
     "elapsed": 3712,
     "status": "ok",
     "timestamp": 1649033905036,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "7ZeSMhQZEXlW"
   },
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(img_train, text_train, neg_text_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "pGNFrAgCEXlW",
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1649033905038,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "pGNFrAgCEXlW"
   },
   "outputs": [],
   "source": [
    "img_model = EmbeddingNetwork(64)\n",
    "# img_model= nn.DataParallel(img_model, device_ids=[2,3])\n",
    "img_model.to(device);\n",
    "\n",
    "txt_model = EmbeddingNetwork(64);\n",
    "# txt_model= nn.DataParallel(txt_model, device_ids=[2,3])\n",
    "txt_model.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model.parameters()) + list(txt_model.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "uooQbrROEXlX",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1649033905661,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "uooQbrROEXlX"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model = img_model.to(device)\n",
    "    txt_model = txt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "YzuyL-4zEXlX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227924,
     "status": "ok",
     "timestamp": 1649034138829,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "YzuyL-4zEXlX",
    "outputId": "6f56785c-c0a6-41cc-b322-cb9da3e11bfb",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0011592025756835937\n",
      "Epoch: [0][0/4400]\tTime 0.016534090042114258 (0.016534090042114258)\tData 0.00530552864074707 (0.00530552864074707)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 4.945969581604004e-05\n",
      "Epoch: [1][0/4400]\tTime 0.009243965148925781 (0.009243965148925781)\tData 0.002117633819580078 (0.002117633819580078)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 4.458427429199219e-06\n",
      "Epoch: [2][0/4400]\tTime 0.006767988204956055 (0.006767988204956055)\tData 0.002257108688354492 (0.002257108688354492)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [3][0/4400]\tTime 0.0064907073974609375 (0.0064907073974609375)\tData 0.0022830963134765625 (0.0022830963134765625)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [4][0/4400]\tTime 0.006551265716552734 (0.006551265716552734)\tData 0.0025415420532226562 (0.0025415420532226562)\t\n",
      "Finished training epoch 4\n",
      "Starting training epoch 5\n",
      "  batch 1 loss: 0.00013299942016601563\n",
      "Epoch: [5][0/4400]\tTime 0.006296634674072266 (0.006296634674072266)\tData 0.0022537708282470703 (0.0022537708282470703)\t\n",
      "Finished training epoch 5\n",
      "Starting training epoch 6\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [6][0/4400]\tTime 0.006171703338623047 (0.006171703338623047)\tData 0.002280712127685547 (0.002280712127685547)\t\n",
      "Finished training epoch 6\n",
      "Starting training epoch 7\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [7][0/4400]\tTime 0.00638890266418457 (0.00638890266418457)\tData 0.002366304397583008 (0.002366304397583008)\t\n",
      "Finished training epoch 7\n",
      "Starting training epoch 8\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [8][0/4400]\tTime 0.00643467903137207 (0.00643467903137207)\tData 0.0023317337036132812 (0.0023317337036132812)\t\n",
      "Finished training epoch 8\n",
      "Starting training epoch 9\n",
      "  batch 1 loss: 0.0\n",
      "Epoch: [9][0/4400]\tTime 0.0063703060150146484 (0.0063703060150146484)\tData 0.0022444725036621094 (0.0022444725036621094)\t\n",
      "Finished training epoch 9\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(train_loader, img_model, txt_model, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model.state_dict(), 'triplet_checkpoints/img-model-full-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model.state_dict(), 'triplet_checkpoints/txt-model-full-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drJFbwb6EXlX",
   "metadata": {
    "id": "drJFbwb6EXlX"
   },
   "source": [
    "#### im2title and title2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dXUVxhAiEXlX",
   "metadata": {
    "executionInfo": {
     "elapsed": 5264,
     "status": "ok",
     "timestamp": 1649034144075,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "dXUVxhAiEXlX"
   },
   "outputs": [],
   "source": [
    "title_dataset = EmbeddingDataset(img_train, title_train, neg_title_train)\n",
    "title_loader = DataLoader(title_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "raZu7AoCEXlY",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1649034144076,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "raZu7AoCEXlY"
   },
   "outputs": [],
   "source": [
    "img_model_title = EmbeddingNetwork(64)\n",
    "# img_model_title = nn.DataParallel(img_model_title, device_ids=[2,3])\n",
    "img_model_title.to(device);\n",
    "\n",
    "txt_model_title = EmbeddingNetwork(64);\n",
    "# txt_model_title = nn.DataParallel(txt_model_title, device_ids=[2,3])\n",
    "txt_model_title.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_title.parameters()) + list(txt_model_title.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "-wpp6b2CEXlY",
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1649034144077,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "-wpp6b2CEXlY"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_title = img_model_title.to(device)\n",
    "    txt_model_title = txt_model_title.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7mf8Hf5REXlY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114725,
     "status": "ok",
     "timestamp": 1649034258786,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "7mf8Hf5REXlY",
    "outputId": "75057ae8-cba1-4d4a-a0d8-c9f55b6513c4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0009304186701774597\n",
      "Epoch: [0][0/4400]\tTime 0.011830329895019531 (0.011830329895019531)\tData 0.0037920475006103516 (0.0037920475006103516)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 0.00020077419281005858\n",
      "Epoch: [1][0/4400]\tTime 0.006789207458496094 (0.006789207458496094)\tData 0.002667665481567383 (0.002667665481567383)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 0.00010777568817138671\n",
      "Epoch: [2][0/4400]\tTime 0.0063877105712890625 (0.0063877105712890625)\tData 0.0018839836120605469 (0.0018839836120605469)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.00017299652099609376\n",
      "Epoch: [3][0/4400]\tTime 0.007192373275756836 (0.007192373275756836)\tData 0.0028738975524902344 (0.0028738975524902344)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 7.336044311523438e-05\n",
      "Epoch: [4][0/4400]\tTime 0.007213115692138672 (0.007213115692138672)\tData 0.0029790401458740234 (0.0029790401458740234)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(title_loader, img_model_title, txt_model_title, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_title.state_dict(), 'triplet_checkpoints/img-model-title-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_title.state_dict(), 'triplet_checkpoints/txt-model-title-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RjHmlZIKEXlY",
   "metadata": {
    "id": "RjHmlZIKEXlY"
   },
   "source": [
    "#### im2ingredients and ingredients2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "gpkF_qHUEXlY",
   "metadata": {
    "executionInfo": {
     "elapsed": 2967,
     "status": "ok",
     "timestamp": 1649034481544,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "gpkF_qHUEXlY"
   },
   "outputs": [],
   "source": [
    "ingredients_dataset = EmbeddingDataset(img_train, ingredients_train, neg_ingredients_train)\n",
    "ingredients_loader = DataLoader(ingredients_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "QhSry2jnEXlY",
   "metadata": {
    "executionInfo": {
     "elapsed": 10077,
     "status": "ok",
     "timestamp": 1649034491616,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "QhSry2jnEXlY"
   },
   "outputs": [],
   "source": [
    "img_model_ingredients = EmbeddingNetwork(64)\n",
    "# img_model_ingredients = nn.DataParallel(img_model_ingredients, device_ids=[2,3])\n",
    "img_model_ingredients.to(device);\n",
    "\n",
    "txt_model_ingredients = EmbeddingNetwork(64);\n",
    "# txt_model_ingredients = nn.DataParallel(txt_model_ingredients, device_ids=[2,3])\n",
    "txt_model_ingredients.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_ingredients.parameters()) + list(txt_model_ingredients.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "_BdiSd1XEXlZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1649034491617,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "_BdiSd1XEXlZ"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_ingredients = img_model_ingredients.to(device)\n",
    "    txt_model_ingredients = txt_model_ingredients.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "PUY_bREEEXlZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114320,
     "status": "ok",
     "timestamp": 1649034605921,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "PUY_bREEEXlZ",
    "outputId": "5173784f-799a-42ef-fe42-b75975c125dc",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0010432976484298707\n",
      "Epoch: [0][0/4400]\tTime 0.23984885215759277 (0.23984885215759277)\tData 0.04573535919189453 (0.04573535919189453)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 4.1985034942626956e-05\n",
      "Epoch: [1][0/4400]\tTime 0.007517814636230469 (0.007517814636230469)\tData 0.0033102035522460938 (0.0033102035522460938)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 4.564476013183594e-05\n",
      "Epoch: [2][0/4400]\tTime 0.006379842758178711 (0.006379842758178711)\tData 0.0022394657135009766 (0.0022394657135009766)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 0.000120880126953125\n",
      "Epoch: [3][0/4400]\tTime 0.00734710693359375 (0.00734710693359375)\tData 0.003060579299926758 (0.003060579299926758)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 7.970809936523437e-05\n",
      "Epoch: [4][0/4400]\tTime 0.006373167037963867 (0.006373167037963867)\tData 0.0023865699768066406 (0.0023865699768066406)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(ingredients_loader, img_model_ingredients, txt_model_ingredients, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_ingredients.state_dict(), 'triplet_checkpoints/img-model-ingredients-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_ingredients.state_dict(), 'triplet_checkpoints/txt-model-ingredients-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Me2JGz91EXlZ",
   "metadata": {
    "id": "Me2JGz91EXlZ"
   },
   "source": [
    "#### im2instructions and instructions2im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4FEaYAv1EXlZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 4417,
     "status": "ok",
     "timestamp": 1649034610320,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "4FEaYAv1EXlZ"
   },
   "outputs": [],
   "source": [
    "instructions_dataset = EmbeddingDataset(img_train, instructions_train, neg_instructions_train)\n",
    "instructions_loader = DataLoader(instructions_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Vwut84WGEXlZ",
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1649034610322,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "Vwut84WGEXlZ"
   },
   "outputs": [],
   "source": [
    "img_model_instructions = EmbeddingNetwork(64)\n",
    "# img_model_instructions = nn.DataParallel(img_model_instructions, device_ids=[2,3])\n",
    "img_model_instructions.to(device);\n",
    "\n",
    "txt_model_instructions = EmbeddingNetwork(64);\n",
    "# txt_model_instructions = nn.DataParallel(txt_model_instructions, device_ids=[2,3])\n",
    "txt_model_instructions.to(device);\n",
    "\n",
    "optimizer = torch.optim.Adam(list(img_model_instructions.parameters()) + list(txt_model_instructions.parameters()), lr=1e-2, weight_decay=0.0)\n",
    "criterion = nn.TripletMarginLoss(margin = 1)\n",
    "# criterion = nn.TripletMarginWithDistanceLoss(distance_function=lambda x, y: 1.0 - F.cosine_similarity(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5in9W1XGEXla",
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1649034610323,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "5in9W1XGEXla"
   },
   "outputs": [],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "# use_gpu = False\n",
    "if use_gpu: \n",
    "    criterion = criterion.to(device)\n",
    "    img_model_instructions = img_model_instructions.to(device)\n",
    "    txt_model_instructions = txt_model_instructions.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "FZoiJ37BEXla",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113265,
     "status": "ok",
     "timestamp": 1649034723580,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "FZoiJ37BEXla",
    "outputId": "7f255782-c36b-42d4-942e-06352550797a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n",
      "  batch 1 loss: 0.0008711038827896118\n",
      "Epoch: [0][0/4400]\tTime 0.010983467102050781 (0.010983467102050781)\tData 0.004624366760253906 (0.004624366760253906)\t\n",
      "Finished training epoch 0\n",
      "Starting training epoch 1\n",
      "  batch 1 loss: 8.974552154541016e-05\n",
      "Epoch: [1][0/4400]\tTime 0.006373405456542969 (0.006373405456542969)\tData 0.002432584762573242 (0.002432584762573242)\t\n",
      "Finished training epoch 1\n",
      "Starting training epoch 2\n",
      "  batch 1 loss: 9.349822998046876e-05\n",
      "Epoch: [2][0/4400]\tTime 0.0062749385833740234 (0.0062749385833740234)\tData 0.0023622512817382812 (0.0023622512817382812)\t\n",
      "Finished training epoch 2\n",
      "Starting training epoch 3\n",
      "  batch 1 loss: 8.623123168945313e-05\n",
      "Epoch: [3][0/4400]\tTime 0.005517482757568359 (0.005517482757568359)\tData 0.0014796257019042969 (0.0014796257019042969)\t\n",
      "Finished training epoch 3\n",
      "Starting training epoch 4\n",
      "  batch 1 loss: 6.567001342773437e-05\n",
      "Epoch: [4][0/4400]\tTime 0.00663447380065918 (0.00663447380065918)\tData 0.002488851547241211 (0.002488851547241211)\t\n",
      "Finished training epoch 4\n"
     ]
    }
   ],
   "source": [
    "best_losses = 1e10\n",
    "epochs = 5\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    train(instructions_loader, img_model_instructions, txt_model_instructions, criterion, optimizer, epoch)\n",
    "        \n",
    "  # Save checkpoint and replace old best model if current model is betterabs\n",
    "torch.save(img_model_instructions.state_dict(), 'triplet_checkpoints/img-model-instructions-64-epoch-{}.pth'.format(epoch+1))\n",
    "torch.save(txt_model_instructions.state_dict(), 'triplet_checkpoints/txt-model-instructions-64-epoch-{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-6dfvKTPcWQ6",
   "metadata": {
    "id": "-6dfvKTPcWQ6"
   },
   "source": [
    "### Dimensional Analysis. dims = 64 / 128 / 256 / 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9qM0qDgpcWQ6",
   "metadata": {
    "id": "9qM0qDgpcWQ6"
   },
   "source": [
    "#### 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hYt1DmICcWQ6",
   "metadata": {
    "id": "hYt1DmICcWQ6"
   },
   "outputs": [],
   "source": [
    "# im2recipe 512\n",
    "img_model_full_512 = EmbeddingNetwork(512)\n",
    "# img_model_full_512 = nn.DataParallel(img_model_full_512, device_ids=[1])\n",
    "img_model_full_512.load_state_dict(torch.load(\"triplet_checkpoints/img-model-full-512-epoch-10.pth\"))\n",
    "# img_model_full_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_full_512.to('cpu')\n",
    "img_model_full_512.eval();\n",
    "txt_model_full_512 = EmbeddingNetwork(512)\n",
    "# txt_model_full_512 = nn.DataParallel(txt_model_full_512, device_ids=[1])\n",
    "txt_model_full_512.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-full-512-epoch-10.pth\"))\n",
    "# txt_model_full_512.to((f'cuda:{txt_model_full_512.device_ids[0]}'));\n",
    "txt_model_full_512.to('cpu')\n",
    "txt_model_full_512.eval();\n",
    "\n",
    "# im2title 512\n",
    "img_model_title_512 = EmbeddingNetwork(512)\n",
    "# img_model_title_512 = nn.DataParallel(img_model_title_512, device_ids=[1])\n",
    "img_model_title_512.load_state_dict(torch.load(\"triplet_checkpoints/img-model-title-512-epoch-5.pth\"))\n",
    "# img_model_title_512.to((f'cuda:{img_model_title_512.device_ids[0]}'));\n",
    "img_model_title_512.to('cpu')\n",
    "img_model_title_512.eval();\n",
    "txt_model_title_512 = EmbeddingNetwork(512)\n",
    "# txt_model_title_512 = nn.DataParallel(txt_model_title_512, device_ids=[1])\n",
    "txt_model_title_512.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-title-512-epoch-5.pth\"))\n",
    "# txt_model_title_512.to((f'cuda:{txt_model_title_512.device_ids[0]}'));\n",
    "txt_model_title_512.to('cpu')\n",
    "txt_model_title_512.eval();\n",
    "\n",
    "# im2instructions 512\n",
    "img_model_instructions_512 = EmbeddingNetwork(512)\n",
    "# img_model_instructions_512 = nn.DataParallel(img_model_instructions_512, device_ids=[1])\n",
    "img_model_instructions_512.load_state_dict(torch.load(\"triplet_checkpoints/img-model-instructions-512-epoch-5.pth\"))\n",
    "# img_model_instructions_512.to((f'cuda:{img_model_instructions_512.device_ids[0]}'));\n",
    "img_model_instructions_512.to('cpu')\n",
    "img_model_instructions_512.eval();\n",
    "txt_model_instructions_512 = EmbeddingNetwork(512)\n",
    "# txt_model_instructions_512 = nn.DataParallel(txt_model_instructions_512, device_ids=[1])\n",
    "txt_model_instructions_512.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-instructions-512-epoch-5.pth\"))\n",
    "# txt_model_instructions_512.to((f'cuda:{txt_model_instructions_512.device_ids[0]}'));\n",
    "txt_model_instructions_512.to('cpu')\n",
    "txt_model_instructions_512.eval();\n",
    "\n",
    "#im2ingredients 512\n",
    "img_model_ingredients_512 = EmbeddingNetwork(512)\n",
    "# img_model_ingredients_512 = nn.DataParallel(img_model_ingredients_512, device_ids=[1])\n",
    "img_model_ingredients_512.load_state_dict(torch.load(\"triplet_checkpoints/img-model-ingredients-512-epoch-5.pth\"))\n",
    "# img_model_ingredients_512.to((f'cuda:{img_model_full_512.device_ids[0]}'));\n",
    "img_model_ingredients_512.to('cpu')\n",
    "img_model_ingredients_512.eval();\n",
    "txt_model_ingredients_512 = EmbeddingNetwork(512)\n",
    "# txt_model_ingredients_512 = nn.DataParallel(txt_model_ingredients_512, device_ids=[1])\n",
    "txt_model_ingredients_512.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-ingredients-512-epoch-5.pth\"))\n",
    "# txt_model_ingredients_512.to((f'cuda:{txt_model_ingredients_512.device_ids[0]}'));\n",
    "txt_model_ingredients_512.to('cpu')\n",
    "txt_model_ingredients_512.eval();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1oFwAiDbcWQ6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129596,
     "status": "ok",
     "timestamp": 1649015323447,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "1oFwAiDbcWQ6",
    "outputId": "4e31b690-7cd3-417b-fc0b-3ad4103d696b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 512 and sample = 1000\n",
      "Mean median 2.4\n",
      "Recall {1: 0.3538, 5: 0.6819, 10: 0.7863000000000001}\n",
      "Running im2recipe for dims = 512 and sample = 10000\n",
      "Mean median 15.9\n",
      "Recall {1: 0.10740000000000001, 5: 0.3032, 10: 0.41979}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "\n",
    "# img_val_nonlinear.to(device)\n",
    "# text_val_nonlinear.to(device)\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_512(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_512(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2CSuJQu8cWQ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 129819,
     "status": "ok",
     "timestamp": 1649015502927,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "2CSuJQu8cWQ7",
    "outputId": "9231b193-3490-4993-d712-7847c73e55e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 512 and sample = 1000\n",
      "Mean median 5.1\n",
      "Recall {1: 0.20360000000000006, 5: 0.5205, 10: 0.6624000000000001}\n",
      "Running im2title for dims = 512 and sample = 10000\n",
      "Mean median 42.3\n",
      "Recall {1: 0.043919999999999994, 5: 0.15214, 10: 0.23866}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_512(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_512(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2BJgBzSucWQ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127634,
     "status": "ok",
     "timestamp": 1649015630555,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "2BJgBzSucWQ7",
    "outputId": "59153d5a-5d8e-4841-81ba-1a38721d6916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 512 and sample = 1000\n",
      "Mean median 4.1\n",
      "Recall {1: 0.263, 5: 0.5714999999999999, 10: 0.6995000000000001}\n",
      "Running im2ingredients for dims = 512 and sample = 10000\n",
      "Mean median 31.0\n",
      "Recall {1: 0.06504, 5: 0.20672000000000001, 10: 0.30518999999999996}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_512(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_512(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-WAb3W4tcWQ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127637,
     "status": "ok",
     "timestamp": 1649015758188,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "-WAb3W4tcWQ7",
    "outputId": "cae52ce6-dba0-42d5-91eb-60b367f22c94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 512 and sample = 1000\n",
      "Mean median 3.0\n",
      "Recall {1: 0.3229, 5: 0.6366999999999999, 10: 0.7559}\n",
      "Running im2instructions for dims = 512 and sample = 10000\n",
      "Mean median 22.1\n",
      "Recall {1: 0.08642999999999999, 5: 0.25167, 10: 0.36322000000000004}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 512))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_512(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_512(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2instructions for dims = 512 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 512 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VnBSnvR-cWQ7",
   "metadata": {
    "id": "VnBSnvR-cWQ7"
   },
   "source": [
    "#### 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HPmZvQ0FcWQ7",
   "metadata": {
    "id": "HPmZvQ0FcWQ7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# im2recipe 256\n",
    "img_model_full_256 = EmbeddingNetwork(256)\n",
    "# img_model_full_256 = nn.DataParallel(img_model_full_256, device_ids=[1])\n",
    "img_model_full_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-full-256-epoch-10.pth\"))\n",
    "# img_model_full_256.to((f'cuda:{img_model_full_256.device_ids[0]}'));\n",
    "img_model_full_256.to('cpu')\n",
    "img_model_full_256.eval();\n",
    "txt_model_full_256 = EmbeddingNetwork(256)\n",
    "# txt_model_full_256 = nn.DataParallel(txt_model_full_256, device_ids=[1])\n",
    "txt_model_full_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-full-256-epoch-10.pth\"))\n",
    "# txt_model_full_256.to((f'cuda:{txt_model_full_256.device_ids[0]}'));\n",
    "txt_model_full_256.to('cpu')\n",
    "txt_model_full_256.eval();\n",
    "\n",
    "#im2title 256\n",
    "img_model_title_256 = EmbeddingNetwork(256)\n",
    "# img_model_title_256 = nn.DataParallel(img_model_title_256, device_ids=[1])\n",
    "img_model_title_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-title-256-epoch-5.pth\"))\n",
    "# img_model_title_256.to((f'cuda:{img_model_title_256.device_ids[0]}'));\n",
    "img_model_title_256.to('cpu')\n",
    "img_model_title_256.eval();\n",
    "txt_model_title_256 = EmbeddingNetwork(256)\n",
    "# txt_model_title_256 = nn.DataParallel(txt_model_title_256, device_ids=[1])\n",
    "txt_model_title_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-title-256-epoch-5.pth\"))\n",
    "# txt_model_title_256.to((f'cuda:{txt_model_title_256.device_ids[0]}'));\n",
    "txt_model_title_256.to('cpu')\n",
    "txt_model_title_256.eval();\n",
    "\n",
    "#im2instructions 256\n",
    "img_model_instructions_256 = EmbeddingNetwork(256)\n",
    "# img_model_instructions_256 = nn.DataParallel(img_model_instructions_256, device_ids=[1])\n",
    "img_model_instructions_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-instructions-256-epoch-5.pth\"))\n",
    "# img_model_instructions_256.to((f'cuda:{img_model_instructions_256.device_ids[0]}'));\n",
    "img_model_instructions_256.to('cpu')\n",
    "img_model_instructions_256.eval();\n",
    "txt_model_instructions_256 = EmbeddingNetwork(256)\n",
    "# txt_model_instructions_256 = nn.DataParallel(txt_model_instructions_256, device_ids=[1])\n",
    "txt_model_instructions_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-instructions-256-epoch-5.pth\"))\n",
    "# txt_model_instructions_256.to((f'cuda:{txt_model_instructions_256.device_ids[0]}'));\n",
    "txt_model_instructions_256.to('cpu')\n",
    "txt_model_instructions_256.eval();\n",
    "\n",
    "#im2ingredients 256\n",
    "img_model_ingredients_256 = EmbeddingNetwork(256)\n",
    "# img_model_ingredients_256 = nn.DataParallel(img_model_ingredients_256, device_ids=[1])\n",
    "img_model_ingredients_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-ingredients-256-epoch-5.pth\"))\n",
    "# img_model_ingredients_256.to((f'cuda:{img_model_full_256.device_ids[0]}'));\n",
    "img_model_ingredients_256.to('cpu')\n",
    "img_model_ingredients_256.eval();\n",
    "txt_model_ingredients_256 = EmbeddingNetwork(256)\n",
    "# txt_model_ingredients_256 = nn.DataParallel(txt_model_ingredients_256, device_ids=[1])\n",
    "txt_model_ingredients_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-ingredients-256-epoch-5.pth\"))\n",
    "# txt_model_ingredients_256.to((f'cuda:{txt_model_ingredients_256.device_ids[0]}'));\n",
    "txt_model_ingredients_256.to('cpu')\n",
    "txt_model_ingredients_256.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JSueVie1cWQ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121276,
     "status": "ok",
     "timestamp": 1649016644861,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "JSueVie1cWQ7",
    "outputId": "e4ee2461-7868-4827-e96c-7129eb32005a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 256 and sample = 1000\n",
      "Mean median 2.1\n",
      "Recall {1: 0.3718, 5: 0.697, 10: 0.7957}\n",
      "Running im2recipe for dims = 256 and sample = 10000\n",
      "Mean median 15.0\n",
      "Recall {1: 0.11236000000000002, 5: 0.31418, 10: 0.43373999999999996}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_256(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_256(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k3yjeIOKcWQ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120722,
     "status": "ok",
     "timestamp": 1649016765578,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "k3yjeIOKcWQ8",
    "outputId": "93fe49af-6c92-4525-fede-8e666f4c84bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 256 and sample = 1000\n",
      "Mean median 4.9\n",
      "Recall {1: 0.2165, 5: 0.5333, 10: 0.6711}\n",
      "Running im2title for dims = 256 and sample = 10000\n",
      "Mean median 39.5\n",
      "Recall {1: 0.04725999999999999, 5: 0.16085, 10: 0.24957}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_256(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_256(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Az7ppSScWQ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119767,
     "status": "ok",
     "timestamp": 1649016885339,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "8Az7ppSScWQ8",
    "outputId": "46449286-af9c-482b-b925-8713e875b3bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 256 and sample = 1000\n",
      "Mean median 3.9\n",
      "Recall {1: 0.2761, 5: 0.5915, 10: 0.7167000000000001}\n",
      "Running im2ingredients for dims = 256 and sample = 10000\n",
      "Mean median 28.5\n",
      "Recall {1: 0.0715, 5: 0.22025999999999998, 10: 0.32059000000000004}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_256(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_256(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2ingredients and ingredients2im\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4JBT1OT_cWQ8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 121735,
     "status": "ok",
     "timestamp": 1649017069179,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "4JBT1OT_cWQ8",
    "outputId": "603329bc-db33-47bb-acb9-56b864ec5858"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 256 and sample = 1000\n",
      "Mean median 3.2\n",
      "Recall {1: 0.3028, 5: 0.6179, 10: 0.74}\n",
      "Running im2instructions for dims = 256 and sample = 10000\n",
      "Mean median 24.7\n",
      "Recall {1: 0.08179, 5: 0.24386000000000002, 10: 0.34886}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 256))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_256(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_256(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2instructions and instructions2im\n",
    "print(\"Running im2instructions for dims = 256 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 256 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vECwGBaH6878",
   "metadata": {
    "id": "vECwGBaH6878"
   },
   "source": [
    "#### 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "xlAC2C1o6879",
   "metadata": {
    "executionInfo": {
     "elapsed": 412,
     "status": "ok",
     "timestamp": 1649031864960,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "xlAC2C1o6879"
   },
   "outputs": [],
   "source": [
    "# im2recipe 128\n",
    "img_model_full_128 = EmbeddingNetwork(128)\n",
    "# img_model_full_128 = nn.DataParallel(img_model_full_128, device_ids=[1])\n",
    "img_model_full_128.load_state_dict(torch.load(\"triplet_checkpoints/img-model-full-128-epoch-10.pth\"))\n",
    "# img_model_full_128.to((f'cuda:{img_model_full_128.device_ids[0]}'));\n",
    "img_model_full_128.to('cpu')\n",
    "img_model_full_128.eval();\n",
    "txt_model_full_128 = EmbeddingNetwork(128)\n",
    "# txt_model_full_128 = nn.DataParallel(txt_model_full_128, device_ids=[1])\n",
    "txt_model_full_128.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-full-128-epoch-10.pth\"))\n",
    "# txt_model_full_128.to((f'cuda:{txt_model_full_128.device_ids[0]}'));\n",
    "txt_model_full_128.to('cpu')\n",
    "txt_model_full_128.eval();\n",
    "\n",
    "#im2title 128\n",
    "img_model_title_128 = EmbeddingNetwork(128)\n",
    "# img_model_title_128 = nn.DataParallel(img_model_title_128, device_ids=[1])\n",
    "img_model_title_128.load_state_dict(torch.load(\"triplet_checkpoints/img-model-title-128-epoch-5.pth\"))\n",
    "# img_model_title_128.to((f'cuda:{img_model_title_128.device_ids[0]}'));\n",
    "img_model_title_128.to('cpu')\n",
    "img_model_title_128.eval();\n",
    "txt_model_title_128 = EmbeddingNetwork(128)\n",
    "# txt_model_title_128 = nn.DataParallel(txt_model_title_128, device_ids=[1])\n",
    "txt_model_title_128.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-title-128-epoch-5.pth\"))\n",
    "# txt_model_title_128.to((f'cuda:{txt_model_title_128.device_ids[0]}'));\n",
    "txt_model_title_128.to('cpu')\n",
    "txt_model_title_128.eval();\n",
    "\n",
    "#im2instructions 128\n",
    "img_model_instructions_128 = EmbeddingNetwork(128)\n",
    "# img_model_instructions_128 = nn.DataParallel(img_model_instructions_128, device_ids=[1])\n",
    "img_model_instructions_128.load_state_dict(torch.load(\"triplet_checkpoints/img-model-instructions-128-epoch-5.pth\"))\n",
    "# img_model_instructions_128.to((f'cuda:{img_model_instructions_128.device_ids[0]}'));\n",
    "img_model_instructions_128.to('cpu')\n",
    "img_model_instructions_128.eval();\n",
    "txt_model_instructions_128 = EmbeddingNetwork(128)\n",
    "# txt_model_instructions_128 = nn.DataParallel(txt_model_instructions_128, device_ids=[1])\n",
    "txt_model_instructions_128.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-instructions-128-epoch-5.pth\"))\n",
    "# txt_model_instructions_128.to((f'cuda:{txt_model_instructions_128.device_ids[0]}'));\n",
    "txt_model_instructions_128.to('cpu')\n",
    "txt_model_instructions_128.eval();\n",
    "\n",
    "#im2ingredients 128\n",
    "img_model_ingredients_128 = EmbeddingNetwork(128)\n",
    "# img_model_ingredients_128 = nn.DataParallel(img_model_ingredients_128, device_ids=[1])\n",
    "img_model_ingredients_128.load_state_dict(torch.load(\"triplet_checkpoints/img-model-ingredients-128-epoch-5.pth\"))\n",
    "# img_model_ingredients_128.to((f'cuda:{img_model_full_128.device_ids[0]}'));\n",
    "img_model_ingredients_128.to('cpu')\n",
    "img_model_ingredients_128.eval();\n",
    "txt_model_ingredients_128 = EmbeddingNetwork(128)\n",
    "# txt_model_ingredients_128 = nn.DataParallel(txt_model_ingredients_128, device_ids=[1])\n",
    "txt_model_ingredients_128.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-ingredients-128-epoch-5.pth\"))\n",
    "# txt_model_ingredients_128.to((f'cuda:{txt_model_ingredients_128.device_ids[0]}'));\n",
    "txt_model_ingredients_128.to('cpu')\n",
    "txt_model_ingredients_128.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ZBvbKfnt6879",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136588,
     "status": "ok",
     "timestamp": 1649032006804,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ZBvbKfnt6879",
    "outputId": "6f6dbc7e-38f3-485c-d932-3aad49b2922a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 128 and sample = 1000\n",
      "Mean median 2.0\n",
      "Recall {1: 0.3781, 5: 0.6936, 10: 0.797}\n",
      "Running im2recipe for dims = 128 and sample = 10000\n",
      "Mean median 13.9\n",
      "Recall {1: 0.12241000000000002, 5: 0.32748000000000005, 10: 0.44809}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_128(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_128(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 128 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 128 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "rtC8FIff6879",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135429,
     "status": "ok",
     "timestamp": 1649032142227,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "rtC8FIff6879",
    "outputId": "8c9a5fd2-0cfa-447e-f33a-245d3a71db5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 128 and sample = 1000\n",
      "Mean median 5.1\n",
      "Recall {1: 0.21220000000000003, 5: 0.5246000000000001, 10: 0.6697}\n",
      "Running im2title for dims = 128 and sample = 10000\n",
      "Mean median 41.6\n",
      "Recall {1: 0.04628, 5: 0.15596, 10: 0.24558}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_128(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_128(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 128 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 128 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "m947zI-u687-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135098,
     "status": "ok",
     "timestamp": 1649032277308,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "m947zI-u687-",
    "outputId": "b433f847-c4a5-4654-d443-2fd284bf665d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 128 and sample = 1000\n",
      "Mean median 3.85\n",
      "Recall {1: 0.26820000000000005, 5: 0.5885, 10: 0.7089000000000001}\n",
      "Running im2ingredients for dims = 128 and sample = 10000\n",
      "Mean median 29.0\n",
      "Recall {1: 0.06841, 5: 0.21638000000000002, 10: 0.31759000000000004}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_128(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_128(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2ingredients and ingredients2im\n",
    "print(\"Running im2ingredients for dims = 128 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 128 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ylXZ62uz687-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135474,
     "status": "ok",
     "timestamp": 1649032412775,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ylXZ62uz687-",
    "outputId": "15cfbc5c-9476-41ab-8906-9dba7cc5819a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 128 and sample = 1000\n",
      "Mean median 3.0\n",
      "Recall {1: 0.32460000000000006, 5: 0.6363, 10: 0.7506}\n",
      "Running im2instructions for dims = 128 and sample = 10000\n",
      "Mean median 21.8\n",
      "Recall {1: 0.08918000000000001, 5: 0.25699000000000005, 10: 0.36541}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 128))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_128(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_128(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2instructions and instructions2im\n",
    "print(\"Running im2instructions for dims = 128 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 128 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A_LUL7UgAaf4",
   "metadata": {
    "id": "A_LUL7UgAaf4"
   },
   "source": [
    "#### 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ZiWv8IjCAaf5",
   "metadata": {
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1649034771619,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ZiWv8IjCAaf5"
   },
   "outputs": [],
   "source": [
    "# im2recipe 64\n",
    "img_model_full_64 = EmbeddingNetwork(64)\n",
    "# img_model_full_64 = nn.DataParallel(img_model_full_64, device_ids=[1])\n",
    "img_model_full_64.load_state_dict(torch.load(\"triplet_checkpoints/img-model-full-64-epoch-10.pth\"))\n",
    "# img_model_full_64.to((f'cuda:{img_model_full_64.device_ids[0]}'));\n",
    "img_model_full_64.to('cpu')\n",
    "img_model_full_64.eval();\n",
    "txt_model_full_64 = EmbeddingNetwork(64)\n",
    "# txt_model_full_64 = nn.DataParallel(txt_model_full_64, device_ids=[1])\n",
    "txt_model_full_64.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-full-64-epoch-10.pth\"))\n",
    "# txt_model_full_64.to((f'cuda:{txt_model_full_64.device_ids[0]}'));\n",
    "txt_model_full_64.to('cpu')\n",
    "txt_model_full_64.eval();\n",
    "\n",
    "#im2title 64\n",
    "img_model_title_64 = EmbeddingNetwork(64)\n",
    "# img_model_title_64 = nn.DataParallel(img_model_title_64, device_ids=[1])\n",
    "img_model_title_64.load_state_dict(torch.load(\"triplet_checkpoints/img-model-title-64-epoch-5.pth\"))\n",
    "# img_model_title_64.to((f'cuda:{img_model_title_64.device_ids[0]}'));\n",
    "img_model_title_64.to('cpu')\n",
    "img_model_title_64.eval();\n",
    "txt_model_title_64 = EmbeddingNetwork(64)\n",
    "# txt_model_title_64 = nn.DataParallel(txt_model_title_64, device_ids=[1])\n",
    "txt_model_title_64.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-title-64-epoch-5.pth\"))\n",
    "# txt_model_title_64.to((f'cuda:{txt_model_title_64.device_ids[0]}'));\n",
    "txt_model_title_64.to('cpu')\n",
    "txt_model_title_64.eval();\n",
    "\n",
    "#im2instructions 64\n",
    "img_model_instructions_64 = EmbeddingNetwork(64)\n",
    "# img_model_instructions_64 = nn.DataParallel(img_model_instructions_64, device_ids=[1])\n",
    "img_model_instructions_64.load_state_dict(torch.load(\"triplet_checkpoints/img-model-instructions-64-epoch-5.pth\"))\n",
    "# img_model_instructions_64.to((f'cuda:{img_model_instructions_64.device_ids[0]}'));\n",
    "img_model_instructions_64.to('cpu')\n",
    "img_model_instructions_64.eval();\n",
    "txt_model_instructions_64 = EmbeddingNetwork(64)\n",
    "# txt_model_instructions_64 = nn.DataParallel(txt_model_instructions_64, device_ids=[1])\n",
    "txt_model_instructions_64.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-instructions-64-epoch-5.pth\"))\n",
    "# txt_model_instructions_64.to((f'cuda:{txt_model_instructions_64.device_ids[0]}'));\n",
    "txt_model_instructions_64.to('cpu')\n",
    "txt_model_instructions_64.eval();\n",
    "\n",
    "#im2ingredients 64\n",
    "img_model_ingredients_64 = EmbeddingNetwork(64)\n",
    "# img_model_ingredients_64 = nn.DataParallel(img_model_ingredients_64, device_ids=[1])\n",
    "img_model_ingredients_64.load_state_dict(torch.load(\"triplet_checkpoints/img-model-ingredients-64-epoch-5.pth\"))\n",
    "# img_model_ingredients_64.to((f'cuda:{img_model_full_64.device_ids[0]}'));\n",
    "img_model_ingredients_64.to('cpu')\n",
    "img_model_ingredients_64.eval();\n",
    "txt_model_ingredients_64 = EmbeddingNetwork(64)\n",
    "# txt_model_ingredients_64 = nn.DataParallel(txt_model_ingredients_64, device_ids=[1])\n",
    "txt_model_ingredients_64.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-ingredients-64-epoch-5.pth\"))\n",
    "# txt_model_ingredients_64.to((f'cuda:{txt_model_ingredients_64.device_ids[0]}'));\n",
    "txt_model_ingredients_64.to('cpu')\n",
    "txt_model_ingredients_64.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "DBfP6O6zAaf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135342,
     "status": "ok",
     "timestamp": 1649034913210,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "DBfP6O6zAaf5",
    "outputId": "6dae4b2b-1d3b-493a-dbb3-5503b0f7e956"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 64 and sample = 1000\n",
      "Mean median 2.4\n",
      "Recall {1: 0.35960000000000003, 5: 0.6848, 10: 0.7952}\n",
      "Running im2recipe for dims = 64 and sample = 10000\n",
      "Mean median 14.9\n",
      "Recall {1: 0.11610999999999998, 5: 0.31612999999999997, 10: 0.43578}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_full_64(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_full_64(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 64 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 64 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "t2GcFP2cAaf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133976,
     "status": "ok",
     "timestamp": 1649035047180,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "t2GcFP2cAaf6",
    "outputId": "27054508-7b48-47fb-f74d-e43347e77449"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 64 and sample = 1000\n",
      "Mean median 4.9\n",
      "Recall {1: 0.21749999999999997, 5: 0.5327999999999999, 10: 0.6723}\n",
      "Running im2title for dims = 64 and sample = 10000\n",
      "Mean median 39.25\n",
      "Recall {1: 0.046579999999999996, 5: 0.16155, 10: 0.25102}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_title_64(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_title_64(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 64 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 64 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rXLCj98XAaf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132807,
     "status": "ok",
     "timestamp": 1649035179981,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "rXLCj98XAaf6",
    "outputId": "9a149738-971c-4f23-cd35-d05218dcba07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 64 and sample = 1000\n",
      "Mean median 4.0\n",
      "Recall {1: 0.269, 5: 0.5833999999999999, 10: 0.7110999999999998}\n",
      "Running im2ingredients for dims = 64 and sample = 10000\n",
      "Mean median 28.6\n",
      "Recall {1: 0.07246999999999999, 5: 0.22054, 10: 0.32203}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_ingredients_64(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_ingredients_64(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2ingredients and ingredients2im\n",
    "print(\"Running im2ingredients for dims = 64 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 64 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9uE4CUEVAaf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132760,
     "status": "ok",
     "timestamp": 1649035312734,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "9uE4CUEVAaf6",
    "outputId": "6edb18b6-02b5-4bd3-aa55-21686c0cedfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 64 and sample = 1000\n",
      "Mean median 3.0\n",
      "Recall {1: 0.3296, 5: 0.6529, 10: 0.7682}\n",
      "Running im2instructions for dims = 64 and sample = 10000\n",
      "Mean median 20.0\n",
      "Recall {1: 0.09388999999999999, 5: 0.27048000000000005, 10: 0.38217}\n"
     ]
    }
   ],
   "source": [
    "img_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "text_val_nonlinear = np.zeros(shape = (len(img_val), 64))\n",
    "\n",
    "for i in range(len(img_val)):\n",
    "    img_val_nonlinear[i] = img_model_instructions_64(torch.Tensor(np.expand_dims(img_val[i], 0))).detach().numpy()\n",
    "    text_val_nonlinear[i] = txt_model_instructions_64(torch.Tensor(np.expand_dims(text_val[i], 0))).detach().numpy()\n",
    "\n",
    "# im2instructions and instructions2im\n",
    "print(\"Running im2instructions for dims = 64 and sample = 1000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 64 and sample = 10000\")\n",
    "ranker(img_val_nonlinear, text_val_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BPyjz3lqeyx7",
   "metadata": {
    "id": "BPyjz3lqeyx7"
   },
   "source": [
    "### Evaluation and Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mO28plXEeyx8",
   "metadata": {
    "id": "mO28plXEeyx8"
   },
   "source": [
    " We can see that dimensions = 256 has a better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AdWuciK8GbHh",
   "metadata": {
    "id": "AdWuciK8GbHh"
   },
   "outputs": [],
   "source": [
    "# im2recipe 256\n",
    "img_model_full_256 = EmbeddingNetwork(256)\n",
    "# img_model_full_256 = nn.DataParallel(img_model_full_256, device_ids=[1])\n",
    "img_model_full_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-full-256-epoch-10.pth\"))\n",
    "# img_model_full_256.to((f'cuda:{img_model_full_256.device_ids[0]}'));\n",
    "img_model_full_256.to('cpu')\n",
    "img_model_full_256.eval();\n",
    "txt_model_full_256 = EmbeddingNetwork(256)\n",
    "# txt_model_full_256 = nn.DataParallel(txt_model_full_256, device_ids=[1])\n",
    "txt_model_full_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-full-256-epoch-10.pth\"))\n",
    "# txt_model_full_256.to((f'cuda:{txt_model_full_256.device_ids[0]}'));\n",
    "txt_model_full_256.to('cpu')\n",
    "txt_model_full_256.eval();\n",
    "\n",
    "#im2title 256\n",
    "img_model_title_256 = EmbeddingNetwork(256)\n",
    "# img_model_title_256 = nn.DataParallel(img_model_title_256, device_ids=[1])\n",
    "img_model_title_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-title-256-epoch-5.pth\"))\n",
    "# img_model_title_256.to((f'cuda:{img_model_title_256.device_ids[0]}'));\n",
    "img_model_title_256.to('cpu')\n",
    "img_model_title_256.eval();\n",
    "txt_model_title_256 = EmbeddingNetwork(256)\n",
    "# txt_model_title_256 = nn.DataParallel(txt_model_title_256, device_ids=[1])\n",
    "txt_model_title_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-title-256-epoch-5.pth\"))\n",
    "# txt_model_title_256.to((f'cuda:{txt_model_title_256.device_ids[0]}'));\n",
    "txt_model_title_256.to('cpu')\n",
    "txt_model_title_256.eval();\n",
    "\n",
    "#im2instructions 256\n",
    "img_model_instructions_256 = EmbeddingNetwork(256)\n",
    "# img_model_instructions_256 = nn.DataParallel(img_model_instructions_256, device_ids=[1])\n",
    "img_model_instructions_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-instructions-256-epoch-5.pth\"))\n",
    "# img_model_instructions_256.to((f'cuda:{img_model_instructions_256.device_ids[0]}'));\n",
    "img_model_instructions_256.to('cpu')\n",
    "img_model_instructions_256.eval();\n",
    "txt_model_instructions_256 = EmbeddingNetwork(256)\n",
    "# txt_model_instructions_256 = nn.DataParallel(txt_model_instructions_256, device_ids=[1])\n",
    "txt_model_instructions_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-instructions-256-epoch-5.pth\"))\n",
    "# txt_model_instructions_256.to((f'cuda:{txt_model_instructions_256.device_ids[0]}'));\n",
    "txt_model_instructions_256.to('cpu')\n",
    "txt_model_instructions_256.eval();\n",
    "\n",
    "#im2ingredients 256\n",
    "img_model_ingredients_256 = EmbeddingNetwork(256)\n",
    "# img_model_ingredients_256 = nn.DataParallel(img_model_ingredients_256, device_ids=[1])\n",
    "img_model_ingredients_256.load_state_dict(torch.load(\"triplet_checkpoints/img-model-ingredients-256-epoch-5.pth\"))\n",
    "# img_model_ingredients_256.to((f'cuda:{img_model_full_256.device_ids[0]}'));\n",
    "img_model_ingredients_256.to('cpu')\n",
    "img_model_ingredients_256.eval();\n",
    "txt_model_ingredients_256 = EmbeddingNetwork(256)\n",
    "# txt_model_ingredients_256 = nn.DataParallel(txt_model_ingredients_256, device_ids=[1])\n",
    "txt_model_ingredients_256.load_state_dict(torch.load(\"triplet_checkpoints/txt-model-ingredients-256-epoch-5.pth\"))\n",
    "# txt_model_ingredients_256.to((f'cuda:{txt_model_ingredients_256.device_ids[0]}'));\n",
    "txt_model_ingredients_256.to('cpu')\n",
    "txt_model_ingredients_256.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AjVNutS7GbHi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120205,
     "status": "ok",
     "timestamp": 1649017853954,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "AjVNutS7GbHi",
    "outputId": "17530e59-4998-4ff3-ea99-7c23dee0799e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2recipe for dims = 256 and sample = 1000\n",
      "Mean median 2.0\n",
      "Recall {1: 0.3794, 5: 0.6984, 10: 0.7993}\n",
      "Running im2recipe for dims = 256 and sample = 10000\n",
      "Mean median 15.1\n",
      "Recall {1: 0.11216, 5: 0.31331000000000003, 10: 0.43186}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_full_256(torch.Tensor(np.expand_dims(img_test[i], 0))).detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_full_256(torch.Tensor(np.expand_dims(text_test[i], 0))).detach().numpy()\n",
    "\n",
    "# im2recipe and recipe2im\n",
    "print(\"Running im2recipe for dims = 256 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2recipe for dims = 256 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NpulaqACGbHi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119823,
     "status": "ok",
     "timestamp": 1649017973771,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "NpulaqACGbHi",
    "outputId": "fc59928a-1e1e-44dc-e2fa-f4671fd22285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2title for dims = 256 and sample = 1000\n",
      "Mean median 4.85\n",
      "Recall {1: 0.217, 5: 0.5293, 10: 0.665}\n",
      "Running im2title for dims = 256 and sample = 10000\n",
      "Mean median 40.25\n",
      "Recall {1: 0.04741000000000001, 5: 0.16038000000000002, 10: 0.25114000000000003}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_title_256(torch.Tensor(np.expand_dims(img_test[i], 0))).detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_title_256(torch.Tensor(np.expand_dims(text_test[i], 0))).detach().numpy()\n",
    "\n",
    "# im2title and title2im\n",
    "print(\"Running im2title for dims = 256 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2title for dims = 256 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-2PrXGGmGbHj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119788,
     "status": "ok",
     "timestamp": 1649018093552,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "-2PrXGGmGbHj",
    "outputId": "9a4acffb-c9b0-4254-e382-9e9e8b069310"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2ingredients for dims = 256 and sample = 1000\n",
      "Mean median 3.8\n",
      "Recall {1: 0.276, 5: 0.5836, 10: 0.7084}\n",
      "Running im2ingredients for dims = 256 and sample = 10000\n",
      "Mean median 28.4\n",
      "Recall {1: 0.07182999999999999, 5: 0.22010000000000002, 10: 0.31999}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_ingredients_256(torch.Tensor(np.expand_dims(img_test[i], 0))).detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_ingredients_256(torch.Tensor(np.expand_dims(text_test[i], 0))).detach().numpy()\n",
    "\n",
    "# im2ingredients and ingredients2im\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2ingredients for dims = 256 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ylC0AKqYGbHj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 120233,
     "status": "ok",
     "timestamp": 1649018213778,
     "user": {
      "displayName": "Neil Pillai",
      "userId": "06533696322730179220"
     },
     "user_tz": 240
    },
    "id": "ylC0AKqYGbHj",
    "outputId": "bd955450-3ff2-43e5-8bec-7466a4faec6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running im2instructions for dims = 256 and sample = 1000\n",
      "Mean median 3.4\n",
      "Recall {1: 0.29660000000000003, 5: 0.6060999999999999, 10: 0.7335}\n",
      "Running im2instructions for dims = 256 and sample = 10000\n",
      "Mean median 24.7\n",
      "Recall {1: 0.07762, 5: 0.23694999999999994, 10: 0.34320000000000006}\n"
     ]
    }
   ],
   "source": [
    "img_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "text_test_nonlinear = np.zeros(shape = (len(img_test), 256))\n",
    "\n",
    "for i in range(len(img_test)):\n",
    "    img_test_nonlinear[i] = img_model_instructions_256(torch.Tensor(np.expand_dims(img_test[i], 0))).detach().numpy()\n",
    "    text_test_nonlinear[i] = txt_model_instructions_256(torch.Tensor(np.expand_dims(text_test[i], 0))).detach().numpy()\n",
    "\n",
    "# im2instructions and instructions2im\n",
    "print(\"Running im2instructions for dims = 256 and sample = 1000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 1000, \"image\")\n",
    "print(\"Running im2instructions for dims = 256 and sample = 10000\")\n",
    "ranker(img_test_nonlinear, text_test_nonlinear, 10000, \"image\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "022f6705",
    "64b12673-5d63-45c2-9090-2179bc0b2fb0",
    "38a4ee6f-1b30-4f88-ba4f-d0015d283cf7",
    "5f0e6af1-ebb9-4a04-bead-4bf7618cd04e",
    "557c8295-86c4-4a71-a669-bb8bc7cc5425",
    "4e9b4352-7e5b-4983-a933-a2d78daa2ae7",
    "2606fc14-019e-40da-8e96-bb575671d842",
    "c9804e73-7c90-4b09-a40e-1a83cdf7e514",
    "dbccae98-d1ef-4f5f-9668-04c101da262e",
    "927d6eaf-f99d-4111-b06b-b531501ada61",
    "1cc8ca56-6956-47ff-9ee5-7012a23b6311",
    "c07e2117-8725-4dc4-9f9a-b874d21e853b",
    "ff253dc1-1f1c-4669-ba8f-ffec14253e51",
    "2a1658c8-da88-427c-9736-5ccf1db5a2cc",
    "87eb3196-262a-4e57-b2fa-80c2cd02cd19",
    "df7f444d-cfff-43ec-97dc-c92d837a69d2",
    "3c05f885",
    "75afd4aa",
    "f9add441",
    "9663baa6",
    "3062b9f9",
    "364fd19b",
    "37c7aeb7",
    "lTmZH8KqLWWN",
    "6Rr1WETBLWWO",
    "GFMxYQRpLWWO",
    "ad96b882",
    "296c17c0",
    "5c60c319",
    "6fa626f8",
    "HM8VRH7HiStf",
    "aVzakDA6UG_p",
    "b0mMr_cyVXn6",
    "vzdCubAmYl9T",
    "ItFJ4j8VZ_En",
    "KJFa2wqibUv6",
    "NG0u8zdTbd1q",
    "FHDEbCxBbd1s",
    "-dEJnEfwbd1s",
    "qrmGP00Wbd1t",
    "IifdpWPV1Are",
    "0bt8dset1Arf",
    "KDEOr14V1Arg",
    "YfVhiOCX1Arh",
    "4j-NZikmEXlW",
    "drJFbwb6EXlX",
    "9qM0qDgpcWQ6",
    "vECwGBaH6878",
    "BPyjz3lqeyx7"
   ],
   "machine_shape": "hm",
   "name": "CCA and Ablation (new).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
